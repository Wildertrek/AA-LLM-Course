{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08e59440-4d9d-46c6-ae31-c1da413496a5",
   "metadata": {},
   "source": [
    "# 2014 - RNNs in Encoder-Decoder Architectures\n",
    "\n",
    "[2014 RNNs in Encoder-Decoder architectures](https://en.wikipedia.org/wiki/Recurrent_neural_network)  \n",
    "RNNs were a significant advancement, capable of computing document embeddings and adding word context. They grew to include LSTM (1997) for long-term dependencies and Bidirectional RNN (1997) for context understanding. Encoder-Decoder RNNs (2014) improved on this method.\n",
    "\n",
    "In this notebook, we will explore the concept of Recurrent Neural Networks (RNNs) and how they are used in Encoder-Decoder architectures, which significantly improved the ability to model sequences and context in text data. We'll implement a basic Encoder-Decoder model using RNNs for sequence-to-sequence tasks.\n",
    "\n",
    "### Step-by-Step Explanation:\n",
    "\n",
    "1. **Understanding RNNs**: RNNs are a type of neural network designed to handle sequential data by maintaining a hidden state that captures information about previous inputs in the sequence.\n",
    "2. **Encoder-Decoder Architecture**: This architecture consists of two RNNs: an Encoder that processes the input sequence and compresses it into a fixed-length context vector, and a Decoder that generates the output sequence based on this context vector.\n",
    "3. **Training an Encoder-Decoder Model**: We'll train a simple Encoder-Decoder model using a sample dataset.\n",
    "4. **Generating Sequences**: Using the trained model, we will generate sequences to demonstrate how the Encoder-Decoder architecture works.\n",
    "\n",
    "### Implementation\n",
    "\n",
    "#### 1. Install Required Libraries\n",
    "\n",
    "Before running the code, ensure you have the necessary libraries installed. You can use:\n",
    "\n",
    "```bash\n",
    "!pip install tensorflow numpy\n",
    "```\n",
    "\n",
    "#### 2. Import Libraries and Define the Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12a0dc56-f60d-4938-be7e-e968f20f46e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e8d062a-fe67-4bd2-b62f-ce571d9f3e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-18 14:12:02.695766: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "\n",
    "# Define a simple dataset\n",
    "input_texts = [\n",
    "    \"hello\",\n",
    "    \"world\",\n",
    "    \"machine\",\n",
    "    \"learning\",\n",
    "    \"encoder\",\n",
    "    \"decoder\"\n",
    "]\n",
    "\n",
    "output_texts = [\n",
    "    \"hola\",\n",
    "    \"mundo\",\n",
    "    \"máquina\",\n",
    "    \"aprendizaje\",\n",
    "    \"codificador\",\n",
    "    \"decodificador\"\n",
    "]\n",
    "\n",
    "# Create character sets\n",
    "input_characters = sorted(set(''.join(input_texts)))\n",
    "output_characters = sorted(set(''.join(output_texts)))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(output_characters)\n",
    "\n",
    "# Create a mapping of characters to integers\n",
    "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
    "output_token_index = dict([(char, i) for i, char in enumerate(output_characters)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3d0d72-a1f4-463c-8fbb-09b7de7b04d5",
   "metadata": {},
   "source": [
    "#### 3. Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0af534df-5e6a-44e8-83f2-97f5ed12586e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define maximum sequence lengths\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in output_texts])\n",
    "\n",
    "# Vectorize the input and output texts\n",
    "encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype='float32')\n",
    "decoder_input_data = np.zeros((len(output_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
    "decoder_target_data = np.zeros((len(output_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, output_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
    "    for t, char in enumerate(target_text):\n",
    "        decoder_input_data[i, t, output_token_index[char]] = 1.0\n",
    "        if t > 0:\n",
    "            decoder_target_data[i, t - 1, output_token_index[char]] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becd62e5-360c-4b02-93e1-3873624629ee",
   "metadata": {},
   "source": [
    "#### 4. Build the Encoder-Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36650eee-18ec-4e24-971a-363a0f9c3766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Encoder\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder_lstm = LSTM(256, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Define the Decoder\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f090c21-b261-4b7b-bbfc-33b49265a01c",
   "metadata": {},
   "source": [
    "#### 5. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1853374d-19c5-474e-9293-963c1be2871f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.0192 - loss: 1.2811 - val_accuracy: 0.1538 - val_loss: 2.4419\n",
      "Epoch 2/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.4038 - loss: 1.2715 - val_accuracy: 0.1538 - val_loss: 2.4392\n",
      "Epoch 3/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.5000 - loss: 1.2638 - val_accuracy: 0.2308 - val_loss: 2.4367\n",
      "Epoch 4/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.5769 - loss: 1.2565 - val_accuracy: 0.2308 - val_loss: 2.4338\n",
      "Epoch 5/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.6154 - loss: 1.2491 - val_accuracy: 0.2308 - val_loss: 2.4301\n",
      "Epoch 6/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.6731 - loss: 1.2408 - val_accuracy: 0.2308 - val_loss: 2.4247\n",
      "Epoch 7/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.6731 - loss: 1.2303 - val_accuracy: 0.2308 - val_loss: 2.4161\n",
      "Epoch 8/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.6538 - loss: 1.2148 - val_accuracy: 0.2308 - val_loss: 2.4082\n",
      "Epoch 9/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.6154 - loss: 1.1895 - val_accuracy: 0.2308 - val_loss: 2.4281\n",
      "Epoch 10/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.6154 - loss: 1.1585 - val_accuracy: 0.2308 - val_loss: 2.4505\n",
      "Epoch 11/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.6154 - loss: 1.1337 - val_accuracy: 0.2308 - val_loss: 2.4877\n",
      "Epoch 12/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.6154 - loss: 1.1172 - val_accuracy: 0.2308 - val_loss: 2.5256\n",
      "Epoch 13/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.6154 - loss: 1.1090 - val_accuracy: 0.2308 - val_loss: 2.5456\n",
      "Epoch 14/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.6154 - loss: 1.1017 - val_accuracy: 0.2308 - val_loss: 2.5519\n",
      "Epoch 15/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.6538 - loss: 1.0930 - val_accuracy: 0.2692 - val_loss: 2.5550\n",
      "Epoch 16/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.6538 - loss: 1.0863 - val_accuracy: 0.2692 - val_loss: 2.5588\n",
      "Epoch 17/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.6346 - loss: 1.0774 - val_accuracy: 0.2692 - val_loss: 2.5457\n",
      "Epoch 18/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.6346 - loss: 1.0708 - val_accuracy: 0.2692 - val_loss: 2.5573\n",
      "Epoch 19/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.6346 - loss: 1.0618 - val_accuracy: 0.2692 - val_loss: 2.5458\n",
      "Epoch 20/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.6346 - loss: 1.0540 - val_accuracy: 0.2692 - val_loss: 2.5518\n",
      "Epoch 21/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.6346 - loss: 1.0443 - val_accuracy: 0.2692 - val_loss: 2.5524\n",
      "Epoch 22/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.6346 - loss: 1.0367 - val_accuracy: 0.2692 - val_loss: 2.5629\n",
      "Epoch 23/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.6346 - loss: 1.0266 - val_accuracy: 0.2692 - val_loss: 2.5607\n",
      "Epoch 24/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.6346 - loss: 1.0237 - val_accuracy: 0.2308 - val_loss: 2.5711\n",
      "Epoch 25/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.6538 - loss: 1.0140 - val_accuracy: 0.2308 - val_loss: 2.5769\n",
      "Epoch 26/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.6538 - loss: 1.0078 - val_accuracy: 0.2308 - val_loss: 2.5820\n",
      "Epoch 27/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.6346 - loss: 0.9996 - val_accuracy: 0.2308 - val_loss: 2.5872\n",
      "Epoch 28/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.6346 - loss: 0.9943 - val_accuracy: 0.2308 - val_loss: 2.5893\n",
      "Epoch 29/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.6346 - loss: 0.9844 - val_accuracy: 0.2308 - val_loss: 2.5951\n",
      "Epoch 30/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.6346 - loss: 0.9821 - val_accuracy: 0.2308 - val_loss: 2.5980\n",
      "Epoch 31/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.6538 - loss: 0.9701 - val_accuracy: 0.2308 - val_loss: 2.5946\n",
      "Epoch 32/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.6538 - loss: 0.9692 - val_accuracy: 0.2308 - val_loss: 2.6066\n",
      "Epoch 33/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.6538 - loss: 0.9556 - val_accuracy: 0.2308 - val_loss: 2.5972\n",
      "Epoch 34/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.6731 - loss: 0.9559 - val_accuracy: 0.2308 - val_loss: 2.6223\n",
      "Epoch 35/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.6731 - loss: 0.9381 - val_accuracy: 0.2308 - val_loss: 2.5961\n",
      "Epoch 36/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.6731 - loss: 0.9486 - val_accuracy: 0.2308 - val_loss: 2.6340\n",
      "Epoch 37/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.6923 - loss: 0.9260 - val_accuracy: 0.2692 - val_loss: 2.6019\n",
      "Epoch 38/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.6538 - loss: 0.9392 - val_accuracy: 0.2308 - val_loss: 2.6422\n",
      "Epoch 39/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.6731 - loss: 0.9097 - val_accuracy: 0.2692 - val_loss: 2.6198\n",
      "Epoch 40/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.6731 - loss: 0.9253 - val_accuracy: 0.2308 - val_loss: 2.6599\n",
      "Epoch 41/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.6923 - loss: 0.8958 - val_accuracy: 0.2692 - val_loss: 2.6197\n",
      "Epoch 42/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.6731 - loss: 0.9159 - val_accuracy: 0.2308 - val_loss: 2.6667\n",
      "Epoch 43/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.7115 - loss: 0.8779 - val_accuracy: 0.2692 - val_loss: 2.6256\n",
      "Epoch 44/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.6731 - loss: 0.9109 - val_accuracy: 0.2308 - val_loss: 2.6757\n",
      "Epoch 45/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.7500 - loss: 0.8655 - val_accuracy: 0.2308 - val_loss: 2.6335\n",
      "Epoch 46/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.6923 - loss: 0.8843 - val_accuracy: 0.2308 - val_loss: 2.6882\n",
      "Epoch 47/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.7115 - loss: 0.8528 - val_accuracy: 0.2692 - val_loss: 2.6172\n",
      "Epoch 48/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.6538 - loss: 0.9185 - val_accuracy: 0.2308 - val_loss: 2.6723\n",
      "Epoch 49/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.7885 - loss: 0.8512 - val_accuracy: 0.2308 - val_loss: 2.6784\n",
      "Epoch 50/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.7885 - loss: 0.8336 - val_accuracy: 0.2308 - val_loss: 2.6679\n",
      "Epoch 51/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.7692 - loss: 0.8234 - val_accuracy: 0.2308 - val_loss: 2.7020\n",
      "Epoch 52/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.7692 - loss: 0.8210 - val_accuracy: 0.2692 - val_loss: 2.6287\n",
      "Epoch 53/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.6346 - loss: 0.9044 - val_accuracy: 0.2308 - val_loss: 2.6907\n",
      "Epoch 54/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.7885 - loss: 0.8213 - val_accuracy: 0.2308 - val_loss: 2.6921\n",
      "Epoch 55/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.7885 - loss: 0.7979 - val_accuracy: 0.2308 - val_loss: 2.6783\n",
      "Epoch 56/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.7692 - loss: 0.7970 - val_accuracy: 0.2308 - val_loss: 2.7313\n",
      "Epoch 57/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.7500 - loss: 0.7903 - val_accuracy: 0.2692 - val_loss: 2.6148\n",
      "Epoch 58/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.6538 - loss: 0.9136 - val_accuracy: 0.2308 - val_loss: 2.6771\n",
      "Epoch 59/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.7885 - loss: 0.8222 - val_accuracy: 0.2308 - val_loss: 2.7247\n",
      "Epoch 60/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7885 - loss: 0.7689 - val_accuracy: 0.2308 - val_loss: 2.6600\n",
      "Epoch 61/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7885 - loss: 0.7792 - val_accuracy: 0.2308 - val_loss: 2.7855\n",
      "Epoch 62/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.7500 - loss: 0.7707 - val_accuracy: 0.2692 - val_loss: 2.6100\n",
      "Epoch 63/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.6731 - loss: 0.8758 - val_accuracy: 0.2308 - val_loss: 2.6705\n",
      "Epoch 64/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.7885 - loss: 0.8018 - val_accuracy: 0.2308 - val_loss: 2.7029\n",
      "Epoch 65/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7885 - loss: 0.7522 - val_accuracy: 0.2308 - val_loss: 2.7003\n",
      "Epoch 66/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.8077 - loss: 0.7371 - val_accuracy: 0.2308 - val_loss: 2.7438\n",
      "Epoch 67/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.7885 - loss: 0.7295 - val_accuracy: 0.2308 - val_loss: 2.6422\n",
      "Epoch 68/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7500 - loss: 0.8063 - val_accuracy: 0.2308 - val_loss: 2.7831\n",
      "Epoch 69/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.8077 - loss: 0.7329 - val_accuracy: 0.2308 - val_loss: 2.6528\n",
      "Epoch 70/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.7692 - loss: 0.7721 - val_accuracy: 0.2308 - val_loss: 2.7464\n",
      "Epoch 71/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.8269 - loss: 0.7261 - val_accuracy: 0.2308 - val_loss: 2.6802\n",
      "Epoch 72/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.7692 - loss: 0.7470 - val_accuracy: 0.2308 - val_loss: 2.8126\n",
      "Epoch 73/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.8077 - loss: 0.7160 - val_accuracy: 0.2692 - val_loss: 2.6311\n",
      "Epoch 74/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.7115 - loss: 0.7895 - val_accuracy: 0.2308 - val_loss: 2.7243\n",
      "Epoch 75/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.8462 - loss: 0.7159 - val_accuracy: 0.2308 - val_loss: 2.7345\n",
      "Epoch 76/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.8462 - loss: 0.6793 - val_accuracy: 0.2308 - val_loss: 2.7472\n",
      "Epoch 77/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.8462 - loss: 0.6731 - val_accuracy: 0.2308 - val_loss: 2.6941\n",
      "Epoch 78/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.8462 - loss: 0.6935 - val_accuracy: 0.2308 - val_loss: 2.8557\n",
      "Epoch 79/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.7885 - loss: 0.7061 - val_accuracy: 0.2692 - val_loss: 2.6280\n",
      "Epoch 80/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.7115 - loss: 0.8296 - val_accuracy: 0.2308 - val_loss: 2.6923\n",
      "Epoch 81/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.8269 - loss: 0.7345 - val_accuracy: 0.2308 - val_loss: 2.7526\n",
      "Epoch 82/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.8462 - loss: 0.6665 - val_accuracy: 0.2308 - val_loss: 2.7572\n",
      "Epoch 83/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.8462 - loss: 0.6446 - val_accuracy: 0.2308 - val_loss: 2.7469\n",
      "Epoch 84/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.8462 - loss: 0.6367 - val_accuracy: 0.2308 - val_loss: 2.7964\n",
      "Epoch 85/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.8269 - loss: 0.6361 - val_accuracy: 0.2308 - val_loss: 2.7155\n",
      "Epoch 86/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.8077 - loss: 0.6589 - val_accuracy: 0.2308 - val_loss: 2.8852\n",
      "Epoch 87/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.7500 - loss: 0.6619 - val_accuracy: 0.2692 - val_loss: 2.6359\n",
      "Epoch 88/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.7308 - loss: 0.7829 - val_accuracy: 0.2308 - val_loss: 2.7345\n",
      "Epoch 89/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.8462 - loss: 0.6761 - val_accuracy: 0.2308 - val_loss: 2.7844\n",
      "Epoch 90/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.8462 - loss: 0.6190 - val_accuracy: 0.2308 - val_loss: 2.7669\n",
      "Epoch 91/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.8462 - loss: 0.6200 - val_accuracy: 0.2308 - val_loss: 2.8210\n",
      "Epoch 92/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.8269 - loss: 0.6032 - val_accuracy: 0.2308 - val_loss: 2.6944\n",
      "Epoch 93/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.8269 - loss: 0.6983 - val_accuracy: 0.2308 - val_loss: 2.8320\n",
      "Epoch 94/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.8269 - loss: 0.6041 - val_accuracy: 0.2308 - val_loss: 2.7334\n",
      "Epoch 95/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.8654 - loss: 0.6206 - val_accuracy: 0.2308 - val_loss: 2.8829\n",
      "Epoch 96/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.8269 - loss: 0.6232 - val_accuracy: 0.2692 - val_loss: 2.6648\n",
      "Epoch 97/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.7692 - loss: 0.7301 - val_accuracy: 0.2308 - val_loss: 2.7685\n",
      "Epoch 98/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.8462 - loss: 0.6439 - val_accuracy: 0.2308 - val_loss: 2.8022\n",
      "Epoch 99/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8462 - loss: 0.5871 - val_accuracy: 0.2308 - val_loss: 2.7938\n",
      "Epoch 100/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.8462 - loss: 0.5826 - val_accuracy: 0.2308 - val_loss: 2.8252\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1957a4ad0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=64,\n",
    "          epochs=100,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd199ca-ce8a-4bc5-94ed-6a3042410628",
   "metadata": {},
   "source": [
    "#### 6. Define Inference Models for Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c51a7f2f-f345-4022-a2eb-a232094a1a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the encoder model for inference\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Define the decoder model for inference\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb45154-93ad-4b9f-a90b-cb8e524be349",
   "metadata": {},
   "source": [
    "#### 7. Generate Sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89163449-b238-4bf6-bd77-1e0d9bd41d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "-\n",
      "Input: hello\n",
      "Decoded: olaaaaaaaaaaaa\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "-\n",
      "Input: world\n",
      "Decoded: unndaaaaaaaaaa\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "-\n",
      "Input: machine\n",
      "Decoded: áquiiaaaaaaaaa\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "-\n",
      "Input: learning\n",
      "Decoded: prennnaaaaaaaa\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "-\n",
      "Input: encoder\n",
      "Decoded: prennaaaaaaaaa\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "-\n",
      "Input: decoder\n",
      "Decoded: unnnaaaaaaaaaa\n"
     ]
    }
   ],
   "source": [
    "# some decoding errors that I need to fix for this one\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = output_characters[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length or find a stop character\n",
    "        if len(decoded_sentence) > max_decoder_seq_length:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (length 1)\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.0\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "# Test the model\n",
    "for seq_index in range(len(input_texts)):\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input:', input_texts[seq_index])\n",
    "    print('Decoded:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2bb504-2325-4405-9cfa-6feb17e8e5b7",
   "metadata": {},
   "source": [
    "### Explanation of Encoder-Decoder Architectures\n",
    "\n",
    "#### Recurrent Neural Networks (RNNs)\n",
    "\n",
    "RNNs are a type of neural network designed to handle sequential data by maintaining a hidden state that captures information about previous inputs in the sequence. However, vanilla RNNs struggle with long-term dependencies due to issues like vanishing gradients, which LSTM and GRU units aim to mitigate.\n",
    "\n",
    "#### Encoder-Decoder Architectures\n",
    "\n",
    "Encoder-Decoder architectures consist of two main components:\n",
    "- **Encoder**: Processes the input sequence and compresses the information into a fixed-length context vector (hidden state).\n",
    "- **Decoder**: Takes the context vector and generates the output sequence.\n",
    "\n",
    "These architectures are widely used in various sequence-to-sequence tasks such as machine translation, text summarization, and conversational modeling.\n",
    "\n",
    "### Mathematical Notation\n",
    "\n",
    "Given a sequence of words $ x_1, x_2, \\ldots, x_T $:\n",
    "- **Encoder**: The encoder processes each word $ x_t $ and updates its hidden state $ h_t $ using:\n",
    "  $$\n",
    "  h_t = f(h_{t-1}, x_t)\n",
    "  $$\n",
    "  Where $ f $ is the RNN function (e.g., LSTM or GRU).\n",
    "\n",
    "- **Decoder**: The decoder generates each word $ y_t $ in the output sequence using the context vector and the previous hidden state:\n",
    "  $$\n",
    "  s_t = g(s_{t-1}, y_{t-1}, c)\n",
    "  $$\n",
    "  Where $ g $ is the RNN function, $ s_t $ is the hidden state of the decoder, and $ c $ is the context vector from the encoder.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e442851e-da0a-4218-87e8-e8bbf213b4af",
   "metadata": {},
   "source": [
    "The overall goal of this notebook is to demonstrate the application of Recurrent Neural Networks (RNNs) within an Encoder-Decoder architecture for sequence-to-sequence tasks. Specifically, it showcases how to build and train a basic Encoder-Decoder model using RNNs to translate or map input sequences to output sequences.\n",
    "\n",
    "### Summary of the Notebook:\n",
    "1. **Introduction to RNNs and Encoder-Decoder Architecture**: The notebook introduces RNNs and how they are used in Encoder-Decoder architectures for tasks that involve transforming an input sequence into an output sequence, such as translation.\n",
    "\n",
    "2. **Creating a Simple Dataset**: A small dataset of simple word pairs is defined, where each input word (e.g., \"hello\") is mapped to an output word in another language (e.g., \"hola\"). This serves as a minimal example to train the Encoder-Decoder model.\n",
    "\n",
    "3. **Building the Model**: The notebook constructs an Encoder-Decoder model using Long Short-Term Memory (LSTM) units:\n",
    "   - **Encoder**: Encodes the input sequence into a fixed-length context vector.\n",
    "   - **Decoder**: Decodes this context vector to generate the output sequence.\n",
    "\n",
    "4. **Training the Model**: The model is trained on the dataset to learn how to map each input sequence to the corresponding output sequence.\n",
    "\n",
    "5. **Generating Sequences**: The trained model is used to generate or predict output sequences for given input sequences, demonstrating how the Encoder-Decoder architecture processes and produces sequences.\n",
    "\n",
    "### Key Takeaways:\n",
    "- **RNNs for Sequence Modeling**: Showcased how RNNs are effective for handling sequential data by maintaining a hidden state that captures information from previous inputs.\n",
    "- **Encoder-Decoder Architecture**: Illustrated how this architecture works for sequence-to-sequence tasks by first encoding an input sequence into a context vector and then decoding this vector to generate an output sequence.\n",
    "- **Practical Implementation**: Provided a hands-on example using TensorFlow and Keras to build, train, and use an Encoder-Decoder model for simple word-to-word translation tasks.\n",
    "\n",
    "This notebook serves as an introductory demonstration of how RNNs in Encoder-Decoder architectures can be applied to tasks such as translation, text summarization, and other sequence transformations. This notebook demonstrates how to provide a basic understanding of how these models process sequences and generate output. It builds upon the concepts of word embeddings and sequential modeling introduced by earlier models like Word2Vec."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe700c5d-57f6-4e38-bd85-e2d71e384215",
   "metadata": {},
   "source": [
    "## Alternative Examples you could create with this type of Encoder-Decoder architecture with RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417ba836-7d79-49d8-8655-449caa85482f",
   "metadata": {},
   "source": [
    "Here are some alternative examples that can be used to showcase the Encoder-Decoder architecture with RNNs. Each of these examples is designed to highlight different aspects of sequence-to-sequence learning, such as translation, transformation, or summarization.\n",
    "\n",
    "### 1. **Date Format Conversion**\n",
    "   - **Task**: Convert dates from one format to another.\n",
    "   - **Input**: \"01-01-2024\"\n",
    "   - **Output**: \"January 1, 2024\"\n",
    "   - **Purpose**: This task demonstrates how the Encoder-Decoder model can learn to map a structured input sequence to a more natural language output format.\n",
    "   - **Benefits**: \n",
    "     - Shows the model’s ability to understand and generate different sequence patterns.\n",
    "     - Useful for practical applications like data preprocessing and natural language understanding.\n",
    "\n",
    "### 2. **Math Equation to Verbal Description**\n",
    "   - **Task**: Convert simple arithmetic equations into their word forms.\n",
    "   - **Input**: \"3 + 5\"\n",
    "   - **Output**: \"three plus five equals eight\"\n",
    "   - **Purpose**: Demonstrates the model's ability to interpret and generate sequences based on arithmetic logic.\n",
    "   - **Benefits**:\n",
    "     - Highlights how sequence-to-sequence models can be used for educational tools.\n",
    "     - Illustrates the model’s capability to understand numerical context.\n",
    "\n",
    "### 3. **Reversing Sentences**\n",
    "   - **Task**: Reverse the words in a sentence.\n",
    "   - **Input**: \"The quick brown fox\"\n",
    "   - **Output**: \"fox brown quick The\"\n",
    "   - **Purpose**: A simple task to demonstrate the model’s ability to handle sequence manipulation.\n",
    "   - **Benefits**:\n",
    "     - Provides an easy-to-understand example of sequence transformation.\n",
    "     - Can be a starting point for understanding more complex tasks like summarization.\n",
    "\n",
    "### 4. **Translation of Phrases**\n",
    "   - **Task**: Translate short phrases from one language to another.\n",
    "   - **Input**: \"Good morning\"\n",
    "   - **Output**: \"Buenos días\"\n",
    "   - **Purpose**: Shows the Encoder-Decoder model’s strength in handling language translation.\n",
    "   - **Benefits**:\n",
    "     - Directly applicable to real-world use cases like language translation services.\n",
    "     - Illustrates the model's understanding of context and semantics.\n",
    "\n",
    "### 5. **Text Summarization**\n",
    "   - **Task**: Summarize longer sentences into shorter phrases.\n",
    "   - **Input**: \"The quick brown fox jumps over the lazy dog because it was feeling very energetic and playful.\"\n",
    "   - **Output**: \"Energetic fox jumps.\"\n",
    "   - **Purpose**: Demonstrates how the Encoder-Decoder architecture can be used for text summarization by learning to capture the essence of a longer text.\n",
    "   - **Benefits**:\n",
    "     - Useful for creating more advanced applications in text processing.\n",
    "     - Showcases the ability to condense information while preserving meaning.\n",
    "\n",
    "### 6. **Sequence Number Mapping**\n",
    "   - **Task**: Map a sequence of numbers to a verbal description.\n",
    "   - **Input**: \"123\"\n",
    "   - **Output**: \"one hundred twenty-three\"\n",
    "   - **Purpose**: Demonstrates how the model can handle digit-to-word conversion, useful in various applications such as voice assistants.\n",
    "   - **Benefits**:\n",
    "     - Highlights the model's ability to understand and verbalize numerical data.\n",
    "     - Useful for building interactive voice-based systems.\n",
    "\n",
    "### Selecting an Alternative Example:\n",
    "- **Complexity**: Choose an example that matches the complexity level you're comfortable with and want to demonstrate. Simple tasks like reversing sentences are easier to implement, while summarization or translation can be more complex.\n",
    "- **Application**: Consider the real-world applicability of the task. For instance, translation and date format conversion are common use cases that resonate well with practical applications.\n",
    "- **Demonstration**: If the goal is to show the model's capability in understanding context and generating coherent outputs, translation and summarization are good choices.\n",
    "\n",
    "### Recommended Example for Implementation:\n",
    "Given the previous translation example in the notebook, **Date Format Conversion** can be a suitable next step. It's simple yet effective in demonstrating how an Encoder-Decoder model can handle structured data and convert it into a natural language format, which can be particularly illustrative for those new to sequence-to-sequence modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "468fec0d-1501-4b09-923f-ab018e4e1104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff08511-1a66-4b9d-baab-b2b5528a949e",
   "metadata": {},
   "source": [
    "#### Step 2: Define the Dataset\n",
    "Let's define a small dataset of date conversions. For simplicity, we'll use a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ada9605d-d4c4-4ed8-b385-62e0adf09197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple dataset with start and end tokens\n",
    "input_dates = [\n",
    "    \"01-01-2024\",\n",
    "    \"02-14-2024\",\n",
    "    \"12-25-2023\",\n",
    "    \"07-04-2024\",\n",
    "    \"11-11-2023\",\n",
    "    \"10-31-2024\"\n",
    "]\n",
    "\n",
    "output_dates = [\n",
    "    \"<start> January 1, 2024 <end>\",\n",
    "    \"<start> February 14, 2024 <end>\",\n",
    "    \"<start> December 25, 2023 <end>\",\n",
    "    \"<start> July 4, 2024 <end>\",\n",
    "    \"<start> November 11, 2023 <end>\",\n",
    "    \"<start> October 31, 2024 <end>\"\n",
    "]\n",
    "\n",
    "# Create character sets\n",
    "input_characters = sorted(set(''.join(input_dates)))\n",
    "output_characters = sorted(set(''.join(output_dates)))\n",
    "\n",
    "# Make sure to include '<start>' and '<end>' in the character set\n",
    "output_characters.extend(['<start>', '<end>'])\n",
    "output_characters = sorted(set(output_characters))\n",
    "\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(output_characters)\n",
    "\n",
    "# Create a mapping of characters to integers\n",
    "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
    "output_token_index = dict([(char, i) for i, char in enumerate(output_characters)])\n",
    "\n",
    "reverse_output_token_index = dict((i, char) for char, i in output_token_index.items())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3064065b-2c9e-446c-bdb3-09b0525fc9db",
   "metadata": {},
   "source": [
    "#### Step 3: Preprocess the Data\n",
    "Convert the dates into a format suitable for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4eee29db-b03b-4634-badc-c714f6071476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define maximum sequence lengths\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_dates])\n",
    "max_decoder_seq_length = max([len(txt) for txt in output_dates])\n",
    "\n",
    "# Vectorize the input and output dates\n",
    "encoder_input_data = np.zeros((len(input_dates), max_encoder_seq_length, num_encoder_tokens), dtype='float32')\n",
    "decoder_input_data = np.zeros((len(output_dates), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
    "decoder_target_data = np.zeros((len(output_dates), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_dates, output_dates)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
    "    for t, char in enumerate(target_text):\n",
    "        decoder_input_data[i, t, output_token_index[char]] = 1.0\n",
    "        if t > 0:\n",
    "            decoder_target_data[i, t - 1, output_token_index[char]] = 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbd89d3-264d-4f2f-a07d-b5ff68e5072f",
   "metadata": {},
   "source": [
    "#### Step 4: Build the Encoder-Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ee7ee2c-c754-485a-aa5c-b675fa958d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "# Define the Encoder\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder_lstm = LSTM(256, return_state=True, recurrent_dropout=0.2)  # Add dropout\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Define the Decoder\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True, recurrent_dropout=0.2)  # Add dropout\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf5d262-33d5-4f76-b03a-6620096250da",
   "metadata": {},
   "source": [
    "#### Step 5: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "717d2beb-146b-454a-a969-802f2401f0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset stdout to default\n",
    "#sys.stdout = sys.__stdout__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1167c798-1613-4302-afc9-b2329fe79225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "1/1 - 2s - 2s/step - accuracy: 0.0887 - loss: 3.1540 - val_accuracy: 0.2742 - val_loss: 3.2634\n",
      "Epoch 2/500\n",
      "1/1 - 0s - 83ms/step - accuracy: 0.2903 - loss: 3.1204 - val_accuracy: 0.2419 - val_loss: 3.2384\n",
      "Epoch 3/500\n",
      "1/1 - 0s - 88ms/step - accuracy: 0.2823 - loss: 3.0914 - val_accuracy: 0.2419 - val_loss: 3.2096\n",
      "Epoch 4/500\n",
      "1/1 - 0s - 81ms/step - accuracy: 0.2823 - loss: 3.0568 - val_accuracy: 0.2419 - val_loss: 3.1654\n",
      "Epoch 5/500\n",
      "1/1 - 0s - 78ms/step - accuracy: 0.2823 - loss: 3.0016 - val_accuracy: 0.2419 - val_loss: 3.0632\n",
      "Epoch 6/500\n",
      "1/1 - 0s - 79ms/step - accuracy: 0.2823 - loss: 2.8770 - val_accuracy: 0.2097 - val_loss: 2.9617\n",
      "Epoch 7/500\n",
      "1/1 - 0s - 79ms/step - accuracy: 0.2500 - loss: 2.7295 - val_accuracy: 0.1613 - val_loss: 2.9609\n",
      "Epoch 8/500\n",
      "1/1 - 0s - 78ms/step - accuracy: 0.1774 - loss: 2.7455 - val_accuracy: 0.1935 - val_loss: 2.8711\n",
      "Epoch 9/500\n",
      "1/1 - 0s - 79ms/step - accuracy: 0.1935 - loss: 2.6065 - val_accuracy: 0.2097 - val_loss: 2.8179\n",
      "Epoch 10/500\n",
      "1/1 - 0s - 78ms/step - accuracy: 0.2419 - loss: 2.5566 - val_accuracy: 0.1613 - val_loss: 2.7716\n",
      "Epoch 11/500\n",
      "1/1 - 0s - 74ms/step - accuracy: 0.1694 - loss: 2.4760 - val_accuracy: 0.2258 - val_loss: 2.7807\n",
      "Epoch 12/500\n",
      "1/1 - 0s - 76ms/step - accuracy: 0.2500 - loss: 2.5149 - val_accuracy: 0.1290 - val_loss: 2.8768\n",
      "Epoch 13/500\n",
      "1/1 - 0s - 76ms/step - accuracy: 0.2016 - loss: 2.5629 - val_accuracy: 0.2097 - val_loss: 2.7384\n",
      "Epoch 14/500\n",
      "1/1 - 0s - 74ms/step - accuracy: 0.2581 - loss: 2.3855 - val_accuracy: 0.2419 - val_loss: 2.6610\n",
      "Epoch 15/500\n",
      "1/1 - 0s - 77ms/step - accuracy: 0.2339 - loss: 2.3175 - val_accuracy: 0.1774 - val_loss: 2.6226\n",
      "Epoch 16/500\n",
      "1/1 - 0s - 81ms/step - accuracy: 0.1694 - loss: 2.2405 - val_accuracy: 0.2258 - val_loss: 2.6016\n",
      "Epoch 17/500\n",
      "1/1 - 0s - 77ms/step - accuracy: 0.2339 - loss: 2.2116 - val_accuracy: 0.2258 - val_loss: 2.5427\n",
      "Epoch 18/500\n",
      "1/1 - 0s - 76ms/step - accuracy: 0.2339 - loss: 2.1644 - val_accuracy: 0.2419 - val_loss: 2.5573\n",
      "Epoch 19/500\n",
      "1/1 - 0s - 76ms/step - accuracy: 0.2258 - loss: 2.1491 - val_accuracy: 0.1935 - val_loss: 2.5745\n",
      "Epoch 20/500\n",
      "1/1 - 0s - 81ms/step - accuracy: 0.2016 - loss: 2.2251 - val_accuracy: 0.1774 - val_loss: 2.7750\n",
      "Epoch 21/500\n",
      "1/1 - 0s - 75ms/step - accuracy: 0.2177 - loss: 2.2850 - val_accuracy: 0.3065 - val_loss: 2.5243\n",
      "Epoch 22/500\n",
      "1/1 - 0s - 76ms/step - accuracy: 0.2984 - loss: 2.0918 - val_accuracy: 0.2903 - val_loss: 2.5190\n",
      "Epoch 23/500\n",
      "1/1 - 0s - 78ms/step - accuracy: 0.2823 - loss: 2.0490 - val_accuracy: 0.2903 - val_loss: 2.4486\n",
      "Epoch 24/500\n",
      "1/1 - 0s - 75ms/step - accuracy: 0.2903 - loss: 2.0457 - val_accuracy: 0.2258 - val_loss: 2.6672\n",
      "Epoch 25/500\n",
      "1/1 - 0s - 74ms/step - accuracy: 0.2500 - loss: 2.1195 - val_accuracy: 0.2742 - val_loss: 2.4773\n",
      "Epoch 26/500\n",
      "1/1 - 0s - 73ms/step - accuracy: 0.2500 - loss: 2.0333 - val_accuracy: 0.2742 - val_loss: 2.5873\n",
      "Epoch 27/500\n",
      "1/1 - 0s - 75ms/step - accuracy: 0.3226 - loss: 2.0310 - val_accuracy: 0.3548 - val_loss: 2.4056\n",
      "Epoch 28/500\n",
      "1/1 - 0s - 79ms/step - accuracy: 0.3226 - loss: 1.9576 - val_accuracy: 0.2419 - val_loss: 2.5469\n",
      "Epoch 29/500\n",
      "1/1 - 0s - 78ms/step - accuracy: 0.3065 - loss: 1.9835 - val_accuracy: 0.2903 - val_loss: 2.3909\n",
      "Epoch 30/500\n",
      "1/1 - 0s - 75ms/step - accuracy: 0.3065 - loss: 1.9604 - val_accuracy: 0.2097 - val_loss: 2.5491\n",
      "Epoch 31/500\n",
      "1/1 - 0s - 77ms/step - accuracy: 0.2984 - loss: 1.9562 - val_accuracy: 0.3065 - val_loss: 2.3664\n",
      "Epoch 32/500\n",
      "1/1 - 0s - 75ms/step - accuracy: 0.3065 - loss: 1.9640 - val_accuracy: 0.2097 - val_loss: 2.6134\n",
      "Epoch 33/500\n",
      "1/1 - 0s - 78ms/step - accuracy: 0.3065 - loss: 1.9737 - val_accuracy: 0.3548 - val_loss: 2.3340\n",
      "Epoch 34/500\n",
      "1/1 - 0s - 76ms/step - accuracy: 0.3226 - loss: 1.8794 - val_accuracy: 0.2581 - val_loss: 2.5036\n",
      "Epoch 35/500\n",
      "1/1 - 0s - 76ms/step - accuracy: 0.2581 - loss: 1.9236 - val_accuracy: 0.3387 - val_loss: 2.3182\n",
      "Epoch 36/500\n",
      "1/1 - 0s - 81ms/step - accuracy: 0.3226 - loss: 1.8262 - val_accuracy: 0.2581 - val_loss: 2.4436\n",
      "Epoch 37/500\n",
      "1/1 - 0s - 74ms/step - accuracy: 0.3145 - loss: 1.8411 - val_accuracy: 0.3710 - val_loss: 2.2997\n",
      "Epoch 38/500\n",
      "1/1 - 0s - 75ms/step - accuracy: 0.3468 - loss: 1.8211 - val_accuracy: 0.2419 - val_loss: 2.5221\n",
      "Epoch 39/500\n",
      "1/1 - 0s - 75ms/step - accuracy: 0.2984 - loss: 1.8122 - val_accuracy: 0.3871 - val_loss: 2.2571\n",
      "Epoch 40/500\n",
      "1/1 - 0s - 76ms/step - accuracy: 0.3226 - loss: 1.7724 - val_accuracy: 0.2419 - val_loss: 2.5259\n",
      "Epoch 41/500\n",
      "1/1 - 0s - 74ms/step - accuracy: 0.3065 - loss: 1.8418 - val_accuracy: 0.3548 - val_loss: 2.2917\n",
      "Epoch 42/500\n",
      "1/1 - 0s - 75ms/step - accuracy: 0.2742 - loss: 1.8898 - val_accuracy: 0.1613 - val_loss: 2.7531\n",
      "Epoch 43/500\n",
      "1/1 - 0s - 76ms/step - accuracy: 0.2177 - loss: 2.1389 - val_accuracy: 0.3065 - val_loss: 2.2746\n",
      "Epoch 44/500\n",
      "1/1 - 0s - 75ms/step - accuracy: 0.3387 - loss: 1.7497 - val_accuracy: 0.3548 - val_loss: 2.3240\n",
      "Epoch 45/500\n",
      "1/1 - 0s - 74ms/step - accuracy: 0.4032 - loss: 1.7252 - val_accuracy: 0.4032 - val_loss: 2.2321\n",
      "Epoch 46/500\n",
      "1/1 - 0s - 75ms/step - accuracy: 0.3871 - loss: 1.7276 - val_accuracy: 0.2903 - val_loss: 2.3904\n",
      "Epoch 47/500\n",
      "1/1 - 0s - 73ms/step - accuracy: 0.3306 - loss: 1.7019 - val_accuracy: 0.4839 - val_loss: 2.2047\n",
      "Epoch 48/500\n",
      "1/1 - 0s - 78ms/step - accuracy: 0.4032 - loss: 1.6663 - val_accuracy: 0.2258 - val_loss: 2.4767\n",
      "Epoch 49/500\n",
      "1/1 - 0s - 77ms/step - accuracy: 0.3387 - loss: 1.7656 - val_accuracy: 0.4032 - val_loss: 2.1976\n",
      "Epoch 50/500\n",
      "1/1 - 0s - 74ms/step - accuracy: 0.2984 - loss: 1.7240 - val_accuracy: 0.2581 - val_loss: 2.5753\n",
      "Epoch 51/500\n",
      "1/1 - 0s - 75ms/step - accuracy: 0.3387 - loss: 1.8624 - val_accuracy: 0.4194 - val_loss: 2.1938\n",
      "Epoch 52/500\n",
      "1/1 - 0s - 78ms/step - accuracy: 0.3387 - loss: 1.7374 - val_accuracy: 0.2258 - val_loss: 2.4823\n",
      "Epoch 53/500\n",
      "1/1 - 0s - 82ms/step - accuracy: 0.3629 - loss: 1.7107 - val_accuracy: 0.4839 - val_loss: 2.1606\n",
      "Epoch 54/500\n",
      "1/1 - 0s - 78ms/step - accuracy: 0.4032 - loss: 1.6383 - val_accuracy: 0.3065 - val_loss: 2.3962\n",
      "Epoch 55/500\n",
      "1/1 - 0s - 77ms/step - accuracy: 0.3790 - loss: 1.7384 - val_accuracy: 0.3871 - val_loss: 2.1974\n",
      "Epoch 56/500\n",
      "1/1 - 0s - 77ms/step - accuracy: 0.3065 - loss: 1.7323 - val_accuracy: 0.2742 - val_loss: 2.4824\n",
      "Epoch 57/500\n",
      "1/1 - 0s - 74ms/step - accuracy: 0.3629 - loss: 1.7286 - val_accuracy: 0.5323 - val_loss: 2.1708\n",
      "Epoch 58/500\n",
      "1/1 - 0s - 74ms/step - accuracy: 0.4435 - loss: 1.5784 - val_accuracy: 0.3871 - val_loss: 2.2942\n",
      "Epoch 59/500\n",
      "1/1 - 0s - 77ms/step - accuracy: 0.4597 - loss: 1.5487 - val_accuracy: 0.5000 - val_loss: 2.1328\n",
      "Epoch 60/500\n",
      "1/1 - 0s - 75ms/step - accuracy: 0.4032 - loss: 1.5947 - val_accuracy: 0.3065 - val_loss: 2.5072\n",
      "Epoch 61/500\n",
      "1/1 - 0s - 77ms/step - accuracy: 0.3548 - loss: 1.7203 - val_accuracy: 0.4677 - val_loss: 2.1695\n",
      "Epoch 62/500\n",
      "1/1 - 0s - 76ms/step - accuracy: 0.4435 - loss: 1.6947 - val_accuracy: 0.2742 - val_loss: 2.4982\n",
      "Epoch 63/500\n",
      "1/1 - 0s - 77ms/step - accuracy: 0.3065 - loss: 1.7970 - val_accuracy: 0.4194 - val_loss: 2.1431\n",
      "Epoch 64/500\n",
      "1/1 - 0s - 75ms/step - accuracy: 0.3468 - loss: 1.7087 - val_accuracy: 0.2258 - val_loss: 2.5264\n",
      "Epoch 65/500\n",
      "1/1 - 0s - 76ms/step - accuracy: 0.3306 - loss: 1.7528 - val_accuracy: 0.4839 - val_loss: 2.1422\n",
      "Epoch 66/500\n",
      "1/1 - 0s - 78ms/step - accuracy: 0.4274 - loss: 1.5863 - val_accuracy: 0.3387 - val_loss: 2.2427\n",
      "Epoch 67/500\n",
      "1/1 - 0s - 75ms/step - accuracy: 0.4194 - loss: 1.5672 - val_accuracy: 0.4355 - val_loss: 2.1495\n",
      "Epoch 68/500\n",
      "1/1 - 0s - 80ms/step - accuracy: 0.4677 - loss: 1.5049 - val_accuracy: 0.5000 - val_loss: 2.0997\n",
      "Epoch 69/500\n",
      "1/1 - 0s - 74ms/step - accuracy: 0.4194 - loss: 1.5352 - val_accuracy: 0.3548 - val_loss: 2.2260\n",
      "Epoch 70/500\n",
      "1/1 - 0s - 76ms/step - accuracy: 0.4355 - loss: 1.5169 - val_accuracy: 0.5000 - val_loss: 2.1145\n",
      "Epoch 71/500\n",
      "1/1 - 0s - 77ms/step - accuracy: 0.4435 - loss: 1.5953 - val_accuracy: 0.2742 - val_loss: 2.6018\n",
      "Epoch 72/500\n",
      "1/1 - 0s - 76ms/step - accuracy: 0.3306 - loss: 1.8015 - val_accuracy: 0.4194 - val_loss: 2.1546\n",
      "Epoch 73/500\n",
      "1/1 - 0s - 74ms/step - accuracy: 0.4194 - loss: 1.5779 - val_accuracy: 0.3387 - val_loss: 2.2646\n",
      "Epoch 74/500\n",
      "1/1 - 0s - 77ms/step - accuracy: 0.4355 - loss: 1.5088 - val_accuracy: 0.4677 - val_loss: 2.1007\n",
      "Epoch 75/500\n",
      "1/1 - 0s - 76ms/step - accuracy: 0.4435 - loss: 1.4520 - val_accuracy: 0.4677 - val_loss: 2.1005\n",
      "Epoch 76/500\n",
      "1/1 - 0s - 77ms/step - accuracy: 0.4597 - loss: 1.4404 - val_accuracy: 0.5000 - val_loss: 2.0948\n",
      "Epoch 77/500\n",
      "1/1 - 0s - 74ms/step - accuracy: 0.4597 - loss: 1.4643 - val_accuracy: 0.3710 - val_loss: 2.1195\n",
      "Epoch 78/500\n",
      "1/1 - 0s - 76ms/step - accuracy: 0.4758 - loss: 1.4168 - val_accuracy: 0.5484 - val_loss: 2.0685\n",
      "Epoch 79/500\n",
      "1/1 - 0s - 79ms/step - accuracy: 0.4274 - loss: 1.4739 - val_accuracy: 0.2581 - val_loss: 2.6831\n",
      "Epoch 80/500\n",
      "1/1 - 0s - 78ms/step - accuracy: 0.3145 - loss: 1.8492 - val_accuracy: 0.4355 - val_loss: 2.1370\n",
      "Epoch 81/500\n",
      "1/1 - 0s - 74ms/step - accuracy: 0.3952 - loss: 1.5803 - val_accuracy: 0.3065 - val_loss: 2.2883\n",
      "Epoch 82/500\n",
      "1/1 - 0s - 78ms/step - accuracy: 0.3952 - loss: 1.5357 - val_accuracy: 0.4839 - val_loss: 2.0470\n",
      "Epoch 83/500\n",
      "1/1 - 0s - 75ms/step - accuracy: 0.4677 - loss: 1.4847 - val_accuracy: 0.3710 - val_loss: 2.2040\n",
      "Epoch 84/500\n",
      "1/1 - 0s - 76ms/step - accuracy: 0.4435 - loss: 1.4233 - val_accuracy: 0.5000 - val_loss: 2.0505\n",
      "Epoch 85/500\n",
      "1/1 - 0s - 76ms/step - accuracy: 0.4597 - loss: 1.4330 - val_accuracy: 0.3226 - val_loss: 2.4849\n",
      "Epoch 86/500\n",
      "1/1 - 0s - 78ms/step - accuracy: 0.4032 - loss: 1.6264 - val_accuracy: 0.4355 - val_loss: 2.0989\n",
      "Epoch 87/500\n",
      "1/1 - 0s - 75ms/step - accuracy: 0.3387 - loss: 1.5261 - val_accuracy: 0.3065 - val_loss: 2.2517\n",
      "Epoch 88/500\n",
      "1/1 - 0s - 75ms/step - accuracy: 0.4032 - loss: 1.5276 - val_accuracy: 0.4355 - val_loss: 2.1355\n",
      "Epoch 89/500\n",
      "1/1 - 0s - 75ms/step - accuracy: 0.3871 - loss: 1.6642 - val_accuracy: 0.2742 - val_loss: 2.5906\n",
      "Epoch 90/500\n",
      "1/1 - 0s - 74ms/step - accuracy: 0.3306 - loss: 1.7534 - val_accuracy: 0.4355 - val_loss: 2.0758\n",
      "Epoch 91/500\n",
      "1/1 - 0s - 75ms/step - accuracy: 0.4516 - loss: 1.4762 - val_accuracy: 0.4032 - val_loss: 2.1008\n",
      "Epoch 92/500\n",
      "1/1 - 0s - 76ms/step - accuracy: 0.5242 - loss: 1.4076 - val_accuracy: 0.5161 - val_loss: 2.0131\n",
      "Epoch 93/500\n",
      "1/1 - 0s - 74ms/step - accuracy: 0.4919 - loss: 1.3844 - val_accuracy: 0.4355 - val_loss: 2.0784\n",
      "Epoch 94/500\n",
      "1/1 - 0s - 74ms/step - accuracy: 0.4355 - loss: 1.4026 - val_accuracy: 0.4839 - val_loss: 2.0180\n",
      "Epoch 95/500\n",
      "1/1 - 0s - 75ms/step - accuracy: 0.5081 - loss: 1.3488 - val_accuracy: 0.3548 - val_loss: 2.2301\n",
      "Epoch 96/500\n",
      "1/1 - 0s - 77ms/step - accuracy: 0.4274 - loss: 1.4343 - val_accuracy: 0.4355 - val_loss: 2.0855\n",
      "Epoch 97/500\n",
      "1/1 - 0s - 76ms/step - accuracy: 0.3952 - loss: 1.5268 - val_accuracy: 0.2903 - val_loss: 2.5247\n",
      "Epoch 98/500\n",
      "1/1 - 0s - 83ms/step - accuracy: 0.3548 - loss: 1.6735 - val_accuracy: 0.5000 - val_loss: 2.0438\n",
      "Epoch 99/500\n",
      "1/1 - 0s - 77ms/step - accuracy: 0.4919 - loss: 1.3994 - val_accuracy: 0.3548 - val_loss: 2.1529\n",
      "Epoch 100/500\n",
      "1/1 - 0s - 79ms/step - accuracy: 0.4435 - loss: 1.3778 - val_accuracy: 0.5484 - val_loss: 1.9957\n",
      "Epoch 101/500\n",
      "1/1 - 0s - 75ms/step - accuracy: 0.5081 - loss: 1.3258 - val_accuracy: 0.3548 - val_loss: 2.1523\n",
      "Epoch 102/500\n",
      "1/1 - 0s - 77ms/step - accuracy: 0.4597 - loss: 1.3893 - val_accuracy: 0.4677 - val_loss: 2.0480\n",
      "Epoch 103/500\n",
      "1/1 - 0s - 77ms/step - accuracy: 0.5000 - loss: 1.4660 - val_accuracy: 0.3226 - val_loss: 2.3867\n",
      "Epoch 104/500\n",
      "1/1 - 0s - 75ms/step - accuracy: 0.3952 - loss: 1.5394 - val_accuracy: 0.5323 - val_loss: 2.0360\n",
      "Epoch 105/500\n",
      "1/1 - 0s - 76ms/step - accuracy: 0.4839 - loss: 1.4092 - val_accuracy: 0.3387 - val_loss: 2.2232\n",
      "Epoch 106/500\n",
      "1/1 - 0s - 78ms/step - accuracy: 0.4355 - loss: 1.4109 - val_accuracy: 0.5161 - val_loss: 1.9824\n",
      "Epoch 107/500\n",
      "1/1 - 0s - 76ms/step - accuracy: 0.5000 - loss: 1.3400 - val_accuracy: 0.3387 - val_loss: 2.2043\n",
      "Epoch 108/500\n",
      "1/1 - 0s - 76ms/step - accuracy: 0.4435 - loss: 1.3769 - val_accuracy: 0.5484 - val_loss: 1.9904\n",
      "Epoch 109/500\n",
      "1/1 - 0s - 75ms/step - accuracy: 0.4839 - loss: 1.3640 - val_accuracy: 0.3548 - val_loss: 2.1930\n",
      "Epoch 110/500\n",
      "1/1 - 0s - 75ms/step - accuracy: 0.4435 - loss: 1.3795 - val_accuracy: 0.4677 - val_loss: 2.0665\n",
      "Epoch 111/500\n",
      "1/1 - 0s - 75ms/step - accuracy: 0.3790 - loss: 1.5012 - val_accuracy: 0.2903 - val_loss: 2.6016\n",
      "Epoch 112/500\n",
      "1/1 - 0s - 121ms/step - accuracy: 0.3710 - loss: 1.6509 - val_accuracy: 0.5000 - val_loss: 1.9834\n",
      "Epoch 113/500\n",
      "1/1 - 0s - 91ms/step - accuracy: 0.5323 - loss: 1.3201 - val_accuracy: 0.5161 - val_loss: 1.9605\n",
      "Epoch 114/500\n",
      "1/1 - 0s - 83ms/step - accuracy: 0.4677 - loss: 1.3196 - val_accuracy: 0.4516 - val_loss: 2.0082\n",
      "Epoch 115/500\n",
      "1/1 - 0s - 76ms/step - accuracy: 0.4839 - loss: 1.2638 - val_accuracy: 0.5323 - val_loss: 1.9495\n",
      "Epoch 116/500\n",
      "1/1 - 0s - 75ms/step - accuracy: 0.5242 - loss: 1.2583 - val_accuracy: 0.3226 - val_loss: 2.2790\n",
      "Epoch 117/500\n",
      "1/1 - 0s - 77ms/step - accuracy: 0.4597 - loss: 1.3049 - val_accuracy: 0.5484 - val_loss: 1.9455\n",
      "Epoch 118/500\n",
      "1/1 - 0s - 76ms/step - accuracy: 0.4919 - loss: 1.2735 - val_accuracy: 0.3226 - val_loss: 2.3593\n",
      "Epoch 119/500\n",
      "1/1 - 0s - 76ms/step - accuracy: 0.4274 - loss: 1.4286 - val_accuracy: 0.4677 - val_loss: 2.0166\n",
      "Epoch 120/500\n",
      "1/1 - 0s - 78ms/step - accuracy: 0.4597 - loss: 1.4159 - val_accuracy: 0.3226 - val_loss: 2.2829\n",
      "Epoch 121/500\n",
      "1/1 - 0s - 78ms/step - accuracy: 0.4355 - loss: 1.3948 - val_accuracy: 0.5484 - val_loss: 1.9355\n",
      "Epoch 122/500\n",
      "1/1 - 0s - 75ms/step - accuracy: 0.4919 - loss: 1.2938 - val_accuracy: 0.3548 - val_loss: 2.0738\n",
      "Epoch 123/500\n",
      "1/1 - 0s - 78ms/step - accuracy: 0.5161 - loss: 1.2701 - val_accuracy: 0.5806 - val_loss: 1.9071\n",
      "Epoch 124/500\n",
      "1/1 - 0s - 74ms/step - accuracy: 0.5484 - loss: 1.2801 - val_accuracy: 0.3226 - val_loss: 2.2553\n",
      "Epoch 125/500\n",
      "1/1 - 0s - 75ms/step - accuracy: 0.4113 - loss: 1.4475 - val_accuracy: 0.5000 - val_loss: 2.0003\n",
      "Epoch 126/500\n",
      "1/1 - 0s - 75ms/step - accuracy: 0.4597 - loss: 1.4012 - val_accuracy: 0.3387 - val_loss: 2.2135\n",
      "Epoch 127/500\n",
      "1/1 - 0s - 75ms/step - accuracy: 0.4597 - loss: 1.3712 - val_accuracy: 0.5484 - val_loss: 1.9257\n",
      "Epoch 128/500\n",
      "1/1 - 0s - 77ms/step - accuracy: 0.5323 - loss: 1.2361 - val_accuracy: 0.3548 - val_loss: 2.0904\n",
      "Epoch 129/500\n",
      "1/1 - 0s - 85ms/step - accuracy: 0.4839 - loss: 1.2515 - val_accuracy: 0.5645 - val_loss: 1.8936\n",
      "Epoch 130/500\n",
      "1/1 - 0s - 76ms/step - accuracy: 0.5081 - loss: 1.2558 - val_accuracy: 0.3226 - val_loss: 2.2283\n",
      "Epoch 131/500\n",
      "1/1 - 0s - 81ms/step - accuracy: 0.4597 - loss: 1.3754 - val_accuracy: 0.5484 - val_loss: 1.9693\n",
      "Epoch 132/500\n",
      "1/1 - 0s - 75ms/step - accuracy: 0.4516 - loss: 1.3379 - val_accuracy: 0.3226 - val_loss: 2.2456\n",
      "Epoch 133/500\n",
      "1/1 - 0s - 75ms/step - accuracy: 0.4435 - loss: 1.4090 - val_accuracy: 0.5161 - val_loss: 1.9461\n",
      "Epoch 134/500\n",
      "1/1 - 0s - 75ms/step - accuracy: 0.5161 - loss: 1.3253 - val_accuracy: 0.3387 - val_loss: 2.0960\n",
      "Epoch 135/500\n",
      "1/1 - 0s - 79ms/step - accuracy: 0.4597 - loss: 1.3219 - val_accuracy: 0.5645 - val_loss: 1.8670\n",
      "Epoch 136/500\n",
      "1/1 - 0s - 74ms/step - accuracy: 0.5000 - loss: 1.2295 - val_accuracy: 0.4032 - val_loss: 1.9583\n",
      "Epoch 137/500\n",
      "1/1 - 0s - 75ms/step - accuracy: 0.4597 - loss: 1.2049 - val_accuracy: 0.5645 - val_loss: 1.8740\n",
      "Epoch 138/500\n",
      "1/1 - 0s - 74ms/step - accuracy: 0.5161 - loss: 1.2242 - val_accuracy: 0.3226 - val_loss: 2.2005\n",
      "Epoch 139/500\n",
      "1/1 - 0s - 80ms/step - accuracy: 0.4758 - loss: 1.2699 - val_accuracy: 0.5645 - val_loss: 1.8715\n",
      "Epoch 140/500\n",
      "1/1 - 0s - 75ms/step - accuracy: 0.5726 - loss: 1.1614 - val_accuracy: 0.3871 - val_loss: 1.9759\n",
      "Epoch 141/500\n",
      "1/1 - 0s - 88ms/step - accuracy: 0.5645 - loss: 1.1294 - val_accuracy: 0.5645 - val_loss: 1.8595\n",
      "Epoch 142/500\n",
      "1/1 - 0s - 78ms/step - accuracy: 0.5565 - loss: 1.1285 - val_accuracy: 0.5806 - val_loss: 1.8472\n",
      "Epoch 143/500\n",
      "1/1 - 0s - 75ms/step - accuracy: 0.5323 - loss: 1.1458 - val_accuracy: 0.3548 - val_loss: 2.0857\n",
      "Epoch 144/500\n",
      "1/1 - 0s - 78ms/step - accuracy: 0.4677 - loss: 1.2289 - val_accuracy: 0.4516 - val_loss: 2.0003\n",
      "Epoch 145/500\n",
      "1/1 - 0s - 75ms/step - accuracy: 0.4032 - loss: 1.3758 - val_accuracy: 0.2903 - val_loss: 2.6105\n",
      "Epoch 146/500\n",
      "1/1 - 0s - 75ms/step - accuracy: 0.3306 - loss: 1.6240 - val_accuracy: 0.5645 - val_loss: 1.8778\n",
      "Epoch 147/500\n",
      "1/1 - 0s - 75ms/step - accuracy: 0.5323 - loss: 1.2305 - val_accuracy: 0.4516 - val_loss: 1.9085\n",
      "Epoch 148/500\n",
      "1/1 - 0s - 77ms/step - accuracy: 0.5000 - loss: 1.1731 - val_accuracy: 0.5484 - val_loss: 1.8245\n",
      "Epoch 149/500\n",
      "1/1 - 0s - 76ms/step - accuracy: 0.5323 - loss: 1.1403 - val_accuracy: 0.4032 - val_loss: 1.9893\n",
      "Epoch 150/500\n",
      "1/1 - 0s - 73ms/step - accuracy: 0.5242 - loss: 1.1738 - val_accuracy: 0.5968 - val_loss: 1.8338\n",
      "Epoch 151/500\n",
      "1/1 - 0s - 74ms/step - accuracy: 0.6290 - loss: 1.0734 - val_accuracy: 0.3871 - val_loss: 1.9754\n",
      "Epoch 152/500\n",
      "1/1 - 0s - 75ms/step - accuracy: 0.4839 - loss: 1.1847 - val_accuracy: 0.4677 - val_loss: 1.9176\n",
      "Epoch 153/500\n",
      "1/1 - 0s - 77ms/step - accuracy: 0.5242 - loss: 1.1964 - val_accuracy: 0.3226 - val_loss: 2.1810\n",
      "Epoch 154/500\n",
      "1/1 - 0s - 78ms/step - accuracy: 0.4597 - loss: 1.2911 - val_accuracy: 0.5161 - val_loss: 1.8432\n",
      "Epoch 155/500\n",
      "1/1 - 0s - 76ms/step - accuracy: 0.6129 - loss: 1.1229 - val_accuracy: 0.5000 - val_loss: 1.9262\n",
      "Epoch 156/500\n",
      "1/1 - 0s - 75ms/step - accuracy: 0.5565 - loss: 1.1060 - val_accuracy: 0.5161 - val_loss: 1.8461\n",
      "Epoch 157/500\n",
      "1/1 - 0s - 80ms/step - accuracy: 0.5323 - loss: 1.1035 - val_accuracy: 0.3548 - val_loss: 2.0872\n",
      "Epoch 158/500\n",
      "1/1 - 0s - 75ms/step - accuracy: 0.4839 - loss: 1.2013 - val_accuracy: 0.4032 - val_loss: 1.9587\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1a42101d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "#%%capture training_output\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=64,\n",
    "          epochs=500,  # Set a higher maximum but rely on early stopping\n",
    "          validation_split=0.2,\n",
    "          verbose=2,  # One line per epoch\n",
    "          callbacks=[early_stopping])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86f749a5-28cf-4647-94e4-6eda750538fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out results of training\n",
    "#print(training_output.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f182750-a3d4-4103-8a8a-832a68624490",
   "metadata": {},
   "source": [
    "#### Step 6: Define Inference Models for Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d65eb754-0458-4a78-833d-4b76600660e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the encoder model for inference\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Define the decoder model for inference\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5a8394-4d55-428d-8cac-d15ee0f09054",
   "metadata": {},
   "source": [
    "#### Step 7: Generate Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98139213-56f4-4505-bfde-d8dad78fc7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "-\n",
      "Input: 01-01-2024\n",
      "Decoded: start> Jeeuarry 1, 2222 <eend>>>>>>>>>>>>>>>>>>>>>>\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "-\n",
      "Input: 02-14-2024\n",
      "Decoded: start> Jeeuarry 1, 2222 <eend>>>>>>>>>>>>>>>>>>>>>>\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "-\n",
      "Input: 12-25-2023\n",
      "Decoded: start>  eeebrrr  , 22222 <eed>>>>>>>>>>>>>>>>>>>>>>\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "-\n",
      "Input: 07-04-2024\n",
      "Decoded: start> Jeuarry  , 2222 <eend>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "-\n",
      "Input: 11-11-2023\n",
      "Decoded: start> Jeeuarry 1, 2222 <eend>>>>>>>>>>>>>>>>>>>>>>\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "-\n",
      "Input: 10-31-2024\n",
      "Decoded: start> Jeeuarry 1, 2222 <eend>>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    }
   ],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1 with the start token\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    target_seq[0, 0, output_token_index['<start>']] = 1.0\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    max_decoder_seq_length = 50  # Set a maximum length for the decoded sequence\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_output_token_index[sampled_token_index]\n",
    "\n",
    "        # Check for end token\n",
    "        if sampled_char == '<end>':\n",
    "            stop_condition = True\n",
    "        else:\n",
    "            decoded_sentence += sampled_char\n",
    "\n",
    "        # Break the loop if it reaches max length\n",
    "        if len(decoded_sentence) > max_decoder_seq_length:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (length 1)\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.0\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence.strip()  # Strip any leading/trailing whitespace\n",
    "\n",
    "# Test the model\n",
    "for seq_index in range(len(input_dates)):\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input:', input_dates[seq_index])\n",
    "    print('Decoded:', decoded_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8477655e-9297-4492-a80a-7b65d0007684",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "- Data Preparation: The dates are encoded into one-hot vectors for both input and output.\n",
    "- Model Training: We train an LSTM-based Encoder-Decoder model to learn the mapping between the date formats.\n",
    "- Inference: We use the trained model to decode and predict the natural language format of the dates.\n",
    "\n",
    "### Conclusion:\n",
    "This notebook demonstrates how an Encoder-Decoder architecture can be used for the task of converting dates from a numerical format to a more natural language format. It highlights the model's ability to learn and generate sequences, showcasing the power of RNN-based Encoder-Decoder models for sequence-to-sequence tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
