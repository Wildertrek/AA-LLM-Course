{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ade694cb-dbd4-4d40-a4f3-ab9885190570",
   "metadata": {},
   "source": [
    "# History of Neural Networks\n",
    "\n",
    "Neural networks have a rich history, beginning with early theoretical foundations and evolving into the powerful machine learning models we use today.\n",
    "\n",
    "1. **1940s-1950s: The Birth of Neural Networks**  \n",
    "   The concept of neural networks was inspired by the human brain. In 1943, Warren McCulloch and Walter Pitts created a mathematical model of neurons, proposing that neurons are binary devices that can compute functions. This was one of the earliest attempts to mimic brain function using a computational model.\n",
    "\n",
    "2. **1958: Perceptron**  \n",
    "   In 1958, Frank Rosenblatt introduced the *Perceptron*, a simple neural network with one layer that could solve linearly separable problems. It was the first algorithm designed for supervised learning and became the basis for early neural network research.\n",
    "\n",
    "3. **1960s-1970s: Limitations and Winter**  \n",
    "   Neural networks faced a major setback when Marvin Minsky and Seymour Papert published *Perceptrons* (1969), proving the limitations of single-layer Perceptrons. They could not solve non-linearly separable problems, like the XOR problem. This caused a decline in interest, leading to an \"AI winter\" for neural network research. ```Winter of Neural Network Discontent -- hehe```\n",
    "\n",
    "4. **1980s: Backpropagation and Revival**  \n",
    "   In 1986, Geoffrey Hinton, David Rumelhart, and Ronald Williams popularized the *Backpropagation* algorithm, which allowed multi-layer neural networks (also known as multi-layer perceptrons, MLPs) to train effectively. This breakthrough solved many complex problems and reignited interest in neural networks.\n",
    "\n",
    "5. **1990s-2000s: Deep Learning Foundations**  \n",
    "   Neural networks began showing promise in various fields, but the computational resources were still a limiting factor. Researchers like Yann LeCun made significant progress in applying neural networks, particularly convolutional neural networks (CNNs), to tasks such as image recognition, notably using them for handwritten digit recognition (LeNet). ```Discuss RNNs```\n",
    "\n",
    "6. **2010s: Deep Learning Era**  \n",
    "   The 2010s marked a significant boom for neural networks, particularly deep learning, thanks to advancements in computational power (GPUs) and the availability of large datasets. Networks like *AlexNet* (2012) showed the immense power of deep neural networks in image classification, and models such as *ResNet*, *Transformer*, and *GPT* further advanced the field in various domains, from vision to natural language processing.\n",
    "\n",
    "7. **2020s: LLMs and AI Breakthroughs**  \n",
    "   Neural networks have since evolved into sophisticated architectures like *Transformers*, used in *Large Language Models* (LLMs) such as GPT and BERT. These architectures dominate many AI fields today, enabling cutting-edge capabilities in natural language processing, computer vision, and generative AI.\n",
    "\n",
    "Neural networks have transformed from simple computational models to the backbone of modern AI, driving innovation across industries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c1a33d-93e3-4f15-9787-bc396cca75aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
