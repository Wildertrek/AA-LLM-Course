{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29094414-93e8-4cd2-b9b4-57a2a108838f",
   "metadata": {},
   "source": [
    "<a href=\"https://openai.com\" target=\"_blank\">\n",
    "    <img src=\"../../LLM-Images/openai-lockup.png\" alt=\"OpenAI Logo\" width=\"300\" height=\"100\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "</a>\n",
    "\n",
    "\n",
    "- [OpenAI.com](https://openai.com/)\n",
    "- [OpenAI GitHub](https://github.com/openai)\n",
    "- [OpenAI Docs](https://platform.openai.com/docs/concepts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcc5d51-cc58-4423-aea7-73c0a502ba00",
   "metadata": {},
   "source": [
    "# Generative Pre-Trained Transformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f30b3c-1e0b-4e4d-b3f2-44d2fb1207ef",
   "metadata": {},
   "source": [
    "## GPT-1\n",
    "\n",
    "The first paper on **Generative Pre-trained Transformer (GPT)** is titled [**\"Improving Language Understanding by Generative Pre-Training\"** (Radford et al., 2018)](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf), published by Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever in 2018. This seminal work by researchers from OpenAI introduced the concept of pre-training a large language model using a generative approach on vast amounts of text data and then fine-tuning it on specific tasks. The authors proposed using a transformer-based architecture, which had previously been developed by Vaswani et al. (2017) for neural machine translation, to create a model that could be fine-tuned for a wide range of natural language processing (NLP) tasks. \n",
    "\n",
    "The authors’ innovation lay in training the model in an unsupervised manner on a large corpus of text, where it learned to predict the next word in a sentence. Following this pre-training phase, the model could then be fine-tuned on supervised datasets for specific tasks like question answering, text classification, and summarization, showing improved performance across diverse NLP benchmarks. The success of this approach led to the development of subsequent versions, GPT-2, GPT-3, and GPT-4, each with increased model size and capabilities. The impact of this work is profound, as it paved the way for modern NLP models that leverage large-scale pre-training and transfer learning to achieve state-of-the-art results.\n",
    "\n",
    "### Abstract\n",
    "Natural language understanding comprises a wide range of diverse tasks such as [textual entailment](https://academic.oup.com/edited-volume/42643/chapter-abstract/358152055?redirectedFrom=fulltext), [question answering](https://paperswithcode.com/task/question-answering), [semantic similarity assessment](https://paperswithcode.com/task/semantic-similarity), and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9% on commonsense reasoning (Stories Cloze Test), 5.7% on question answering (RACE), and 1.5% on textual entailment (MultiNLI).\n",
    "\n",
    "### Explanation of the Abstract\n",
    "\n",
    "- **Diverse Tasks in Natural Language Understanding**: The paper addresses a variety of tasks in natural language understanding, such as textual entailment, question answering, semantic similarity assessment, and document classification. It highlights the challenge of limited labeled data for these tasks, which hinders the performance of models trained solely on such data .\n",
    "\n",
    "- **Generative Pre-Training Approach**: The authors propose a method involving generative pre-training of a language model on a large, diverse corpus of unlabeled text. This is followed by discriminative fine-tuning on specific tasks, which leads to significant improvements in performance .\n",
    "\n",
    "- **Task-Aware Input Transformations**: Unlike previous methods, this approach incorporates task-aware input transformations during the fine-tuning phase. This allows for effective transfer of the pre-trained model to various tasks with minimal architectural changes .\n",
    "\n",
    "- **Performance on Benchmarks**: The proposed model, which is task-agnostic, outperforms models that are specifically designed for individual tasks. It achieves state-of-the-art results in 9 out of the 12 tasks studied, demonstrating its effectiveness across a wide range of benchmarks .\n",
    "\n",
    "- **Specific Improvements**: The paper reports notable improvements in specific tasks, such as an 8.9% increase in commonsense reasoning (Stories Cloze Test), a 5.7% improvement in question answering (RACE), and a 1.5% enhancement in textual entailment (MultiNLI) .\n",
    "\n",
    "\n",
    "### Specific Tasks Performed by GPT\n",
    "\n",
    "- **Natural Language Inference**: The model is applied to tasks such as SNLI, MultiNLI, Question NLI, RTE, and SciTail. These tasks involve determining the relationship between pairs of sentences, such as entailment, contradiction, or neutrality .\n",
    "\n",
    "- **Question Answering**: GPT is used for question answering tasks, including datasets like RACE. This involves providing answers to questions based on a given context or document  .\n",
    "\n",
    "- **Commonsense Reasoning**: The model is evaluated on tasks like the Story Cloze Test, which requires understanding and reasoning about everyday situations to predict the next event in a story .\n",
    "\n",
    "- **Sentence Similarity**: Tasks such as the MSR Paraphrase Corpus, Quora Question Pairs, and STS Benchmark are used to assess the model's ability to determine the similarity between sentences .\n",
    "\n",
    "- **Text Classification**: The model is fine-tuned for text classification tasks, such as those involving sentiment analysis using the Stanford Sentiment Treebank-2 and grammaticality using CoLA  .\n",
    "\n",
    "- **Textual Entailment**: This involves determining if one sentence logically follows from another, as seen in tasks like MultiNLI .\n",
    "\n",
    "These tasks demonstrate the versatility of the GPT model in handling a wide range of natural language understanding challenges by leveraging generative pre-training and task-specific fine-tuning.\n",
    "\n",
    "### Comparison of GPT to Previous NLP Models\n",
    "\n",
    "- **Task-Agnostic Model**: GPT is a general task-agnostic model that outperforms discriminatively trained models specifically crafted for each task. This is a significant improvement over previous models that required task-specific architectures  .\n",
    "\n",
    "- **Generative Pre-Training**: Unlike earlier models that relied heavily on supervised learning, GPT uses generative pre-training on a diverse corpus of unlabeled text. This approach allows it to acquire significant world knowledge and process long-range dependencies, which are then fine-tuned for specific tasks  .\n",
    "\n",
    "- **Minimal Architectural Changes**: GPT requires minimal changes to its architecture during transfer to different tasks. This contrasts with previous models that often needed substantial modifications and additional parameters for each target task .\n",
    "\n",
    "- **Performance on Benchmarks**: GPT achieves significant improvements on various benchmarks, outperforming state-of-the-art results in 9 out of 12 tasks studied. This includes tasks like commonsense reasoning, question answering, and textual entailment, where it shows substantial gains over previous models  .\n",
    "\n",
    "- **Handling Long-Range Dependencies**: The use of transformer networks in GPT allows it to capture longer-range linguistic structures compared to models using LSTM, which are limited to short-range dependencies .\n",
    "\n",
    "- **Unsupervised Pre-Training**: GPT's unsupervised pre-training phase captures several linguistic aspects relevant to target tasks, reducing the need for extensive labeled data, which was a limitation in earlier models .\n",
    "\n",
    "- **Zero-Shot Capabilities**: The model demonstrates zero-shot behaviors, acquiring useful linguistic knowledge for downstream tasks without task-specific training, which is a step forward from previous models that required task-specific training data .\n",
    "\n",
    "Overall, GPT represents a significant advancement in NLP by leveraging generative pre-training and a task-agnostic approach, which allows it to outperform previous models across a wide range of tasks with minimal architectural changes.\n",
    "\n",
    "\n",
    "### Dataset\n",
    "\n",
    "The BooksCorpus dataset, used in training GPT-1, was originally introduced in the paper **\"BookCorpus: A Dataset for Story Understanding\"** by authors Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, and Veselin Stoyanov. This dataset comprises over 11,000 books, predominantly novels, and was designed to assist in training models for tasks involving natural language understanding and generation by providing a large, coherent collection of text.\n",
    "\n",
    "Unfortunately, the BooksCorpus dataset itself is not directly linked in this paper due to copyright considerations, but it can be referenced from its mention in various papers and datasets listings online. If you are looking for direct access or more details about the dataset, you may find academic discussions on repositories like [arXiv](https://arxiv.org/) or research datasets on platforms like [TensorFlow Datasets](https://www.tensorflow.org/datasets/catalog/bookcorpus) and [Hugging Face](https://huggingface.co/datasets/bookcorpus). \n",
    "\n",
    "The BookCorpus dataset was introduced in the paper titled [**\"Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books\"** (Zhu et al., 2015)](https://arxiv.org/abs/1506.06724) by Yukun Zhu, Ryan Kiros, Richard Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. This paper discusses the alignment of books and movies for story understanding and introduced the BookCorpus as part of their research.\n",
    "\n",
    "### BookCorpus Abstract\n",
    "\n",
    "Books are a rich source of both fine-grained information, how a character, an object or a scene looks like, as well as high-level semantics, what someone is thinking, feeling and how these states evolve through a story. This paper aims to align books to their movie releases in order to provide rich descriptive explanations for visual content that go semantically far beyond the captions available in the current datasets. To align movies and books we propose a neural sentence embedding that is trained in an unsupervised way from a large corpus of books, as well as a video-text neural embedding for computing similarities between movie clips and sentences in the book. We propose a context-aware CNN to combine information from multiple sources. We demonstrate good quantitative performance for movie/book alignment and show several qualitative examples that showcase the diversity of tasks our model can be used for.\n",
    "\n",
    "### Datasets of the same ilk for further research\n",
    "For studying character development and emotional progression throughout stories, there are several datasets that focus on narrative structure, character interactions, and emotional dynamics. Here are some notable ones:\n",
    "\n",
    "1. **NarrativeQA:** This dataset contains summaries and questions about books and movie scripts, with an emphasis on understanding plot progression and character development. It helps in analyzing how characters evolve within a story and can be accessed here: [NarrativeQA](https://github.com/google-deepmind/narrativeqa).\n",
    "\n",
    "2. **EmotionLines:** This dataset includes dialogues from TV shows with labeled emotions for each line, allowing for the analysis of character emotions and their evolution throughout conversations. It’s particularly useful for studying emotional dynamics in character interactions. You can find more information and access it through [EmotionLines-MELD](https://paperswithcode.com/dataset/meld).\n",
    "\n",
    "3. **COMET:** While not a traditional text corpus, COMET (Commonsense Transformers) is a model trained on the ATOMIC dataset, which captures commonsense reasoning, including characters’ motivations and reactions. This can be useful for generating or inferring character thoughts and emotions. Access it via [ATOMIC on AllenAI](https://allenai.org/data/atomic-2020).\n",
    "\n",
    "4. **Character Mining Dataset (CMD):** CMD contains annotated data for character-based analyses in narratives, such as identifying character arcs and traits. This dataset is particularly focused on understanding character roles and evolution in texts. Though not readily accessible online, CMD and similar datasets are discussed in academic research, and you might find derived versions on platforms like GitHub. [emoryNLP](https://github.com/emorynlp/character-mining)\n",
    "\n",
    "5. **Story Commonsense (SOC):** This dataset provides annotations on character intentions, emotional responses, and motivations across short stories, making it useful for tracking character development through various stages of narrative. SOC helps model how characters' thoughts and feelings evolve in relation to events. You can find it on [the Story Commonsense page](https://github.com/ronaldosantosv/storystorm). [Commonsense](https://uwnlp.github.io/storycommonsense/)\n",
    "\n",
    "6. **DramaQA:** Designed for visual and textual analyses of drama series, this dataset contains character interactions and emotional dynamics, annotated for scenes and dialogues. It is suitable for studying character emotions in multimedia narratives and is accessible here: [DramaQA](https://dramaqa.snu.ac.kr/).\n",
    "\n",
    "These datasets offer a mix of text-based and multimodal resources that facilitate detailed studies on character evolution, emotional arcs, and interpersonal dynamics within narratives.\n",
    "\n",
    "### Research Gap GPT\n",
    "\n",
    "- The research paper does not explore the potential benefits of multi-task training for the model, despite its strong performance on larger natural language inference (NLI) datasets, indicating a gap in understanding how multi-task learning could enhance model capabilities.  (Radford et al., n.d.)\n",
    "\n",
    "- There is limited investigation into the effects of the auxiliary language modeling objective on smaller datasets, suggesting a need for further research to determine its impact across varying dataset sizes.  (Radford et al., n.d.)\n",
    "\n",
    "- The paper lacks a comprehensive analysis of the model's performance compared to other architectures beyond the transformer, particularly in specific tasks.  (Radford et al., n.d.)\n",
    "\n",
    "### Future Research GPT\n",
    "\n",
    "- Future research aims to enhance understanding of unsupervised learning, particularly in natural language understanding and other domains. This exploration seeks to clarify the conditions under which unsupervised learning is effective.  (Radford et al., n.d.)\n",
    "\n",
    "- Investigating the integration of more complex semantic representations, such as phrase-level or sentence-level embeddings, from unlabeled data is another area of interest.  (Radford et al., n.d.)\n",
    "\n",
    "- Additionally, there is potential for further studies on the impact of generative pre-training on various tasks and its effectiveness compared to traditional supervised learning methods.  (Radford et al., n.d.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bf60fa-6008-4d8b-bdfa-37ebfc071fca",
   "metadata": {},
   "source": [
    "## GPT-2\n",
    "\n",
    "The paper titled [**\"Language Models are Unsupervised Multitask Learners\"** (Radford et al., 2019)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)was released by OpenAI and is often associated with the introduction of GPT-2. This paper discusses how the GPT-2 model was trained as a general-purpose language model and demonstrated remarkable zero-shot learning capabilities across a variety of NLP tasks without task-specific training.\n",
    "\n",
    "This work builds on the concepts introduced in the original GPT paper and demonstrates the scalability and versatility of large-scale language models in performing multiple tasks unsupervised.\n",
    "\n",
    "### Abstract\n",
    "\n",
    "Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the [CoQA dataset](https://stanfordnlp.github.io/coqa/) matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.\n",
    "\n",
    "### Explanation of the Abstract\n",
    "\n",
    "- **Natural Language Processing Tasks**: The paper discusses how tasks like question answering, machine translation, reading comprehension, and summarization are traditionally handled using supervised learning on specific datasets. However, the authors demonstrate that language models can begin to learn these tasks without explicit supervision when trained on a large dataset called WebText .\n",
    "\n",
    "- **Performance on CoQA Dataset**: When the language model is conditioned on a document plus questions, it achieves a 55 F1 score on the CoQA dataset. This performance matches or exceeds that of three out of four baseline systems, even without using the 127,000+ training examples typically required .\n",
    "\n",
    "- **Importance of Model Capacity**: The abstract highlights that the capacity of the language model is crucial for successful zero-shot task transfer. Increasing the model's capacity improves performance across tasks in a log-linear fashion, indicating that larger models are more effective at generalizing to new tasks without additional training .\n",
    "\n",
    "- **GPT-2 Model**: The largest model discussed, GPT-2, is a 1.5 billion parameter Transformer. It achieves state-of-the-art results on seven out of eight tested language modeling datasets in a zero-shot setting. Despite these achievements, the model still underfits the WebText dataset, suggesting room for further improvement .\n",
    "\n",
    "- **Coherent Text Generation**: Samples generated by the model reflect improvements in coherence, producing paragraphs of text that are logically structured and contextually relevant .\n",
    "\n",
    "- **Future Directions**: The findings suggest a promising path towards developing language processing systems that learn to perform tasks from naturally occurring demonstrations, potentially reducing the need for task-specific training data . \n",
    "\n",
    "This abstract provides an overview of the capabilities and potential of unsupervised language models, particularly GPT-2, in performing various natural language processing tasks without explicit supervision.\n",
    "\n",
    "### Specific Tasks Performed by GPT-2\n",
    "\n",
    "- **Summarization**: GPT-2 is capable of generating summaries of text. However, its performance on the ROUGE metrics shows that it only begins to approach the performance of classic neural baselines. The generated summaries often focus on recent content from the article and may confuse specific details .\n",
    "\n",
    "- **Translation**: GPT-2 has been tested for translation tasks. On the WMT-14 English-French test set, it achieves a BLEU score of 5, which is slightly worse than a word-by-word substitution with a bilingual lexicon. However, on the WMT-14 French-English test set, it performs better, achieving a BLEU score of 11.5, outperforming several unsupervised machine translation baselines .\n",
    "\n",
    "- **Question Answering**: When conditioned on a document plus questions, GPT-2 can generate answers that reach a 55 F1 score on the CoQA dataset, matching or exceeding the performance of several baseline systems without using extensive training examples .\n",
    "\n",
    "- **LAMBADA Task**: GPT-2 significantly improves the state of the art on the LAMBADA dataset, which tests the ability to model long-range dependencies in text. It reduces perplexity and increases accuracy, although it sometimes fails to predict the final word of a sentence correctly .\n",
    "\n",
    "- **General Language Modeling**: GPT-2 achieves state-of-the-art results on seven out of eight tested language modeling datasets in a zero-shot setting, indicating its strong general language modeling capabilities .\n",
    "\n",
    "These tasks demonstrate GPT-2's versatility and ability to perform various natural language processing tasks without explicit supervision, leveraging its large-scale training on diverse data.\n",
    "\n",
    "### Comparison of GPT-2 to GPT Model and Previous NLP Models\n",
    "\n",
    "- **Model Size and Capacity**:\n",
    "  - GPT-2 is significantly larger than the original GPT model, with over an order of magnitude more parameters. This increase in size allows GPT-2 to perform better on various tasks due to its enhanced capacity to learn from data .\n",
    "  - The capacity of GPT-2 is crucial for its success in zero-shot task transfer, and increasing the model size improves performance in a log-linear fashion across tasks .\n",
    "\n",
    "- **Performance on Language Modeling**:\n",
    "  - GPT-2 achieves state-of-the-art results on seven out of eight tested language modeling datasets in a zero-shot setting, indicating its superior performance compared to previous models .\n",
    "  - The model still underfits WebText, suggesting that even with its large size, there is room for improvement with more training time .\n",
    "\n",
    "- **Translation and Summarization**:\n",
    "  - In translation tasks, GPT-2 performs better than several unsupervised machine translation baselines, although it is still not as effective as the best unsupervised approaches .\n",
    "  - For summarization, GPT-2's performance is comparable to classic neural baselines, but it struggles with specific details and coherence in the generated summaries .\n",
    "\n",
    "- **Question Answering**:\n",
    "  - GPT-2 shows improved performance in question answering tasks compared to smaller models, suggesting that model capacity is a significant factor in its success .\n",
    "  - It achieves a 55 F1 score on the CoQA dataset, matching or exceeding several baseline systems without extensive training data .\n",
    "\n",
    "- **LAMBADA Task**:\n",
    "  - GPT-2 improves the state of the art on the LAMBADA dataset, demonstrating its ability to handle long-range dependencies in text better than previous models .\n",
    "\n",
    "- **General Observations**:\n",
    "  - GPT-2's fully abstractive output is a departure from the extractive pointer network-based outputs that are state of the art in many question answering and reading comprehension datasets .\n",
    "  - The model's zero-shot performance establishes a baseline for its potential, but the ceiling with fine-tuning remains unclear .\n",
    "\n",
    "Overall, GPT-2 represents a significant advancement over the original GPT and other previous NLP models, particularly in its ability to perform a wide range of tasks without explicit supervision. However, there are still areas where it can be improved, especially with fine-tuning and additional training.\n",
    "\n",
    "### Dataset - WebText\n",
    "\n",
    "The [**WebText**](https://paperswithcode.com/dataset/webtext) dataset, originally curated by OpenAI for training GPT-2, is not publicly available due to the use of web content that may not be freely redistributable. OpenAI constructed WebText by scraping high-quality webpages that were frequently shared on Reddit, excluding those with low karma scores. However, the specific dataset itself has not been released to the public because of copyright considerations.\n",
    "\n",
    "For an alternative, the **OpenWebText** project was created by researchers as an open-source version that replicates the methodology used by OpenAI. You can access the OpenWebText dataset, which is widely used as a substitute for WebText, on the following GitHub repository:\n",
    "\n",
    "- **OpenWebText**: [OpenWebText on GitHub](https://github.com/skylion007/openwebtext)\n",
    "\n",
    "This dataset is designed to be similar to WebText in terms of quality and structure, making it a suitable alternative for experiments and research.\n",
    "\n",
    "The GPT-2 paper by OpenAI is one of the influential works that popularized the concept of **zero-shot learning** in large language models. However, the concept of zero-shot learning itself predates the GPT-2 paper and has been explored in various contexts, particularly within computer vision and NLP, for several years.\n",
    "\n",
    "### Early Origins of Zero-Shot Learning:\n",
    "Zero-shot learning, as a formal concept, was first discussed in the context of **computer vision**. One of the foundational papers in this area is **\"Zero-Shot Learning and Clustering by Convex Combination of Prototypes\"** by Larochelle et al. (2008). This paper introduced zero-shot learning as a method for recognizing objects that were not part of the training dataset, using attribute-based representations to transfer knowledge from seen to unseen categories. \n",
    "\n",
    "**Zero-data** and **zero-shot** learning are closely related concepts, both involving models making predictions on tasks or categories for which they haven't seen explicit training data. However, there are subtle differences between the two:\n",
    "\n",
    "1. **Zero-Data Learning**: This term was initially used to describe scenarios where a model is trained without any labeled data for a specific class or task but still attempts to generalize to unseen classes. In Larochelle et al.'s (2008) work, zero-data learning often relied on transferring knowledge from similar or related classes that share certain attributes. Essentially, zero-data learning seeks to address the question: *How can a model recognize or infer something for which it has no direct data?* This approach typically uses auxiliary information, such as semantic attributes or prior knowledge about the relationships between classes, to fill in the gaps.\n",
    "\n",
    "2. **Zero-Shot Learning**: While zero-shot learning builds upon the idea of zero-data learning, it focuses more on tasks where a model applies its knowledge to completely new tasks without additional task-specific training. In zero-shot learning, models usually leverage their understanding of seen classes to infer properties or outcomes for unseen classes. For example, GPT-2’s zero-shot capabilities involve completing NLP tasks (like translation or question-answering) without task-specific fine-tuning. Zero-shot learning often emphasizes a model's ability to handle novel tasks in a “plug-and-play” manner, essentially extending the idea of zero-data learning to multiple, unanticipated tasks across various domains.\n",
    "\n",
    "[**Zero-data Learning of New Tasks**](https://cdn.aaai.org/AAAI/2008/AAAI08-103.pdf) learning primarily focuses on unseen classes within a known task and often requires auxiliary data for those unseen classes. Zero-shot learning, on the other hand, has a broader application and is generally task-independent, enabling a model to tackle new tasks or unseen classes without specific training.\n",
    "\n",
    "In the context of **natural language processing (NLP)**, the idea of transferring knowledge to perform tasks without task-specific training data has been explored prior to GPT-2. One notable work is [**\"Zero-Shot Learning with Semantic Output Codes\"** by Socher et al. (2013)](https://www.cs.toronto.edu/~hinton/absps/palatucci.pdf), which applied zero-shot learning to text classification using semantic representations of output labels.\n",
    "\n",
    "\n",
    "### Zero-Shot Learning in Large Language Models:\n",
    "The GPT-2 paper is significant because it applied zero-shot learning to a wide range of NLP tasks within a unified framework, showcasing how a single model, trained on a large corpus without specific task labels, could generalize across tasks. This approach demonstrated that large-scale pre-trained language models could achieve impressive results without fine-tuning on particular downstream tasks, which was groundbreaking at the time. The GPT-2 work extended the concept to language modeling, showing zero-shot capabilities on tasks like translation, summarization, and question answering without any task-specific training, making it a landmark achievement in NLP.\n",
    "\n",
    "In summary, while the GPT-2 paper by OpenAI brought significant attention to zero-shot learning in the realm of language models, the concept itself has been discussed and utilized in machine learning for over a decade, with early work in computer vision and some initial explorations in NLP.\n",
    "\n",
    "### Research Gap GPT-2\n",
    "\n",
    "- The research identifies a gap in understanding the ceiling of performance for GPT-2 when fine-tuned on various tasks, as it is unclear how much improvement can be achieved beyond zero-shot performance.  (Radford et al., n.d.)\n",
    "\n",
    "- There is a need for better de-duplication techniques to assess the impact of highly similar text on performance, suggesting that current methods may not adequately address this issue.  (Radford et al., n.d.)\n",
    "\n",
    "- The paper indicates that while GPT-2 performs competitively in zero-shot settings, its performance on tasks like summarization remains rudimentary, highlighting a gap in practical usability.  (Radford et al., n.d.)\n",
    "\n",
    "### Future Research GPT-2\n",
    "\n",
    "- Future research should explore unsupervised task learning as a promising area, given its potential to enhance the performance of language models without the need for supervised adaptation or modification.  (Radford et al., n.d.)\n",
    "\n",
    "- Investigating the fine-tuning of models like GPT-2 on various benchmarks, such as decaNLP and GLUE, is essential to understand the ceiling of performance improvements.  (Radford et al., n.d.)\n",
    "\n",
    "- There is a need to evaluate the performance of language models on additional practical tasks, as current models may still perform no better than random on many tasks.  (Radford et al., n.d.)\n",
    "\n",
    "- Further research could focus on optimizing input representation methods to improve the competitiveness of byte-level language models against word-level models.  (Radford et al., n.d.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861d89ec-9ff2-4ba9-9156-68dd56b14962",
   "metadata": {},
   "source": [
    "## GPT-3\n",
    "\n",
    "The origin paper for GPT-3 is titled [**\"Language Models are Few-Shot Learners\"** (Brown et al., 2020)](https://arxiv.org/abs/2005.14165), authored by researchers at OpenAI, including Tom B. Brown, Benjamin Mann, Nick Ryder, and several others. Published in 2020, this paper introduces GPT-3, the third iteration in the Generative Pre-trained Transformer series, and highlights its capabilities as a powerful language model with 175 billion parameters. The paper emphasizes GPT-3's remarkable few-shot, one-shot, and zero-shot learning capabilities, allowing it to perform a wide range of tasks without task-specific fine-tuning, simply by conditioning on a few examples provided in the input prompt.\n",
    "\n",
    "This work marks a significant advancement in the field of NLP by demonstrating that scaling up model size leads to substantial improvements in generalization and task performance, showcasing GPT-3’s ability to generate human-like text and perform diverse tasks with minimal guidance.\n",
    "\n",
    "### Abstract\n",
    "\n",
    "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.\n",
    "\n",
    "### Understanding the Abstract\n",
    "\n",
    "The abstract of the paper \"Language Models are Few-Shot Learners\" provides a concise overview of the research and its significance in the field of Natural Language Processing (NLP). Here are the key points explained in detail:\n",
    "\n",
    "- **Pre-training and Fine-tuning**: The paper begins by discussing the traditional approach in NLP, which involves pre-training models on large text corpora followed by fine-tuning on specific tasks. This method typically requires extensive task-specific datasets, often comprising thousands of examples, to achieve good performance .\n",
    "\n",
    "- **Human-like Learning**: It highlights a critical difference between human learning and current NLP systems. Humans can often learn new tasks with just a few examples or simple instructions, while existing models struggle with this few-shot learning capability .\n",
    "\n",
    "- **Scaling Up Language Models**: The authors present their findings that scaling up language models significantly enhances their ability to perform tasks in a few-shot manner. They introduce GPT-3, a large autoregressive language model with 175 billion parameters, which is ten times larger than any previous non-sparse model .\n",
    "\n",
    "- **Performance without Fine-tuning**: A notable aspect of GPT-3 is that it can perform various NLP tasks without any gradient updates or fine-tuning. Instead, it relies on text interactions to specify tasks and provide few-shot demonstrations .\n",
    "\n",
    "- **Strong Performance Across Tasks**: The results show that GPT-3 achieves impressive performance on a variety of NLP datasets, including translation, question-answering, and cloze tasks. It also demonstrates capabilities in reasoning and domain adaptation, such as unscrambling words and performing arithmetic .\n",
    "\n",
    "- **Limitations and Challenges**: Despite its strengths, the paper acknowledges that GPT-3 still faces challenges with certain datasets and methodological issues related to its training on large web corpora .\n",
    "\n",
    "- **Societal Impacts**: Finally, the authors discuss the broader societal implications of their findings, particularly the ability of GPT-3 to generate human-like text, which raises questions about the potential consequences of such technology .\n",
    "\n",
    "The abstract encapsulates the innovative approach of using a large-scale language model to enhance few-shot learning in NLP, while also addressing its limitations and societal implications.\n",
    "\n",
    "### Specific Tasks Performed by GPT-3\n",
    "\n",
    "- **Translation**: GPT-3 is capable of performing translation tasks by learning from a blend of training data that includes multiple languages. It combines languages naturally at the word, sentence, and document levels without a task-specific training objective .\n",
    "\n",
    "- **Question-Answering and Cloze Tasks**: The model demonstrates strong performance on question-answering and cloze tasks, which involve filling in missing words or phrases in a text .\n",
    "\n",
    "- **On-the-Fly Reasoning and Domain Adaptation**: GPT-3 can handle tasks that require immediate reasoning or adapting to new domains. Examples include unscrambling words, using novel words in sentences, and performing arithmetic calculations .\n",
    "\n",
    "- **Reading Comprehension and SuperGLUE Benchmark**: The model is evaluated on reading comprehension tasks and the SuperGLUE benchmark suite, which are designed to test its understanding and reasoning capabilities .\n",
    "\n",
    "- **Commonsense Reasoning and Question Answering**: GPT-3 is tested on datasets that involve commonsense reasoning, such as the Winograd Schema-like tasks, and question-answering tasks .\n",
    "\n",
    "- **Synthetic and Qualitative Tasks**: The model is also evaluated on synthetic tasks like arithmetic and qualitative tasks such as solving SAT-style analogy problems, correcting English grammar, and generating news articles .\n",
    "\n",
    "- **Language Modeling and Text Completion**: GPT-3 is tested on traditional language modeling tasks, which include predicting a single word, completing sentences or paragraphs, and choosing between possible text completions .\n",
    "\n",
    "These tasks highlight GPT-3's versatility and its ability to perform a wide range of language-related tasks without task-specific fine-tuning. However, it still faces challenges in certain areas, such as common sense physics and some reading comprehension tasks .\n",
    "\n",
    "### Comparison of GPT-3 to Previous Models\n",
    "\n",
    "GPT-3 represents a significant advancement in the field of Natural Language Processing (NLP) compared to its predecessors. Here are some key points of comparison:\n",
    "\n",
    "- **Model Size**: GPT-3 is an autoregressive language model with 175 billion parameters, which is 10 times larger than any previous non-sparse language model. This substantial increase in size allows GPT-3 to capture more complex patterns and nuances in language, leading to improved performance on various tasks .\n",
    "\n",
    "- **Few-Shot Learning**: Unlike earlier models that required extensive fine-tuning on task-specific datasets, GPT-3 demonstrates strong few-shot learning capabilities. It can perform tasks with only a few examples or simple instructions, which is a significant leap from previous models that typically needed thousands of examples for effective training .\n",
    "\n",
    "- **Task-Agnostic Performance**: GPT-3's architecture is designed to be task-agnostic, meaning it can handle a wide range of NLP tasks without the need for task-specific adjustments. This contrasts with earlier models that often required fine-tuning for each specific task, making GPT-3 more versatile and easier to deploy across different applications .\n",
    "\n",
    "- **Competitiveness with Fine-Tuning Approaches**: The performance of GPT-3 in few-shot settings is competitive with prior state-of-the-art fine-tuning approaches. This suggests that GPT-3 can achieve similar or better results without the extensive training typically required by previous models .\n",
    "\n",
    "- **Broader Capabilities**: GPT-3 not only excels in traditional NLP tasks like translation and question-answering but also in more complex tasks that require reasoning and domain adaptation, such as unscrambling words and performing arithmetic. This broad capability further distinguishes it from earlier models that were often limited in scope .\n",
    "\n",
    "GPT-3's larger size, few-shot learning ability, task-agnostic performance, competitiveness with fine-tuning methods, and broader capabilities mark a significant improvement over previous models in the NLP landscape.\n",
    "\n",
    "\n",
    "### Research Gap GPT-3\n",
    "\n",
    "- The paper identifies a lack of understanding regarding how few-shot learning operates in GPT-3, particularly whether it learns tasks from scratch or recognizes previously learned tasks during inference. This ambiguity presents a significant research gap.  (Brown et al., n.d.)\n",
    "\n",
    "- There is a need for exploration into the limitations of scaling language models, as they may encounter the limits of their pretraining objectives, which currently treat all tokens equally without prioritizing important predictions.  (Brown et al., n.d.)\n",
    "\n",
    "- The paper suggests that future research should focus on improving sample efficiency during pre-training and grounding models in real-world experiences.  (Brown et al., n.d.)\n",
    "\n",
    "### Future Research GPT-3\n",
    "\n",
    "- Future research is expected to focus on characterizing biases in large-scale generative models, particularly regarding gender, race, and religion, as these areas present inherent difficulties and subjectivity in analysis.   (Brown et al., n.d.)\n",
    "\n",
    "- There is a need for continuous exploration of methodological approaches to better understand and mitigate biases in language models.  (Brown et al., n.d.)\n",
    "\n",
    "- Additionally, the integration of different modalities, such as images, and the development of goal-directed language systems may enhance the capabilities of language models beyond pure self-supervised prediction.  (Brown et al., n.d.)\n",
    "\n",
    "- Meta-learning and in-context learning are also promising directions for improving task adaptation in language models.  (Brown et al., n.d.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe47344-4585-44ba-872b-ae3af5dd6c57",
   "metadata": {},
   "source": [
    "## GPT-4 (Multimodal)\n",
    "\n",
    "OpenAI released a technical report detailing GPT-4, titled [**\"GPT-4 Technical Report\"** (OpenAI, 2023)](https://cdn.openai.com/papers/gpt-4.pdf), which serves as the origin document for GPT-4. This report was published in March 2023 and provides insights into the architecture, capabilities, and evaluations of GPT-4. Unlike previous GPT iterations, OpenAI has not disclosed detailed specifics about the model's architecture, parameter count, or training data due to proprietary considerations. However, the report highlights that GPT-4 significantly advances multimodal capabilities, enabling it to process both text and image inputs. It also showcases improved performance across a wide range of benchmarks, demonstrating advancements in accuracy, reasoning, and safety over GPT-3.\n",
    "\n",
    "This report underscores GPT-4's enhancements in zero-shot, one-shot, and few-shot learning, and it outlines various applications in areas like education, healthcare, and professional services, while also emphasizing efforts to mitigate risks related to bias, misinformation, and misuse.\n",
    "\n",
    "### Abstract\n",
    "We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformerbased model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4’s performance based on models trained with no more than 1/1,000th the compute of GPT-4.\n",
    "\n",
    "### Summary of the Abstract\n",
    "\n",
    "- **Development of GPT-4**: The paper discusses the creation of GPT-4, a large-scale, multimodal model capable of processing both image and text inputs to generate text outputs. This highlights the model's versatility in handling different types of data .\n",
    "\n",
    "- **Performance Capabilities**: Although GPT-4 is not as capable as humans in many real-world situations, it demonstrates human-level performance on various professional and academic benchmarks. Notably, it achieved a score in the top 10% of test takers on a simulated bar exam, showcasing its advanced capabilities in specific tasks .\n",
    "\n",
    "- **Model Architecture**: GPT-4 is based on the Transformer architecture and is pre-trained to predict the next token in a document. This foundational approach is common in large language models, enabling them to understand and generate coherent text .\n",
    "\n",
    "- **Post-Training Alignment**: The model undergoes a post-training alignment process, which enhances its performance in terms of factual accuracy and adherence to desired behaviors. This process is crucial for ensuring the model's outputs are reliable and aligned with user expectations .\n",
    "\n",
    "- **Infrastructure and Optimization**: A significant part of the project involved developing infrastructure and optimization methods that perform predictably across various scales. This ensures that the model can be effectively scaled and utilized in different applications .\n",
    "\n",
    "- **Performance Prediction**: The researchers were able to accurately predict some aspects of GPT-4's performance using models trained with significantly less computational power, specifically no more than 1/1,000th of the compute used for GPT-4. This indicates the efficiency and foresight in the model's development process .\n",
    "\n",
    "### Specific Tasks Performed by GPT-4\n",
    "\n",
    "- **Multimodal Processing**: GPT-4 is capable of processing both image and text inputs, which allows it to perform tasks that require understanding and generating text based on visual and textual information  .\n",
    "\n",
    "- **Professional and Academic Benchmarks**: The model exhibits human-level performance on various professional and academic benchmarks. For instance, it has passed a simulated bar exam with a score in the top 10% of test takers, demonstrating its ability to handle complex, high-stakes tasks .\n",
    "\n",
    "- **NLP Benchmarks**: On traditional NLP benchmarks, GPT-4 outperforms previous large language models and most state-of-the-art systems, even those with benchmark-specific training or hand-engineering. This includes tasks like language understanding and generation across multiple languages .\n",
    "\n",
    "- **MMLU Benchmark**: GPT-4 excels in the MMLU benchmark, which is an English-language suite of multiple-choice questions covering 57 subjects. It not only surpasses existing models in English but also performs strongly in other languages, outperforming the English-language state-of-the-art in 24 of 26 languages considered .\n",
    "\n",
    "- **Cybersecurity Applications**: While GPT-4 is not a ready-made upgrade for social engineering, it can draft realistic phishing emails and explain vulnerabilities when provided with appropriate background knowledge. It can also assist in parsing audit logs and summarizing data from cyberattacks, although it has limitations in more complex cybersecurity operations  .\n",
    "\n",
    "These tasks highlight GPT-4's versatility and advanced capabilities in handling a wide range of applications, from language processing to cybersecurity, while also acknowledging its limitations in certain areas.\n",
    "\n",
    "### Comparison of GPT-4 to Previous Models\n",
    "\n",
    "- **Performance Improvement**: GPT-4 shows significant improvements over previous models, particularly GPT-3.5. It improves by 19 percentage points in accuracy across various topics, indicating a substantial enhancement in its ability to generate correct responses .\n",
    "\n",
    "- **Multimodal Capabilities**: Unlike its predecessors, GPT-4 is a multimodal model, meaning it can process both text and image inputs. This capability allows it to perform a broader range of tasks compared to earlier models that were limited to text-only inputs .\n",
    "\n",
    "- **Language Proficiency**: On the MMLU benchmark, GPT-4 not only outperforms previous models in English but also demonstrates strong performance in other languages. It surpasses the English-language state-of-the-art in 24 out of 26 languages tested, showcasing its enhanced multilingual capabilities .\n",
    "\n",
    "- **Human-Level Performance**: GPT-4 achieves human-level performance on certain professional and academic benchmarks, such as passing a simulated bar exam in the top 10% of test takers. This level of performance was not observed in earlier models, highlighting GPT-4's advanced capabilities .\n",
    "\n",
    "- **Safety and Reliability**: Despite improvements, GPT-4 still shares some limitations with earlier models, such as the potential for hallucinations and reasoning errors. However, efforts have been made to improve its safety and reliability through post-training alignment processes  .\n",
    "\n",
    "- **User Intent and Preference**: In a dataset of 5,214 prompts, responses generated by GPT-4 were preferred over those from GPT-3.5 in 70.2% of cases, indicating a better alignment with user intent and preferences .\n",
    "\n",
    "These points illustrate the advancements GPT-4 has made over its predecessors, while also acknowledging areas where it still faces challenges.\n",
    "\n",
    "### Research Gap GPT-4\n",
    "\n",
    "- The paper identifies that GPT-4 has various biases in its outputs, which require further characterization and management to ensure reasonable default behaviors that reflect diverse user values.  (Berner et al., n.d.)\n",
    "\n",
    "- There is a need for more research into the economic impacts of AI and increased automation, as well as the necessary structures to facilitate a smoother societal transition.  (Berner et al., n.d.)\n",
    "\n",
    "- The limitations of GPT-4, such as its unreliability and tendency to 'hallucinate' facts, highlight the necessity for ongoing studies to address these safety challenges and their societal implications.  (Berner et al., n.d.)\n",
    "\n",
    "- The report emphasizes the importance of independent auditing and transparency in the development of large-scale models like GPT-4.  (Berner et al., n.d.)\n",
    "\n",
    "### Future Research GPT-4\n",
    "\n",
    "- Future research should focus on robust evaluations for risky emergent behaviors in language models, including situational awareness, persuasion, and long-horizon planning.  (Berner et al., n.d.)\n",
    "\n",
    "- There is a need for interpretability, explainability, and calibration to address the challenges posed by 'black-box' AI models.  (Berner et al., n.d.)\n",
    "\n",
    "- Promoting AI literacy is essential to ensure appropriate scrutiny of model outputs.  (Berner et al., n.d.)\n",
    "\n",
    "- Research should also explore the economic impacts of AI and increased automation, along with structures to facilitate smoother societal transitions.  (Berner et al., n.d.)\n",
    "\n",
    "- Broader public participation in defining optimal behaviors for AI models is encouraged.  (Berner et al., n.d.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0be422-62e4-41fd-80e4-6cbfbcf18430",
   "metadata": {},
   "source": [
    "## GPT-4o (Multimodal)\n",
    "\n",
    "There is a paper on GPT-4o titled [**\"Putting GPT-4o to the Sword: A Comprehensive Evaluation of Language, Vision, Speech, and Multimodal Proficiency\"**](https://ar5iv.org/abs/2407.09519), published in 2024. This paper provides an in-depth evaluation of GPT-4o, detailing its advancements over previous models in terms of multimodal capabilities, which include processing and integrating text, vision, and audio. The research examines GPT-4o's performance on various tasks, such as language comprehension, image classification, and speech recognition, showcasing its enhanced accuracy and efficiency in multimodal tasks compared to earlier versions like GPT-4. The study also highlights limitations in handling complex audio and visual inputs, and the authors suggest that future research should focus on expanding evaluation frameworks and datasets to further test GPT-4o’s practical applications.\n",
    "\n",
    "For more detailed information about GPT-4o's capabilities and evaluations, another relevant discussion of GPT-4o's advancements and applications is available in a [TechRxiv paper](https://www.techrxiv.org/doi/full/10.36227/techrxiv.171986596.65533294/v1), which offers an overview of its performance in comparison with other large language models across various metrics, including response time and efficiency. \n",
    "\n",
    "### Abstract\n",
    "\n",
    "As large language models (LLMs) continue to advance, evaluating their comprehensive capabilities becomes significant for their application in various fields. This research study comprehensively evaluates the language, vision, speech, and multimodal capabilities of GPT-4o. The study employs standardized exam questions, reasoning tasks, and translation assessments to assess the model's language capability. Additionally, GPT-4o's vision and speech capabilities are tested through image classification and object recognition tasks, as well as accent classification. The multimodal evaluation assesses the model's performance in integrating visual and linguistic data. Our findings reveal that GPT-4o demonstrates high accuracy and efficiency across multiple domains in language and reasoning capabilities, excelling in tasks that require few-shot learning. GPT-4o also provides notable improvements in multimodal tasks compared to its predecessors. However, the model shows variability and faces limitations in handling complex and ambiguous inputs, particularly in audio and vision capabilities. This paper highlights the need for more comprehensive benchmarks and robust evaluation frameworks, encompassing qualitative assessments involving human judgment as well as error analysis. Future work should focus on expanding datasets, investigating prompt-based assessment, and enhancing few-shot learning techniques to test the model's practical applicability and performance in real-world scenarios.\n",
    "\n",
    "### Summary of the Abstract\n",
    "\n",
    "- The paper focuses on evaluating the comprehensive capabilities of GPT-4o, a large language model (LLM), across various domains including language, vision, speech, and multimodal tasks .\n",
    "- The study employs standardized exam questions, reasoning tasks, and translation assessments to evaluate the language capabilities of GPT-4o. Additionally, the model's vision and speech capabilities are tested through tasks like image classification, object recognition, and accent classification .\n",
    "- The multimodal evaluation assesses how well GPT-4o integrates visual and linguistic data, highlighting its performance improvements in these areas compared to previous models .\n",
    "- Findings indicate that GPT-4o demonstrates high accuracy and efficiency, particularly excelling in tasks that require few-shot learning. However, the model shows variability and limitations when dealing with complex and ambiguous inputs, especially in audio and vision capabilities .\n",
    "- The paper emphasizes the need for more comprehensive benchmarks and robust evaluation frameworks, which should include qualitative assessments involving human judgment and error analysis .\n",
    "- Future work is suggested to focus on expanding datasets, investigating prompt-based assessments, and enhancing few-shot learning techniques to better test the model's practical applicability and performance in real-world scenarios.\n",
    "\n",
    "### Specific Tasks Performed by GPT-4o\n",
    "\n",
    "- **Language Tasks**: GPT-4o was evaluated using standardized exam questions, reasoning tasks, and translation assessments. These tasks were designed to test the model's language capabilities, including its ability to understand and generate text in multiple languages .\n",
    "\n",
    "- **Translation Tasks**: The model was specifically tested for its translation accuracy across six major languages: Spanish, Arabic, Hindi, French, Portuguese, and Russian. The results showed high accuracy, particularly in Spanish and Portuguese, but highlighted challenges in languages like Arabic and French due to their complex linguistic structures .\n",
    "\n",
    "- **Vision Tasks**: GPT-4o's vision capabilities were assessed through image classification and object recognition tasks. These tasks aimed to evaluate the model's ability to interpret and understand visual data .\n",
    "\n",
    "- **Speech Tasks**: The model's speech capabilities were tested through accent classification tasks. This involved recognizing and distinguishing between different accents, which is a complex task requiring nuanced understanding of audio inputs .\n",
    "\n",
    "- **Multimodal Tasks**: The evaluation also included multimodal tasks that required the integration of visual and linguistic data. This aspect of the evaluation aimed to assess GPT-4o's ability to synthesize information from multiple sources, enhancing its responses with contextually enriched content  .\n",
    "\n",
    "These tasks collectively aimed to provide a comprehensive evaluation of GPT-4o's capabilities across different domains, highlighting both its strengths and areas for improvement.\n",
    "\n",
    "### Comparison of GPT-4o to Previous Models\n",
    "\n",
    "- **Performance on MBE**: GPT-4o demonstrates a significant improvement over GPT-3.5, achieving a 75% accuracy compared to GPT-3.5's 45.10% on the MBE (Multistate Bar Examination) sample questions. This indicates a substantial enhancement in its ability to handle complex legal questions and provide accurate responses  .\n",
    "\n",
    "- **Advancements Over GPT-4**: While GPT-4o performs comparably to GPT-4 on the MBE, it introduces new capabilities in vision, speech, and multimodal tasks, which were not as developed in GPT-4. This expansion into cross-modal activities marks a significant step forward in the model's versatility and application potential  .\n",
    "\n",
    "- **Comparison with Other Models**: The study also compares GPT-4o with other contemporary models like Google's Gemini and Anthropic's Claude 3. The findings suggest that GPT-4o offers notable improvements in multimodal tasks, setting it apart from its predecessors and competitors in terms of integrating visual and linguistic data .\n",
    "\n",
    "- **Strengths and Limitations**: GPT-4o excels in tasks requiring few-shot learning and demonstrates high accuracy and efficiency across multiple domains. However, it still faces challenges with complex and ambiguous inputs, particularly in audio and vision capabilities, indicating areas where further development is needed .\n",
    "\n",
    "- **Innovations in Speech and Vision**: The introduction of speech capabilities in GPT-4o is a new innovation, although it has faced some criticism regarding the choice of voice. Its vision capabilities have also been expanded, allowing for more comprehensive evaluations beyond what was possible with previous models like GPT-4V .\n",
    "\n",
    "Overall, GPT-4o represents a significant advancement over previous models, particularly in its ability to handle a wider range of tasks across different modalities. However, it also highlights the ongoing need for improvements in handling complex inputs and further development of evaluation frameworks to fully assess its capabilities.\n",
    "\n",
    "### Research Gap GPT-4o\n",
    "\n",
    "- The research highlights a significant gap in the comprehensive evaluation of GPT-4o, as existing studies have not rigorously tested the model across diverse tasks and datasets, limiting the understanding of its full capabilities and weaknesses.  (Shahriar et al., n.d.)\n",
    "\n",
    "- There is a lack of qualitative assessments and human judgment in evaluating the model's performance, which is crucial for understanding practical usability and contextual accuracy.  (Shahriar et al., n.d.)\n",
    "\n",
    "- Additionally, the evaluation datasets, particularly for image and audio data, were relatively small and not exhaustive, necessitating further expansion to capture the model's performance across various scenarios.   (Shahriar et al., n.d.)\n",
    "\n",
    "### Future Research GPT-4o\n",
    "\n",
    "- Future research should focus on expanding evaluation datasets to include a more diverse range of tasks, enhancing understanding of the model's capabilities and limitations.  (Shahriar et al., n.d.)\n",
    "\n",
    "- Integrating real-time and longitudinal data assessments can provide insights into the model's adaptability and performance stability over time.  (Shahriar et al., n.d.)\n",
    "\n",
    "- Refining few-shot learning techniques is essential, particularly for tasks with high variability.  (Shahriar et al., n.d.)\n",
    "\n",
    "- Investigating advanced prompting strategies and the impact of prompt quality on model performance is crucial.  (Shahriar et al., n.d.)\n",
    "\n",
    "- Conducting thorough error analysis to understand the reasons behind low performance will inform targeted training efforts.  (Shahriar et al., n.d.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8029d676-acaf-46a1-93e0-abc73da44885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>Summarized Abstract</th>\n",
       "      <th>Conclusions</th>\n",
       "      <th>Methods Used</th>\n",
       "      <th>Objectives</th>\n",
       "      <th>Findings</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Research Gap</th>\n",
       "      <th>Future Research</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>language_understanding_paper.pdf</td>\n",
       "      <td>- The paper presents a method for improving na...</td>\n",
       "      <td>- The paper introduces a framework that enhanc...</td>\n",
       "      <td>- The paper employs a two-stage training proce...</td>\n",
       "      <td>- The research objectives of the paper focus o...</td>\n",
       "      <td>- The research demonstrates that generative pr...</td>\n",
       "      <td>- The study utilized several datasets for vari...</td>\n",
       "      <td>- The research paper does not explore the pote...</td>\n",
       "      <td>- Future research aims to enhance understandin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>language_models_are_unsupervised_multitask_lea...</td>\n",
       "      <td>- The paper demonstrates that language models,...</td>\n",
       "      <td>- The paper concludes that large language mode...</td>\n",
       "      <td>- The paper employs unsupervised learning meth...</td>\n",
       "      <td>- The primary objective of the research paper ...</td>\n",
       "      <td>- The research demonstrates that language mode...</td>\n",
       "      <td>- The study primarily utilized a new dataset c...</td>\n",
       "      <td>- The research identifies a gap in understandi...</td>\n",
       "      <td>- Future research should explore unsupervised ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Language Models are Few-Shot Learners.pdf</td>\n",
       "      <td>- The paper presents GPT-3, a 175 billion para...</td>\n",
       "      <td>- The paper presents a 175 billion parameter l...</td>\n",
       "      <td>- The paper focuses on three primary methods f...</td>\n",
       "      <td>- The research paper aims to demonstrate the e...</td>\n",
       "      <td>- The research presents GPT-3, a 175 billion p...</td>\n",
       "      <td>- The study utilized the Common Crawl dataset,...</td>\n",
       "      <td>- The paper identifies a lack of understanding...</td>\n",
       "      <td>- Future research is expected to focus on char...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt-4.pdf</td>\n",
       "      <td>- The paper reports on the development of GPT-...</td>\n",
       "      <td>- The paper concludes that GPT-4 is a large mu...</td>\n",
       "      <td>- The paper employs a methodology that include...</td>\n",
       "      <td>- The research objectives of the paper include...</td>\n",
       "      <td>- The research findings indicate that GPT-4 is...</td>\n",
       "      <td>- The study utilized the MMLU benchmark, which...</td>\n",
       "      <td>- The paper identifies that GPT-4 has various ...</td>\n",
       "      <td>- Future research should focus on robust evalu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Putting GPT-4o to the Sword.pdf</td>\n",
       "      <td>- The research paper evaluates the comprehensi...</td>\n",
       "      <td>- The research concludes that GPT-4o demonstra...</td>\n",
       "      <td>- The research employs standardized exam quest...</td>\n",
       "      <td>- The primary objective of the research is to ...</td>\n",
       "      <td>- The research findings indicate that GPT-4o d...</td>\n",
       "      <td>- The study utilized several datasets to evalu...</td>\n",
       "      <td>- The research highlights a significant gap in...</td>\n",
       "      <td>- Future research should focus on expanding ev...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                   language_understanding_paper.pdf   \n",
       "1  language_models_are_unsupervised_multitask_lea...   \n",
       "2          Language Models are Few-Shot Learners.pdf   \n",
       "3                                          gpt-4.pdf   \n",
       "4                    Putting GPT-4o to the Sword.pdf   \n",
       "\n",
       "                                 Summarized Abstract  \\\n",
       "0  - The paper presents a method for improving na...   \n",
       "1  - The paper demonstrates that language models,...   \n",
       "2  - The paper presents GPT-3, a 175 billion para...   \n",
       "3  - The paper reports on the development of GPT-...   \n",
       "4  - The research paper evaluates the comprehensi...   \n",
       "\n",
       "                                         Conclusions  \\\n",
       "0  - The paper introduces a framework that enhanc...   \n",
       "1  - The paper concludes that large language mode...   \n",
       "2  - The paper presents a 175 billion parameter l...   \n",
       "3  - The paper concludes that GPT-4 is a large mu...   \n",
       "4  - The research concludes that GPT-4o demonstra...   \n",
       "\n",
       "                                        Methods Used  \\\n",
       "0  - The paper employs a two-stage training proce...   \n",
       "1  - The paper employs unsupervised learning meth...   \n",
       "2  - The paper focuses on three primary methods f...   \n",
       "3  - The paper employs a methodology that include...   \n",
       "4  - The research employs standardized exam quest...   \n",
       "\n",
       "                                          Objectives  \\\n",
       "0  - The research objectives of the paper focus o...   \n",
       "1  - The primary objective of the research paper ...   \n",
       "2  - The research paper aims to demonstrate the e...   \n",
       "3  - The research objectives of the paper include...   \n",
       "4  - The primary objective of the research is to ...   \n",
       "\n",
       "                                            Findings  \\\n",
       "0  - The research demonstrates that generative pr...   \n",
       "1  - The research demonstrates that language mode...   \n",
       "2  - The research presents GPT-3, a 175 billion p...   \n",
       "3  - The research findings indicate that GPT-4 is...   \n",
       "4  - The research findings indicate that GPT-4o d...   \n",
       "\n",
       "                                             Dataset  \\\n",
       "0  - The study utilized several datasets for vari...   \n",
       "1  - The study primarily utilized a new dataset c...   \n",
       "2  - The study utilized the Common Crawl dataset,...   \n",
       "3  - The study utilized the MMLU benchmark, which...   \n",
       "4  - The study utilized several datasets to evalu...   \n",
       "\n",
       "                                        Research Gap  \\\n",
       "0  - The research paper does not explore the pote...   \n",
       "1  - The research identifies a gap in understandi...   \n",
       "2  - The paper identifies a lack of understanding...   \n",
       "3  - The paper identifies that GPT-4 has various ...   \n",
       "4  - The research highlights a significant gap in...   \n",
       "\n",
       "                                     Future Research  \n",
       "0  - Future research aims to enhance understandin...  \n",
       "1  - Future research should explore unsupervised ...  \n",
       "2  - Future research is expected to focus on char...  \n",
       "3  - Future research should focus on robust evalu...  \n",
       "4  - Future research should focus on expanding ev...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "file_path = '../../Papers/GPT/2024-10-14_18_45_30_export.csv'  # Change this path if your CSV file is located elsewhere\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Set pandas options for maximum view\n",
    "#pd.set_option('display.max_columns', None)       # Show all columns\n",
    "#pd.set_option('display.max_rows', None)          # Show all rows\n",
    "#pd.set_option('display.max_colwidth', None)      # Show entire content of each cell\n",
    "#pd.set_option('display.width', None)             # Adjust display width to fit the notebook's width\n",
    "\n",
    "# Reset only the specific options you've changed\n",
    "pd.reset_option('display.max_columns')\n",
    "pd.reset_option('display.max_rows')\n",
    "pd.reset_option('display.max_colwidth')\n",
    "pd.reset_option('display.width')\n",
    "\n",
    "# Reset all display settings to their defaults\n",
    "#pd.reset_option('all')\n",
    "\n",
    "# Set display options back to default values\n",
    "#pd.set_option('display.max_columns', 0)    # Default is 0 (pandas chooses the number of columns to display)\n",
    "#pd.set_option('display.max_rows', 60)      # Default number of rows to display (often 60)\n",
    "#pd.set_option('display.max_colwidth', 50)  # Default colwidth (usually 50)\n",
    "#pd.set_option('display.width', 80)         # Default width (usually 80)\n",
    "\n",
    "# Drop the 'TL;DR' column from the DataFrame\n",
    "df = df.drop(columns=['TL;DR'])\n",
    "\n",
    "# Display the updated DataFrame to verify the column is removed\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b735c6e-dd25-4b2b-9740-fd445704bc08",
   "metadata": {},
   "source": [
    "## Overview of the GPT Family\n",
    "\n",
    "The GPT (Generative Pre-trained Transformer) family represents a series of language models developed by OpenAI, each iteration building upon the capabilities of its predecessors. Here's a summary of the evolution and features of the GPT family:\n",
    "\n",
    "- **GPT-1**: The first model in the series, GPT-1, introduced the concept of pre-training a transformer model on a large corpus of text data, followed by fine-tuning on specific tasks. This approach demonstrated the potential of transfer learning in natural language processing (NLP).\n",
    "\n",
    "- **GPT-2**: GPT-2 significantly increased the model size and training data, leading to improved performance across a variety of NLP tasks. It gained attention for its ability to generate coherent and contextually relevant text, sparking discussions about the ethical implications of AI-generated content.\n",
    "\n",
    "- **GPT-3**: With 175 billion parameters, GPT-3 marked a substantial leap in model size and capability. It excelled in few-shot and zero-shot learning, allowing it to perform tasks with minimal task-specific data. GPT-3's versatility made it applicable in diverse areas, from creative writing to coding assistance.\n",
    "\n",
    "- **GPT-4**: Building on GPT-3, GPT-4 introduced enhancements in reasoning and understanding complex queries. It also began exploring multimodal capabilities, integrating text with other data types like images, although these features were not fully developed.\n",
    "\n",
    "- **GPT-4o (Omni)**: The latest in the series, GPT-4o, expands significantly on multimodal tasks, integrating vision, speech, and language capabilities. It demonstrates high accuracy and efficiency in language and reasoning tasks, excelling in few-shot learning. However, it still faces challenges with complex and ambiguous inputs, particularly in audio and vision capabilities  .\n",
    "\n",
    "The GPT family has progressively advanced the field of AI, with each model introducing new capabilities and setting benchmarks for language understanding and generation. The evolution from GPT-1 to GPT-4o highlights the increasing complexity and application potential of these models, while also underscoring the need for ongoing research to address their limitations and ethical considerations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf6a444-6125-4114-a988-934d3fc44027",
   "metadata": {},
   "source": [
    "| **Model**       | **Series** | **Parameters**       | **Release** | **Open Source** | **#Tokens**   | **Training Dataset**                                    | **Context Window Size** | **Capabilities**                                       | **Best At**                                 |\n",
    "|-----------------|------------|----------------------|-------------|-----------------|---------------|---------------------------------------------------------|--------------------------|--------------------------------------------------------|----------------------------------------------|\n",
    "| GPT-1           | GPT-1      | 117M                 | 2018        | ✓               | 1.3B          | BooksCorpus, English Wikipedia                          | 2048 tokens             | Basic natural language tasks                           | Basic language tasks                         |\n",
    "| GPT-2 Small     | GPT-2      | 124M                 | 2019        | ✓               | 10B           | Reddit outbound                                         | 2048 tokens             | Improved language understanding                        | Language comprehension improvements          |\n",
    "| GPT-2 Medium    | GPT-2      | 355M                 | 2019        | ✓               | 10B           | Reddit outbound                                         | 2048 tokens             | Enhanced language understanding                        | Better contextual understanding              |\n",
    "| GPT-2 Large     | GPT-2      | 774M                 | 2019        | ✓               | 10B           | Reddit outbound                                         | 2048 tokens             | More fluent and coherent language generation           | Generating coherent, fluent text             |\n",
    "| GPT-2 XL        | GPT-2      | 1.5B                 | 2019        | ✓               | 10B           | Reddit outbound                                         | 2048 tokens             | Advanced language generation                           | High-quality text generation                 |\n",
    "| text-ada-001    | GPT-3      | 350M                 | 2020        | ×               | 300B          | Common Crawl (filtered), WebText2, Books1, Books2, Wikipedia | 4096 tokens             | Simple tasks, fast and cost-effective                  | Fast execution on simple tasks               |\n",
    "| text-babbage-001 | GPT-3     | 1.3B                 | 2020        | ×               | 300B          | Common Crawl (filtered), WebText2, Books1, Books2, Wikipedia | 4096 tokens             | General tasks, balanced speed and cost                 | Balanced performance for general tasks       |\n",
    "| text-curie-001  | GPT-3      | 6.7B                 | 2020        | ×               | 300B          | Common Crawl (filtered), WebText2, Books1, Books2, Wikipedia | 4096 tokens             | Broader tasks, more capable                            | Broader tasks with better contextual understanding |\n",
    "| text-davinci-003 | GPT-3     | 175B                | 2020        | ×               | 300B          | Common Crawl (filtered), WebText2, Books1, Books2, Wikipedia | 4096 tokens             | Advanced tasks, complex reasoning                      | Complex tasks with advanced reasoning        |\n",
    "| Codex           | GPT-3      | 12B                  | 2021        | ✓               | -             | Public GitHub software repositories                     | 4096 tokens             | Programming and code generation                        | Writing and interpreting code                |\n",
    "| WebGPT          | GPT-3      | 760M, 13B, 175B      | 2021        | ×               | -             | ELI5                                                    | 4096 tokens             | Information retrieval and fact-based QA                | Retrieving and synthesizing web-based information |\n",
    "| gpt-3.5-turbo   | GPT-3.5    | 1.3B, 6B, 20B             | 2022        | ×               | 2.5T          | WebText, Common Crawl, Books, Wikipedia                 | 4096 tokens             | Chat-based applications, optimized for conversations   | Conversational tasks and chat applications   |\n",
    "| gpt-4           | GPT-4      | 1.76 Trillion       | 2023        | ×               | 13T           | Diverse internet text                                   | 128,000 tokens          | Advanced reasoning and complex tasks                   | Complex tasks requiring nuanced understanding |\n",
    "| gpt-4-turbo     | GPT-4      | Unknown             | 2023        | ×               | 13T           | Diverse internet text                                   | 128,000 tokens          | Optimized for chat and general use                     | Efficient general and chat-based tasks       |\n",
    "| gpt-4o          | GPT-4      | 220B (Mixture of Experts) | 2024 | × | 13T           | Web, technical documents, and multimodal data           | 128,000 tokens          | Optimized for large-scale reasoning and NLP            | Large-scale NLP tasks and comprehensive reasoning |\n",
    "| gpt-4o-canvas   | GPT-4      | 220B (Experts)       | 2024        | ×               | 13T           | diverse internet text, technical documents, and multimodal data to support a mixture of experts specializing in various tasks | 128,000 tokens          | Collaborative writing and coding interface             | In-depth project collaboration with editing and refinement capabilities |\n",
    "| gpt-4o-mini     | GPT-4      | 8B                   | 2024        | ×               | 13T?          | Web, general internet text, cost-effective subset       | 128,000 tokens          | Cost-effective model with simplified reasoning (hypothetical) | Cost-efficient solutions and basic reasoning |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ccfe8369-7062-48cd-9420-55f62936c184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```mermaid\n",
       "\n",
       "%%{init: {'flowchart': {'curve': 'linear'}}}%%\n",
       "graph LR;\n",
       "    __start__([<p>__start__</p>]):::first\n",
       "    gpt[\"GPT-1<br/>Year: 2018<br/>Params: 117M\"]:::gpt_style\n",
       "    gpt2_small[\"GPT-2 Small<br/>Year: 2019<br/>Params: 124M\"]:::gpt2_style\n",
       "    gpt2_medium[\"GPT-2 Medium<br/>Year: 2019<br/>Params: 355M\"]:::gpt2_style\n",
       "    gpt2_large[\"GPT-2 Large<br/>Year: 2019<br/>Params: 774M\"]:::gpt2_style\n",
       "    gpt2_xl[\"GPT-2 XL<br/>Year: 2019<br/>Params: 1.5B\"]:::gpt2_style\n",
       "    gpt3[\"GPT-3<br/>Year: 2020<br/>Params: 175B\"]:::gpt3_style\n",
       "    gpt35[\"GPT-3.5<br/>Year: 2022<br/>Params: Unknown\"]:::gpt35_style\n",
       "    gpt4[\"GPT-4<br/>Year: 2023<br/>Params: Unknown\"]:::gpt4_style\n",
       "    gpt4_turbo[\"GPT-4 Turbo<br/>Year: 2023<br/>Params: Optimized\"]:::gpt4_style\n",
       "    gpt4o[\"GPT-4o<br/>Year: 2024<br/>Params: Optimized\"]:::gpt4o_style\n",
       "    gpt4o_canvas[\"GPT-4o-canvas<br/>Year: 2024<br/>Params: Visual\"]:::gpt4o_canvas_style\n",
       "    gpt4o_mini[\"GPT-4o-mini<br/>Year: 2024<br/>Params: Smaller Optimized\"]:::gpt4o_mini_style\n",
       "    __end__([<p>__end__</p>]):::last\n",
       "\n",
       "    __start__ --> gpt;\n",
       "    gpt --> gpt2_small;\n",
       "    gpt2_small --> gpt2_medium;\n",
       "    gpt2_medium --> gpt2_large;\n",
       "    gpt2_large --> gpt2_xl;\n",
       "    gpt2_xl --> gpt3;\n",
       "    gpt3 --> gpt35;\n",
       "    gpt35 --> gpt4;\n",
       "    gpt4 --> gpt4_turbo;\n",
       "    gpt4_turbo --> gpt4o;\n",
       "    gpt4o --> gpt4o_canvas;\n",
       "    gpt4o_canvas --> gpt4o_mini;\n",
       "    gpt4o_mini --> __end__;\n",
       "\n",
       "    classDef default fill:#f2f0ff,stroke:#333,stroke-width:2px,line-height:1.2,text-align:left;\n",
       "    classDef first fill-opacity:0;\n",
       "    classDef last fill:#bfb6fc,stroke:#333,stroke-width:2px;\n",
       "    classDef gpt_style fill:#c6dbef,stroke:#1c638e,stroke-width:2px;\n",
       "    classDef gpt2_style fill:#9ecae1,stroke:#1c638e,stroke-width:2px;\n",
       "    classDef gpt3_style fill:#6baed6,stroke:#1c638e,stroke-width:2px;\n",
       "    classDef gpt35_style fill:#5b8fd9,stroke:#1c638e,stroke-width:2px;\n",
       "    classDef gpt4_style fill:#4292c6,stroke:#1c638e,stroke-width:2px;\n",
       "    classDef gpt4o_style fill:#5a8bd8,stroke:#1c638e,stroke-width:2px;\n",
       "    classDef gpt4o_canvas_style fill:#4a75c4,stroke:#1c638e,stroke-width:2px;\n",
       "    classDef gpt4o_mini_style fill:#3a5db2,stroke:#1c638e,stroke-width:2px;\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Define the GPT model progression in Mermaid syntax\n",
    "mermaid_code = \"\"\"\n",
    "%%{init: {'flowchart': {'curve': 'linear'}}}%%\n",
    "graph LR;\n",
    "    __start__([<p>__start__</p>]):::first\n",
    "    gpt[\"GPT-1<br/>Year: 2018<br/>Params: 117M\"]:::gpt_style\n",
    "    gpt2_small[\"GPT-2 Small<br/>Year: 2019<br/>Params: 124M\"]:::gpt2_style\n",
    "    gpt2_medium[\"GPT-2 Medium<br/>Year: 2019<br/>Params: 355M\"]:::gpt2_style\n",
    "    gpt2_large[\"GPT-2 Large<br/>Year: 2019<br/>Params: 774M\"]:::gpt2_style\n",
    "    gpt2_xl[\"GPT-2 XL<br/>Year: 2019<br/>Params: 1.5B\"]:::gpt2_style\n",
    "    gpt3[\"GPT-3<br/>Year: 2020<br/>Params: 175B\"]:::gpt3_style\n",
    "    gpt35[\"GPT-3.5<br/>Year: 2022<br/>Params: Unknown\"]:::gpt35_style\n",
    "    gpt4[\"GPT-4<br/>Year: 2023<br/>Params: Unknown\"]:::gpt4_style\n",
    "    gpt4_turbo[\"GPT-4 Turbo<br/>Year: 2023<br/>Params: Optimized\"]:::gpt4_style\n",
    "    gpt4o[\"GPT-4o<br/>Year: 2024<br/>Params: Optimized\"]:::gpt4o_style\n",
    "    gpt4o_canvas[\"GPT-4o-canvas<br/>Year: 2024<br/>Params: Visual\"]:::gpt4o_canvas_style\n",
    "    gpt4o_mini[\"GPT-4o-mini<br/>Year: 2024<br/>Params: Smaller Optimized\"]:::gpt4o_mini_style\n",
    "    __end__([<p>__end__</p>]):::last\n",
    "\n",
    "    __start__ --> gpt;\n",
    "    gpt --> gpt2_small;\n",
    "    gpt2_small --> gpt2_medium;\n",
    "    gpt2_medium --> gpt2_large;\n",
    "    gpt2_large --> gpt2_xl;\n",
    "    gpt2_xl --> gpt3;\n",
    "    gpt3 --> gpt35;\n",
    "    gpt35 --> gpt4;\n",
    "    gpt4 --> gpt4_turbo;\n",
    "    gpt4_turbo --> gpt4o;\n",
    "    gpt4o --> gpt4o_canvas;\n",
    "    gpt4o_canvas --> gpt4o_mini;\n",
    "    gpt4o_mini --> __end__;\n",
    "\n",
    "    classDef default fill:#f2f0ff,stroke:#333,stroke-width:2px,line-height:1.2,text-align:left;\n",
    "    classDef first fill-opacity:0;\n",
    "    classDef last fill:#bfb6fc,stroke:#333,stroke-width:2px;\n",
    "    classDef gpt_style fill:#c6dbef,stroke:#1c638e,stroke-width:2px;\n",
    "    classDef gpt2_style fill:#9ecae1,stroke:#1c638e,stroke-width:2px;\n",
    "    classDef gpt3_style fill:#6baed6,stroke:#1c638e,stroke-width:2px;\n",
    "    classDef gpt35_style fill:#5b8fd9,stroke:#1c638e,stroke-width:2px;\n",
    "    classDef gpt4_style fill:#4292c6,stroke:#1c638e,stroke-width:2px;\n",
    "    classDef gpt4o_style fill:#5a8bd8,stroke:#1c638e,stroke-width:2px;\n",
    "    classDef gpt4o_canvas_style fill:#4a75c4,stroke:#1c638e,stroke-width:2px;\n",
    "    classDef gpt4o_mini_style fill:#3a5db2,stroke:#1c638e,stroke-width:2px;\n",
    "\"\"\"\n",
    "\n",
    "# Display the Mermaid graph in Jupyter Notebook\n",
    "display(Markdown(f\"```mermaid\\n{mermaid_code}\\n```\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0aead15-26b2-4917-b4fb-4b583bd88535",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<a href=\"https://en.wikipedia.org/wiki/Generative_pre-trained_transformer\" target=\"_blank\">\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Full_GPT_architecture.svg\" alt=\"GPT-1 Architecture\" width=\"500\" height=\"500\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "</a>\n",
    "\n",
    "\n",
    "This breakdown of the GPT architecture focuses on each component in sequence, offering detailed explanations of how they contribute to the overall function of the model. Here's a cleaner version of the explanation:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Input Embedding in GPT:**\n",
    "   - **Purpose:** Converts words or tokens into numerical vectors (dense embeddings) so the model can process them. These embeddings capture semantic relationships between tokens.\n",
    "   - **Explanation:** Words, subwords, or characters are represented as vectors in a continuous space (embeddings). Similar words have similar embeddings, while different words are farther apart in the vector space. Embeddings are learned during training and are passed to the transformer layers.\n",
    "\n",
    "#### **Steps in Input Embedding Process:**\n",
    "1. **Tokenization:** Text is divided into tokens. GPT uses techniques like **Byte-Pair Encoding (BPE)**, which splits text into manageable tokens (e.g., \"running\" -> \"run\" and \"ing\").\n",
    "2. **Vocabulary Mapping:** Each token is mapped to an index in the model's vocabulary. For instance, \"run\" might map to index 563.\n",
    "3. **Embedding Lookup Table:** Each token index is converted into a high-dimensional vector (embedding) using an embedding matrix.\n",
    "4. **Dense Representation (Embedding):** The output is a sequence of vectors representing tokens, passed into the transformer layers for further processing.\n",
    "\n",
    "---\n",
    "\n",
    "### Example:\n",
    "For the sentence `\"The cat sat on the mat.\"`, tokens are mapped to indices and converted to embeddings like:\n",
    "\n",
    "```plaintext\n",
    "Tokenized: [The] [cat] [sat] [on] [the] [mat]\n",
    "Token Indices: [3] [102] [205] [87] [3] [490]\n",
    "Embeddings: [[0.5, -1.3, 0.2, ...], [-0.8, 0.3, 1.1, ...], ...]\n",
    "```\n",
    "\n",
    "These dense embeddings are then fed into the transformer layers for processing.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Positional Encoding:**\n",
    "   - **Explanation:** Since transformers don't inherently understand the order of tokens, positional encoding is added to the embeddings to provide information about token positions in the sequence.\n",
    "\n",
    "#### **How Positional Encoding Works:**\n",
    "1. **Combination of Position and Embedding:** Position is encoded using sinusoidal functions, ensuring each token's position is uniquely represented.\n",
    "2. **Mathematical Formula:**\n",
    "   - For even dimensions:\n",
    "     $$ PE(p, 2i) = \\sin(p / 10000^{2i/d_{\\text{model}}}) $$\n",
    "   - For odd dimensions:\n",
    "     $$ PE(p, 2i+1) = \\cos(p / 10000^{2i/d_{\\text{model}}}) $$\n",
    "\n",
    "Where $p$ is the position, and $d_{\\text{model}}$ is the dimension of the embedding. These positional encodings are added element-wise to the embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "### Example:\n",
    "For the sentence `\"The cat sat on the mat.\"`, positional encodings are added to the embeddings:\n",
    "\n",
    "```plaintext\n",
    "Embedding + Positional Encoding for \"The\": [0.51, -1.28, 0.23, ...]\n",
    "Embedding + Positional Encoding for \"cat\": [-0.78, 0.34, 1.16, ...]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Dropout Layer:**\n",
    "   - **Explanation:** Dropout helps prevent overfitting by randomly \"dropping\" (setting to zero) a percentage of neurons during training. This forces the network to learn more robust patterns.\n",
    "\n",
    "#### **Why Dropout Helps:**\n",
    "- **Prevents Co-Adaptation:** Dropout prevents the model from becoming too reliant on any specific neurons, forcing different neurons to collaborate more effectively.\n",
    "- **Improves Generalization:** Helps the network perform better on unseen data.\n",
    "- **Simulates an Ensemble:** By randomly dropping neurons, dropout behaves like training multiple models that share parameters, improving robustness.\n",
    "\n",
    "During training, dropout randomly sets neurons to zero with a specific probability, e.g., 20% dropout. During inference, dropout is turned off, and the weights are scaled.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Transformer Blocks:**\n",
    "   - **Explanation:** GPT's architecture consists of stacked transformer blocks, each containing self-attention mechanisms, feedforward networks, and normalization layers.\n",
    "\n",
    "#### **Components of a Transformer Block:**\n",
    "1. **Layer Normalization (LayerNorm):** Stabilizes training by normalizing the input to each layer, ensuring consistent activation scaling.\n",
    "2. **Multi-Head Self-Attention:** Allows the model to focus on different parts of the input sequence simultaneously. Each head attends to a different subspace of the input, capturing multiple relationships between tokens.\n",
    "   \n",
    "   - **Query, Key, and Value Matrices:** Self-attention works by creating query, key, and value matrices from the input.\n",
    "   - **Attention Scores:** Calculated by multiplying the query matrix with the transposed key matrix and normalizing with softmax.\n",
    "   - **Weighted Sum of Values:** The attention scores are used to compute a weighted sum of the value matrix, determining the final output of the attention mechanism.\n",
    "   \n",
    "3. **Feedforward Neural Network:** A two-layer network that further transforms the output of the self-attention mechanism. It applies a **GELU activation function** to introduce non-linearity.\n",
    "4. **Residual Connection (+):** Adds the input of a layer back to its output, preventing vanishing gradient problems and stabilizing training.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **LayerNorm and Residual Connections:**\n",
    "   - **Explanation:** LayerNorm and residual connections occur twice in each transformer block, after both the self-attention and feedforward layers.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Feedforward Network (GELU Activation):**\n",
    "   - **Explanation:** The feedforward network transforms the output of the attention mechanism. The **GELU activation** introduces non-linearity, helping the model learn complex patterns.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **Final Output (Linear Layer and Softmax):**\n",
    "   - **Explanation:** After passing through all transformer blocks, the final output is processed by a linear layer and a softmax function to generate probabilities over the vocabulary, enabling next-token prediction during text generation.\n",
    "\n",
    "---\n",
    "\n",
    "This summary covers the key components of the GPT architecture and how each one contributes to the model's ability to process and generate text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff75ed6-6aae-4dd5-b7d8-4cafa166b255",
   "metadata": {},
   "source": [
    "| **Type** | **Model Name** | **#Parameters**  | **Release** | **Base Models** | **Open Source** | **#Tokens** | **Training Dataset** | **Context Window Size** |\n",
    "|-----------------|----------------|--------------------|-------------|-----------------|-----------------|---------------------------|-----------------|------------------|\n",
    "| **GPT Family**  | GPT-1           | 110M                | 2018   | ✓               | ✓               | 1.3B        | BooksCorpus, English Wikipedia |  |\n",
    "|                 | GPT-2           | 1.5B                | 2019   | ✓               | ✓               | 10B         | Reddit outbound                |  |\n",
    "|                 | GPT-3           | 6.7B, 13B, 175B     | 2020   | ×               | ×  | 300B | Common Crawl (filtered), WebText2, Books1, Books2, Wikipedia | |\n",
    "|                 | GPT-3.5         | 1.3B, 6B, 20B       | 2022   | ×               | ×               | 2.5T        | WebText, Common Crawl, Books, Wikipedia | |\n",
    "|                 | CODEX           | 12B                 | 2021   | GPT             | ✓               | -           | Public GitHub software repositories     | |\n",
    "|                 | WebGPT          | 760M, 13B, 175B     | 2021   | GPT-3           | ×               | -           | ELI5                                    | |\n",
    "|| [GPT-4](https://cdn.openai.com/papers/gpt-4.pdf)| 1.76T| 2023   | -               | ×               | 13T         | Diverse internet text | 128,000 tokens|\n",
    "|| [GPT-4o](https://platform.openai.com/docs/models/gpt-4o)| 220B (Experts)| 2024 | GPT-4 | ×  | 13T?                | Web, technical documents, and multimodal data |128,000 tokens |\n",
    "|| [GPT-4o Mini](https://platform.openai.com/docs/models/gpt-4o-mini)| 8B| 2024| GPT-4o| ×             | 13T?        | Cost-effective model for general API usage | 128,000 tokens|\n",
    "\n",
    "**Key updates**:\n",
    "1. **GPT-3.5** was introduced as an intermediate model between GPT-3 and GPT-4, with variants ranging from 1.3B to 20B parameters. It brought improvements in language generation and is used in ChatGPT's free version.\n",
    "2. **GPT-4o**, launched in 2024, uses a \"Mixture of Experts\" architecture with experts specialized in different tasks. It consists of multiple expert models (220B parameters per expert) that together sum up to a total of 1.76 trillion parameters.\n",
    "3. **GPT-4o Mini** is a lighter and more affordable version of GPT-4o, specifically aimed at smaller applications and businesses needing cost-effective AI solutions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d0725e-71b5-4f91-b4ab-327c1dc0bfe9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
