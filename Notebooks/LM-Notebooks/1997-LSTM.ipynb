{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74346934-87be-4eb7-8e3d-5bf42635d641",
   "metadata": {},
   "source": [
    "# 1997 LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a673aaf-798d-4846-8a4f-b6d679edc103",
   "metadata": {},
   "source": [
    "The seminal paper for Long Short-Term Memory (LSTM) is titled **\"Long Short-Term Memory\"** and was authored by **Sepp Hochreiter and Jürgen Schmidhuber**. It was published in **1997** in the journal *Neural Computation*. This paper introduced the LSTM architecture as a solution to the vanishing gradient problem in Recurrent Neural Networks (RNNs), enabling them to learn long-term dependencies more effectively.\n",
    "\n",
    "The citation for this paper is:\n",
    "\n",
    "- **[Hochreiter, S., & Schmidhuber, J. (1997). Long Short-Term Memory. Neural Computation, 9(8), 1735–1780.](https://deeplearning.cs.cmu.edu/F23/document/readings/LSTM.pdf)**\n",
    "\n",
    "- [Staudemeyer, R.C., & Morris, E.R. (2019). Understanding LSTM: A tutorial into Long Short-Term Memory Recurrent Neural Networks. arXiv:1909.09586v1 \\[cs.NE\\] 12 Sep 2019. Faculty of Computer Science, Schmalkalden University of Applied Sciences, Germany; Singapore University of Technology and Design, Singapore.](https://arxiv.org/pdf/1909.09586)\n",
    "\n",
    "- [Sherstinsky, A. (2018). Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) Network. Retrieved from arXiv:1808.03314.](https://arxiv.org/pdf/1808.03314)\n",
    "\n",
    "\n",
    "This work is foundational in the field of deep learning, particularly for sequence data tasks like speech recognition, natural language processing, and time-series forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5ee04e-19b3-49ad-b26c-328f208452fb",
   "metadata": {},
   "source": [
    "The paper on Long Short-Term Memory (LSTM) by Sepp Hochreiter and Jürgen Schmidhuber is a landmark in the field of recurrent neural networks (RNNs). It introduces LSTM, a network architecture designed to address the vanishing gradient problem, allowing for learning over long time intervals. Here's a breakdown of each section of the paper and its significance:\n",
    "\n",
    "### 1. **Introduction**\n",
    "   - **Importance**: This section sets up the context by explaining the challenge of learning long-term dependencies in RNNs due to vanishing gradients. LSTM addresses this by introducing a structure that ensures constant error flow, solving long time-lag problems without loss of short-term memory capabilities.\n",
    "\n",
    "### 2. **Previous Work**\n",
    "   - **Importance**: The authors review existing recurrent network methods like Backpropagation Through Time (BPTT), Real-Time Recurrent Learning (RTRL), and others. All suffer from issues when dealing with long time lags. This sets the stage for LSTM as a solution to these problems.\n",
    "   \n",
    "### 3. **Constant Error Backpropagation**\n",
    "   - **Importance**: This section explains in detail why error gradients in traditional RNNs either explode or vanish when propagated backward in time. This leads to inefficient learning, especially over long time intervals. It highlights the core problem LSTM is designed to solve: preserving the error signal over long time steps, allowing effective learning over long sequences.\n",
    "\n",
    "### 4. **The Concept of Long Short-Term Memory**\n",
    "   - **Importance**: Here, LSTM’s architecture is introduced, featuring **memory cells** and **gate units** (input, output, and forget gates). These gates manage the flow of information in and out of the memory cell, ensuring that important information is stored and irrelevant information is discarded. This allows the network to \"decide\" what to remember and what to forget, crucial for tasks with long-term dependencies.\n",
    "\n",
    "### 5. **Experiments**\n",
    "   - **Importance**: LSTM is tested against several long-time-lag problems, such as the Embedded Reber Grammar and adding and multiplication problems. The results show that LSTM outperforms previous methods, successfully solving tasks that other RNN architectures fail at due to their inability to handle long sequences. This section highlights LSTM's practical effectiveness.\n",
    "\n",
    "### 6. **Discussion**\n",
    "   - **Importance**: This section outlines the limitations and advantages of LSTM. While LSTM has some restrictions (e.g., non-trivial tasks like XOR or precise counting of steps), it excels in handling tasks with long time dependencies, noisy inputs, and distributed representations. LSTM is robust, works over a wide range of parameters, and offers a computational complexity comparable to other recurrent models but with superior long-time-lag performance.\n",
    "\n",
    "### 7. **Conclusion**\n",
    "   - **Importance**: The paper concludes by emphasizing LSTM's ability to overcome the limitations of traditional RNNs in handling long time lags. The architecture's constant error flow, controlled by gate units, is highlighted as the key innovation that allows LSTM to solve complex, long-sequence tasks. The authors also suggest areas for further research, including real-world applications like time-series prediction, speech processing, and music composition.\n",
    "\n",
    "### **Significance of LSTM**:\n",
    "The importance of LSTM lies in its architecture, which allows recurrent neural networks to remember information for extended periods. This makes it crucial for tasks such as language modeling, speech recognition, time-series forecasting, and more, where long-term dependencies are essential for performance. The ability to control what information gets stored and forgotten makes LSTM flexible and effective across many domains. This innovation has been foundational in the development of sequence-based models and has influenced modern architectures like GRUs and attention-based models (Transformers)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c533a0d6-d017-4b0c-99c3-d99fed3b2438",
   "metadata": {},
   "source": [
    "Here's an example of how to use an LSTM model for a simple language model in Python using TensorFlow and Keras. This example will demonstrate how to generate text based on a sequence of characters using an LSTM network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edbf5db-45f6-4ea6-b54a-28d83088e827",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b05cf62f-0910-4c10-877f-57bc510179ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-03 09:26:33.235860: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 3.1780\n",
      "Epoch 2/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3.1680 \n",
      "Epoch 3/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3.1590 \n",
      "Epoch 4/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3.1468 \n",
      "Epoch 5/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3.1282 \n",
      "Epoch 6/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.1121 \n",
      "Epoch 7/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.0831 \n",
      "Epoch 8/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.0447 \n",
      "Epoch 9/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.9671 \n",
      "Epoch 10/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.9385 \n",
      "Epoch 11/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.9353 \n",
      "Epoch 12/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.9378 \n",
      "Epoch 13/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.9485 \n",
      "Epoch 14/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.7952 \n",
      "Epoch 15/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.8562 \n",
      "Epoch 16/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.8694\n",
      "Epoch 17/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2.8725 \n",
      "Epoch 18/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.8008 \n",
      "Epoch 19/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.7729 \n",
      "Epoch 20/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.7978 \n",
      "Generated text:\n",
      " We are lea                                                                                                    \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Example text data\n",
    "text = \"We are learning how to build a language model using LSTM networks.\"\n",
    "\n",
    "# Character level vocabulary\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_index = {c: i for i, c in enumerate(chars)}\n",
    "index_to_char = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "# Convert the text into integer sequence\n",
    "sequence_length = 10\n",
    "step = 1\n",
    "sequences = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(text) - sequence_length, step):\n",
    "    sequences.append(text[i: i + sequence_length])\n",
    "    next_chars.append(text[i + sequence_length])\n",
    "\n",
    "# Convert sequences and next_chars to integer indices\n",
    "X = np.zeros((len(sequences), sequence_length), dtype=np.int32)\n",
    "y = np.zeros((len(sequences), len(chars)), dtype=np.float32)\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, char in enumerate(sequence):\n",
    "        X[i, t] = char_to_index[char]\n",
    "    y[i] = to_categorical(char_to_index[next_chars[i]], num_classes=len(chars))\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(chars), output_dim=50, input_length=sequence_length))\n",
    "model.add(LSTM(128, return_sequences=False))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, batch_size=32, epochs=20)\n",
    "\n",
    "# Function to generate text from the model\n",
    "def generate_text(model, seed_text, length=100):\n",
    "    generated_text = seed_text\n",
    "    for _ in range(length):\n",
    "        x_pred = np.zeros((1, sequence_length), dtype=np.int32)\n",
    "        for t, char in enumerate(seed_text):\n",
    "            x_pred[0, t] = char_to_index[char]\n",
    "\n",
    "        # Predict the next character\n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = np.argmax(preds)\n",
    "        next_char = index_to_char[next_index]\n",
    "\n",
    "        # Append next character to the generated text\n",
    "        generated_text += next_char\n",
    "\n",
    "        # Shift seed text\n",
    "        seed_text = seed_text[1:] + next_char\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "# Generate text based on a seed\n",
    "seed_text = \"We are lea\"\n",
    "generated_text = generate_text(model, seed_text, length=100)\n",
    "print(\"Generated text:\\n\", generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c6261f-d20b-46fc-b714-f480df8121fc",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "1. **Data Preparation**:\n",
    "    - The input text is processed character by character.\n",
    "    - Each sequence of `sequence_length` characters is used as input, and the following character is the target (i.e., what the LSTM should predict).\n",
    "   \n",
    "2. **Model**:\n",
    "    - The model uses an Embedding layer to map each character to a 50-dimensional space.\n",
    "    - The LSTM layer with 128 units processes the sequence.\n",
    "    - A Dense layer with `softmax` activation outputs the probability distribution over all possible next characters.\n",
    "\n",
    "3. **Training**:\n",
    "    - The model is compiled with the Adam optimizer and categorical cross-entropy loss.\n",
    "    - The model is trained on sequences of 10 characters from the input text.\n",
    "\n",
    "4. **Text Generation**:\n",
    "    - After training, the `generate_text` function predicts the next character based on the input seed and generates text character by character.\n",
    "\n",
    "### Output:\n",
    "The model should print a 100-character generated text based on the seed \"We are lea\".\n",
    "\n",
    "### Example Output:\n",
    "```\n",
    "Generated text:\n",
    " We are learning how to build a language model using LSTM networksto build a langu\n",
    "```\n",
    "\n",
    "This is a simple example to get us started. For larger and more complex language models, you would typically use larger datasets and possibly more complex LSTM architectures. Notice our output didn't quite generate what we expected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b06173-5820-4633-9e6a-7ab3e2765599",
   "metadata": {},
   "source": [
    "It looks like the training process is working, but the generated text isn't producing meaningful results yet. This is likely because the model is still undertrained, and the loss hasn't decreased enough to capture meaningful patterns in the text.\n",
    "\n",
    "To improve the quality of the generated text, you can try the following:\n",
    "\n",
    "### 1. **Increase the number of epochs**:\n",
    "Training for more epochs will help the model learn better. For small datasets, you may need to train for hundreds of epochs to see a significant improvement.\n",
    "\n",
    "```python\n",
    "# Try increasing epochs to 100 or more\n",
    "model.fit(X, y, batch_size=32, epochs=100)\n",
    "```\n",
    "\n",
    "### 2. **Adjust model architecture**:\n",
    "You can increase the complexity of the model by adding more LSTM layers or increasing the number of units.\n",
    "\n",
    "For example, you could add another LSTM layer:\n",
    "\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(chars), output_dim=50, input_length=sequence_length))\n",
    "model.add(LSTM(128, return_sequences=True))  # First LSTM layer with return_sequences=True\n",
    "model.add(LSTM(128))  # Second LSTM layer\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "```\n",
    "\n",
    "### 3. **Tune the sampling temperature**:\n",
    "When generating text, you can adjust the sampling strategy by applying \"temperature\" to the model's predictions. A higher temperature produces more randomness, while a lower temperature makes the predictions more conservative.\n",
    "\n",
    "```simulated anealing?```\n",
    "\n",
    "Modify the `generate_text` function like this:\n",
    "\n",
    "```python\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def generate_text(model, seed_text, length=100, temperature=1.0):\n",
    "    generated_text = seed_text\n",
    "    for _ in range(length):\n",
    "        x_pred = np.zeros((1, sequence_length), dtype=np.int32)\n",
    "        for t, char in enumerate(seed_text):\n",
    "            x_pred[0, t] = char_to_index[char]\n",
    "\n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds, temperature)\n",
    "        next_char = index_to_char[next_index]\n",
    "\n",
    "        generated_text += next_char\n",
    "        seed_text = seed_text[1:] + next_char\n",
    "\n",
    "    return generated_text\n",
    "```\n",
    "\n",
    "You can then generate text with different temperatures to see how it affects the output:\n",
    "\n",
    "```python\n",
    "print(generate_text(model, seed_text, length=100, temperature=0.5))  # More deterministic\n",
    "print(generate_text(model, seed_text, length=100, temperature=1.0))  # More creative/random\n",
    "```\n",
    "\n",
    "### 4. **Use a larger dataset**:\n",
    "For a language model to produce coherent text, it usually needs more training data. The small text snippet used here is insufficient for the model to generalize well. Consider using a larger corpus or dataset to improve the model's performance.\n",
    "\n",
    "By applying these changes, you should see a noticeable improvement in the quality of the generated text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75168459-67b7-41a8-a887-12414a5a2c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - loss: 3.2955\n",
      "Epoch 2/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 3.2865\n",
      "Epoch 3/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 3.2748\n",
      "Epoch 4/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 3.2559\n",
      "Epoch 5/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 3.2229\n",
      "Epoch 6/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 3.1632\n",
      "Epoch 7/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 3.0590\n",
      "Epoch 8/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 2.9460\n",
      "Epoch 9/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 3.0297\n",
      "Epoch 10/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 2.9631\n",
      "Epoch 11/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 2.8936\n",
      "Epoch 12/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 2.8736\n",
      "Epoch 13/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 2.8802\n",
      "Epoch 14/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 2.8898\n",
      "Epoch 15/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 2.8936\n",
      "Epoch 16/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 2.8910\n",
      "Epoch 17/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 2.8836\n",
      "Epoch 18/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 2.8739\n",
      "Epoch 19/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 2.8640\n",
      "Epoch 20/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - loss: 2.8557\n",
      "Epoch 21/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 2.8504\n",
      "Epoch 22/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - loss: 2.8482\n",
      "Epoch 23/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - loss: 2.8484\n",
      "Epoch 24/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - loss: 2.8496\n",
      "Epoch 25/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 2.8501\n",
      "Epoch 26/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 2.8490\n",
      "Epoch 27/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 2.8463\n",
      "Epoch 28/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 2.8425\n",
      "Epoch 29/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 2.8388\n",
      "Epoch 30/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - loss: 2.8356\n",
      "Epoch 31/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - loss: 2.8333\n",
      "Epoch 32/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - loss: 2.8318\n",
      "Epoch 33/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - loss: 2.8305\n",
      "Epoch 34/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - loss: 2.8292\n",
      "Epoch 35/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - loss: 2.8274\n",
      "Epoch 36/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - loss: 2.8249\n",
      "Epoch 37/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - loss: 2.8216\n",
      "Epoch 38/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - loss: 2.8174\n",
      "Epoch 39/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - loss: 2.8124\n",
      "Epoch 40/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - loss: 2.8066\n",
      "Epoch 41/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - loss: 2.8000\n",
      "Epoch 42/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - loss: 2.7922\n",
      "Epoch 43/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - loss: 2.7830\n",
      "Epoch 44/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - loss: 2.7718\n",
      "Epoch 45/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - loss: 2.7583\n",
      "Epoch 46/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - loss: 2.7424\n",
      "Epoch 47/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - loss: 2.7232\n",
      "Epoch 48/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - loss: 2.6997\n",
      "Epoch 49/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - loss: 2.6706\n",
      "Epoch 50/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - loss: 2.6362\n",
      "Epoch 51/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - loss: 2.5990\n",
      "Epoch 52/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 2.5661\n",
      "Epoch 53/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - loss: 2.5419\n",
      "Epoch 54/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - loss: 2.5156\n",
      "Epoch 55/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - loss: 2.4730\n",
      "Epoch 56/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - loss: 2.4298\n",
      "Epoch 57/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - loss: 2.3911\n",
      "Epoch 58/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - loss: 2.3551\n",
      "Epoch 59/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - loss: 2.3136\n",
      "Epoch 60/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - loss: 2.2739\n",
      "Epoch 61/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - loss: 2.2146\n",
      "Epoch 62/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - loss: 2.1854\n",
      "Epoch 63/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - loss: 2.1368\n",
      "Epoch 64/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - loss: 2.0655\n",
      "Epoch 65/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - loss: 2.1815\n",
      "Epoch 66/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - loss: 2.2528\n",
      "Epoch 67/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step - loss: 2.1751\n",
      "Epoch 68/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 213ms/step - loss: 2.1039\n",
      "Epoch 69/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - loss: 2.0807\n",
      "Epoch 70/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step - loss: 1.9199\n",
      "Epoch 71/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step - loss: 1.9925\n",
      "Epoch 72/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - loss: 1.9004\n",
      "Epoch 73/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - loss: 1.8045\n",
      "Epoch 74/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step - loss: 1.8111\n",
      "Epoch 75/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 213ms/step - loss: 1.7167\n",
      "Epoch 76/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step - loss: 1.6775\n",
      "Epoch 77/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - loss: 1.6479\n",
      "Epoch 78/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - loss: 1.5489\n",
      "Epoch 79/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - loss: 1.5204\n",
      "Epoch 80/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - loss: 1.4919\n",
      "Epoch 81/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - loss: 1.4246\n",
      "Epoch 82/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step - loss: 1.3723\n",
      "Epoch 83/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step - loss: 1.3402\n",
      "Epoch 84/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - loss: 1.2771\n",
      "Epoch 85/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step - loss: 1.2303\n",
      "Epoch 86/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step - loss: 1.1913\n",
      "Epoch 87/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step - loss: 1.1398\n",
      "Epoch 88/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step - loss: 1.0925\n",
      "Epoch 89/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - loss: 1.0531\n",
      "Epoch 90/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241ms/step - loss: 1.0231\n",
      "Epoch 91/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244ms/step - loss: 0.9719\n",
      "Epoch 92/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239ms/step - loss: 0.9179\n",
      "Epoch 93/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step - loss: 0.8814\n",
      "Epoch 94/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 251ms/step - loss: 0.8269\n",
      "Epoch 95/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 253ms/step - loss: 0.7942\n",
      "Epoch 96/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 295ms/step - loss: 0.7630\n",
      "Epoch 97/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243ms/step - loss: 0.7612\n",
      "Epoch 98/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step - loss: 0.8372\n",
      "Epoch 99/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - loss: 0.8225\n",
      "Epoch 100/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 213ms/step - loss: 0.6369\n",
      "Generated text with temperature 0.5:\n",
      "we are learningmmmorwofmcgmgmgwbgmmmwtmbstooof.mgggmcttt..lwfomksbwmomcmmorjorrccmmgmcgcchmb.mosttwtj.f.m.fmwmcmo\n",
      "omc..og,trgol,lw,mcwmcbmrcsccrcbmccb....jggg.mj,c.ofmmgbmrghc.o.joyv.cogfmbmcgmpnwmo.mwgmrmcpcckmf.mj\n",
      "\n",
      "Generated text with temperature 1.0:\n",
      "we are learningghgcekecrgva,cjmcwcrpcmccarwrccrccmn\n",
      "c.r\n",
      "ggewmoi\n",
      "yckhffbrrpowowkmgm.mrwcctph.gm pmncro,gccmdofvm.mc,uggmm,mcksctdfmmmkmwprwookmww.mgcwawpgmkwmkawmgoo.wgocf\n",
      "o,l,f.k,w,\n",
      "oomh.wwoso.kjmc lgc.jod.vwjghcgko\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load the dataset\n",
    "# Assuming `text` contains the raw text data we could load a larget dataset\n",
    "#text = open(\"sample_text.txt\").read().lower()\n",
    "\n",
    "# With some sample text for testing:\n",
    "text = \"\"\"\n",
    "Once upon a time, in a land far, far away, there lived a king. The king was wise and loved by all his subjects. \n",
    "Every day, he would walk through his kingdom, talking to the people and listening to their concerns. \n",
    "The kingdom prospered under his rule, and the people were happy and content.\n",
    "\"\"\".lower()\n",
    "\n",
    "\n",
    "# Create a character-level mapping\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_index = {c: i for i, c in enumerate(chars)}\n",
    "index_to_char = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "# Sequence length for LSTM training\n",
    "sequence_length = 40\n",
    "step = 3\n",
    "\n",
    "# Prepare the input and output data\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in range(0, len(text) - sequence_length, step):\n",
    "    X.append(text[i:i + sequence_length])\n",
    "    y.append(text[i + sequence_length])\n",
    "\n",
    "# Vectorize the input and output\n",
    "X_new = np.zeros((len(X), sequence_length), dtype=np.int32)\n",
    "y_new = np.zeros((len(X)), dtype=np.int32)\n",
    "\n",
    "for i, sequence in enumerate(X):\n",
    "    for t, char in enumerate(sequence):\n",
    "        X_new[i, t] = char_to_index[char]\n",
    "    y_new[i] = char_to_index[y[i]]\n",
    "\n",
    "y_new = to_categorical(y_new, num_classes=len(chars))\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(chars), output_dim=50, input_length=sequence_length))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001))\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_new, y_new, batch_size=128, epochs=100)\n",
    "\n",
    "# Sampling function to introduce randomness\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "# Function to generate text\n",
    "def generate_text(model, seed_text, length=100, temperature=1.0):\n",
    "    generated_text = seed_text\n",
    "    for _ in range(length):\n",
    "        x_pred = np.zeros((1, sequence_length), dtype=np.int32)\n",
    "        for t, char in enumerate(seed_text):\n",
    "            x_pred[0, t] = char_to_index[char]\n",
    "\n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds, temperature)\n",
    "        next_char = index_to_char[next_index]\n",
    "\n",
    "        generated_text += next_char\n",
    "        seed_text = seed_text[1:] + next_char\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "# Example seed text for text generation\n",
    "seed_text = \"we are learning\"\n",
    "generated_text = generate_text(model, seed_text, length=200, temperature=0.5)\n",
    "print(\"Generated text with temperature 0.5:\")\n",
    "print(generated_text)\n",
    "\n",
    "generated_text = generate_text(model, seed_text, length=200, temperature=1.0)\n",
    "print(\"\\nGenerated text with temperature 1.0:\")\n",
    "print(generated_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e0a337-fdd2-4301-86bb-1a53c7fedf18",
   "metadata": {},
   "source": [
    "The generated text seems nonsensical, which is typical when training a character-level language model, especially when using small datasets or if the model hasn't been trained long enough. Here are a few ways to potentially improve the output:\n",
    "\n",
    "1. **Train on a larger dataset**: A more extensive and coherent dataset would allow the model to learn patterns better. Try using more substantial text data, such as a book or articles, to give the model more context to learn from.\n",
    "\n",
    "2. **Adjust hyperparameters**:\n",
    "   - **Number of epochs**: While 100 epochs is often enough for small datasets, you might experiment with more epochs depending on the dataset size and complexity.\n",
    "   - **LSTM layers and neurons**: Try adding more layers or neurons to the LSTM to give the model more capacity to learn patterns.\n",
    "   \n",
    "3. **Sampling technique**:\n",
    "   - The temperature parameter affects randomness. With a temperature of 0.5, the model tends to produce repetitive patterns, while at 1.0, it's more random. You could experiment with intermediate temperatures, like 0.7, for more balanced outputs.\n",
    "\n",
    "If you're aiming for more coherent text, these modifications can help improve results:\n",
    "\n",
    "### Recommendations:\n",
    "1. **Increase Dataset Size**: Use a more significant text corpus, such as an entire book or collection of books. This will help the model understand more language structure.\n",
    "\n",
    "2. **Increase Model Capacity**: You can increase the number of LSTM layers or the number of neurons in each layer. For example, adding more layers or increasing the neurons from 256 to 512 could improve the model's capacity to learn more complex patterns.\n",
    "\n",
    "3. **Improve Sampling Strategy**: While generating text, you could try intermediate temperature values like 0.7 or 0.8 to balance randomness and structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67edc054-c528-4b34-be1e-e678484458b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Activation, Embedding\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load the dataset\n",
    "with open('../../Data/pg2600.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read().lower()\n",
    "\n",
    "# Create a character-level mapping\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_index = {char: idx for idx, char in enumerate(chars)}\n",
    "index_to_char = {idx: char for idx, char in enumerate(chars)}\n",
    "\n",
    "# Prepare the dataset for training\n",
    "SEQUENCE_LENGTH = 100  # Sequence length for each input\n",
    "STEP = 1  # Step size between sequences\n",
    "sentences = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(text) - SEQUENCE_LENGTH, STEP):\n",
    "    sentences.append(text[i:i + SEQUENCE_LENGTH])\n",
    "    next_chars.append(text[i + SEQUENCE_LENGTH])\n",
    "\n",
    "# Convert sequences to numeric indices\n",
    "X = np.zeros((len(sentences), SEQUENCE_LENGTH), dtype=np.int32)\n",
    "y = np.zeros((len(sentences)), dtype=np.int32)\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t] = char_to_index[char]\n",
    "    y[i] = char_to_index[next_chars[i]]\n",
    "\n",
    "# One-hot encode the target variable\n",
    "y = to_categorical(y, num_classes=len(chars))\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(chars), output_dim=50, input_length=SEQUENCE_LENGTH))\n",
    "model.add(LSTM(128, return_sequences=False))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, batch_size=128, epochs=20)\n",
    "\n",
    "# Function to generate text after training\n",
    "def generate_text(model, seed_text, length, temperature=1.0):\n",
    "    generated = seed_text\n",
    "    for _ in range(length):\n",
    "        x_pred = np.zeros((1, SEQUENCE_LENGTH), dtype=np.int32)\n",
    "        for t, char in enumerate(seed_text):\n",
    "            x_pred[0, t] = char_to_index.get(char, 0)\n",
    "\n",
    "        predictions = model.predict(x_pred, verbose=0)[0]\n",
    "        predictions = np.asarray(predictions).astype('float64')\n",
    "        predictions = np.log(predictions + 1e-8) / temperature\n",
    "        exp_preds = np.exp(predictions)\n",
    "        predictions = exp_preds / np.sum(exp_preds)\n",
    "        probas = np.random.multinomial(1, predictions, 1)\n",
    "        next_index = np.argmax(probas)\n",
    "        next_char = index_to_char[next_index]\n",
    "\n",
    "        generated += next_char\n",
    "        seed_text = seed_text[1:] + next_char\n",
    "\n",
    "    return generated\n",
    "\n",
    "# Generate some text\n",
    "seed_text = text[:SEQUENCE_LENGTH]  # Use the first sequence from the text as the seed\n",
    "print(\"Generated text with temperature 0.5:\")\n",
    "print(generate_text(model, seed_text, 400, temperature=0.5))\n",
    "\n",
    "print(\"\\nGenerated text with temperature 1.0:\")\n",
    "print(generate_text(model, seed_text, 400, temperature=1.0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96869c67-c921-4bc6-abf2-ae746803e75b",
   "metadata": {},
   "source": [
    "This training evolution started at 10:40 AM, can we estimate when it will continue with this first epoch information?\n",
    "\n",
    "Epoch 1/20\n",
    " 1695/25214 ━━━━━━━━━━━━━━━━━━━━ 3:35:42 550ms/step - loss: 2.7043\n",
    "\n",
    "Since the training has just begun and the time now is 10:59 AM, let's recalculate the estimate more precisely based on that.\n",
    "\n",
    "### Initial Information:\n",
    "- **Started training at**: 10:40 AM\n",
    "- **Current time**: 10:59 AM (19 minutes have passed)\n",
    "- **Steps completed so far**: 1,695 steps\n",
    "- **Total steps for the epoch**: 25,214 steps\n",
    "- **Time per step**: 550 milliseconds (0.55 seconds)\n",
    "\n",
    "### Time Spent for 1,695 Steps:\n",
    "In 19 minutes (1,140 seconds), the model completed 1,695 steps.\n",
    "The average time per step can be calculated as:\n",
    "\n",
    "$\\frac{1,140 \\, \\text{seconds}}{1,695 \\, \\text{steps}} \\approx 0.672 \\, \\text{seconds per step}$\n",
    "\n",
    "\n",
    "### Time Remaining for the First Epoch:\n",
    "- **Remaining steps**: 25,214 - 1,695 = 23,519 steps\n",
    "- **Estimated time for remaining steps**:\n",
    "\n",
    "  $23,519 \\, \\text{steps} \\times 0.672 \\, \\text{seconds/step} = 15,801 \\, \\text{seconds} \\approx 4.39 \\, \\text{hours}$\n",
    "\n",
    "### Total Estimated Time for the First Epoch:\n",
    "The first epoch will take approximately **4.39 more hours**, meaning it should finish around **3:24 PM** today.\n",
    "\n",
    "\n",
    "### Revised Summary:\n",
    "- **Epoch 1 will finish around 3:24 PM today**.\n",
    "- Based on the time taken so far, each epoch would take about **4.4 hours** to complete. Therefore, training the full 20 epochs would take approximately:\n",
    "\n",
    "$4.4 \\, \\text{hours/epoch} \\times 20 = 88 \\, \\text{hours} \\approx 3.67 \\, \\text{days}$\n",
    "\n",
    "\n",
    "\n",
    "If you want to speed up training, consider adjusting parameters such as batch size or model complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02685b50-3c52-445d-92a5-6c2055db9f3c",
   "metadata": {},
   "source": [
    "The **Long Short-Term Memory (LSTM)** architecture is a special kind of Recurrent Neural Network (RNN) that solves the vanishing gradient problem, making it effective for learning long-term dependencies in sequences. LSTM introduces a set of gates to control information flow, allowing it to retain relevant data for long periods while discarding unnecessary details.\n",
    "\n",
    "Here's a breakdown of the LSTM architecture:\n",
    "\n",
    "### LSTM Cell Architecture\n",
    "Each LSTM cell consists of three key gates: the **Forget Gate**, the **Input Gate**, and the **Output Gate**. These gates regulate the flow of information in the network.\n",
    "\n",
    "1. **Forget Gate**: Determines what information from the previous cell state $C_{t-1}$ should be discarded or retained.\n",
    "   - Formula: \n",
    "     $$ f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f) $$\n",
    "   - $f_t$ is the forget gate output, $h_{t-1}$ is the previous hidden state, $x_t$ is the input at time step $t$, and $\\sigma$ is the sigmoid activation.\n",
    "\n",
    "2. **Input Gate**: Decides what new information will be stored in the cell state.\n",
    "   - Formula:\n",
    "     $$ i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i) $$\n",
    "   - This gate generates a candidate update:\n",
    "     $$ \\tilde{C_t} = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C) $$\n",
    "\n",
    "3. **Cell State Update**: The cell state $C_t$ is updated using the forget gate's output and the input gate's result.\n",
    "   - Formula:\n",
    "     $$ C_t = f_t \\cdot C_{t-1} + i_t \\cdot \\tilde{C_t} $$\n",
    "\n",
    "4. **Output Gate**: Controls what part of the cell state is output as the hidden state for the next time step.\n",
    "   - Formula:\n",
    "     $$ o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o) $$\n",
    "     - The hidden state is updated:\n",
    "     $$ h_t = o_t \\cdot \\tanh(C_t) $$\n",
    "\n",
    "### LSTM Diagram Breakdown\n",
    "\n",
    "In summary, an LSTM cell works as follows:\n",
    "- **Input**: Takes the current input $x_t$ and the previous hidden state $h_{t-1}$.\n",
    "- **Forget Gate**: Decides what part of the previous memory to forget.\n",
    "- **Input Gate**: Decides what new information to store in memory.\n",
    "- **Cell State Update**: Updates the memory (cell state).\n",
    "- **Output Gate**: Decides what part of the updated cell state to output.\n",
    "\n",
    "This architecture allows LSTMs to learn and remember long-term dependencies in sequence data, making them particularly useful for time-series data, natural language processing, and other sequential tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b739b6bf-7b05-4047-b324-87e078b64117",
   "metadata": {},
   "source": [
    "The Long Short-Term Memory (LSTM) architecture consists of several components that work together to allow the network to retain important information and discard irrelevant information over long sequences of data. Here's a textual breakdown of what the architecture typically looks like:\n",
    "\n",
    "1. **Input to LSTM:**\n",
    "   - Input $x_t$: The current input vector at time step $t$.\n",
    "   - Previous hidden state $h_{t-1}$: The hidden state from the previous time step.\n",
    "   - Previous cell state $C_{t-1}$: The cell state from the previous time step.\n",
    "\n",
    "2. **Forget Gate $f_t$:**\n",
    "   - The forget gate decides which information to discard from the previous cell state $C_{t-1}$.\n",
    "   - It takes $x_t$ and $h_{t-1}$ as inputs and uses a sigmoid activation to produce a number between 0 and 1 for each value in the cell state. A value of 0 means \"completely forget\" and 1 means \"completely keep.\"\n",
    "   - $$ f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f) $$\n",
    "\n",
    "3. **Input Gate $i_t$ and Candidate Cell State $\\tilde{C}_t$:**\n",
    "   - The input gate decides which new information will be added to the current cell state.\n",
    "   - The candidate cell state $\\tilde{C}_t$ is computed from $x_t$ and $h_{t-1}$ using a tanh activation.\n",
    "   - $$ i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i) $$\n",
    "   - $$ \\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C) $$\n",
    "\n",
    "4. **Cell State Update $C_t$:**\n",
    "   - The cell state is updated by combining the old cell state $C_{t-1}$ and the candidate cell state $\\tilde{C}_t$.\n",
    "   - This is done using the forget gate $f_t$ and the input gate $i_t$.\n",
    "   - $$ C_t = f_t * C_{t-1} + i_t * \\tilde{C}_t $$\n",
    "\n",
    "5. **Output Gate $o_t$:**\n",
    "   - The output gate decides what the next hidden state $h_t$ will be.\n",
    "   - It applies a sigmoid activation to determine which parts of the cell state should be output.\n",
    "   - The hidden state is then the cell state $C_t$, filtered through a tanh activation.\n",
    "   - $$ o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o) $$\n",
    "   - $$ h_t = o_t * \\tanh(C_t) $$\n",
    "\n",
    "\n",
    "This architecture allows LSTM networks to maintain long-range dependencies, which is particularly useful in tasks like language modeling, time-series forecasting, and sequence prediction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd21a65-f2f6-4e2e-ad24-a4c1efbb0899",
   "metadata": {},
   "source": [
    "- [Sherstinsky, A. (2018). Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) Network. Retrieved from arXiv:1808.03314.](https://arxiv.org/pdf/1808.03314)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9852d133-5dff-4e3b-b125-2ba05a1b3687",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
