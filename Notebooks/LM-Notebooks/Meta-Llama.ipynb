{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0132bd99-abe8-4fde-9642-4738938c49ce",
   "metadata": {},
   "source": [
    "<a href=\"https://ai.meta.com/\" target=\"_blank\">\n",
    "    <img src=\"https://scontent.forf1-4.fna.fbcdn.net/v/t39.8562-6/252294889_575082167077436_6034106545912333281_n.svg/meta-logo-primary_standardsize.svg?_nc_cat=1&ccb=1-7&_nc_sid=e280be&_nc_ohc=E3vN3ZQbVPwQ7kNvgFupN27&_nc_ht=scontent.forf1-4.fna&_nc_gid=AAZpUpv3aqRnMKBnXdssagA&oh=00_AYAKW8N74m1VRdiVLz-Y4oXD7eC2d9gvwm0ICC2ivtndKw&oe=671FF8B9\" alt=\"MetaAI Logo\" width=\"300\" height=\"100\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "</a>\n",
    "\n",
    "\n",
    "- [AI at Meta](https://ai.meta.com/)\n",
    "- [Llama](https://www.llama.com/)\n",
    "- [HuggingFace Meta Llama](https://huggingface.co/meta-llama)\n",
    "- [Llama Docs](https://www.llama.com/docs/overview)\n",
    "- [Llama Paper](https://arxiv.org/pdf/2302.13971)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b570ac9-30e3-452c-a830-3986b9bec543",
   "metadata": {},
   "source": [
    "## Setup Ollama\n",
    "\n",
    "[Setup for your computer](https://www.llama.com/docs/llama-everywhere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0825ea57-7996-4592-8b62-5f91c8425f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The novel \"The Godfather\" was written by Mario Puzo. The book was published in 1969 and became a huge success, selling millions of copies worldwide.\n",
      "\n",
      "Mario Puzo was an American author, screenwriter, and journalist who is best known for his novels about the Italian-American Mafia. \"The Godfather\" is considered one of his most famous works, and it has been adapted into a successful film trilogy by Francis Ford Coppola, which starred Marlon Brando, Al Pacino, James Caan, Robert Duvall, and Diane Keaton.\n",
      "\n",
      "Puzo's novel tells the story of the Corleone family, an Italian-American Mafia family, and their rise to power in New York City. The book explores themes of family, loyalty, betrayal, and violence, and it has been praised for its vivid portrayal of the inner workings of the Mafia and its cultural significance.\n",
      "\n",
      "Mario Puzo wrote several other novels about the Corleone family and the Mafia, including \"The Last Don\" and \"The Fourth K,\" but \"The Godfather\" remains his most famous and influential work.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"http://localhost:11434/api/chat\"\n",
    "\n",
    "def llama3(prompt):\n",
    "    data = {\n",
    "        \"model\": \"llama3\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        \"stream\": False,\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    return response.json()[\"message\"][\"content\"]\n",
    "\n",
    "response = llama3(\"who wrote the book godfather\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e3253f-6f5b-4f3f-a538-058e16c7e868",
   "metadata": {},
   "source": [
    "Here’s the updated table with the information about **LLaMA-3.1** and **LLaMA-3.2** models, along with links to [Llama 3.2's blog post](https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/) and the main [Llama website](https://www.llama.com/).\n",
    "\n",
    "| **Model**                                                                                                                                                         | **Series** | **Parameters**      | **Release** | **Open Source** | **#Tokens**   | **Training Dataset**                                     | **Context Window Size** | **Capabilities**                                       | **Best At**                                  |\n",
    "|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------|---------------------|-------------|-----------------|---------------|----------------------------------------------------------|--------------------------|--------------------------------------------------------|-----------------------------------------------|\n",
    "| [LLaMA-1 7B](https://arxiv.org/pdf/2302.13971)                                                     | LLaMA      | 7B                 | 2023        | ✓               | 1T            | Common Crawl, C4, GitHub, Wikipedia, ArXiv               | 2048 tokens             | Language understanding, basic tasks                    | Cost-effective language tasks                |\n",
    "| [LLaMA-1 13B](https://arxiv.org/pdf/2302.13971)                                                    | LLaMA      | 13B                | 2023        | ✓               | 1T            | Common Crawl, C4, GitHub, Wikipedia, ArXiv               | 2048 tokens             | Language generation and general NLP                    | Generative language tasks                    |\n",
    "| [LLaMA-1 30B](https://arxiv.org/pdf/2302.13971)                                                    | LLaMA      | 30B                | 2023        | ✓               | 1T            | Common Crawl, C4, GitHub, Wikipedia, ArXiv               | 2048 tokens             | Advanced language understanding                        | High-quality NLP tasks                       |\n",
    "| [LLaMA-1 65B](https://arxiv.org/pdf/2302.13971)                                                    | LLaMA      | 65B                | 2023        | ✓               | 1T            | Common Crawl, C4, GitHub, Wikipedia, ArXiv               | 2048 tokens             | Complex NLP tasks, deep contextual understanding       | Complex and nuanced language tasks           |\n",
    "| [LLaMA-2 7B](https://arxiv.org/pdf/2307.09288)                                                                                                                   | LLaMA 2    | 7B                 | 2023        | ✓               | 2T            | Refined Common Crawl, GitHub, Wikipedia, ArXiv           | 4096 tokens             | General NLP tasks, better context understanding        | Cost-effective NLP                           |\n",
    "| [LLaMA-2 13B](https://arxiv.org/pdf/2307.09288)                                                                                                                  | LLaMA 2    | 13B                | 2023        | ✓               | 2T            | Refined Common Crawl, GitHub, Wikipedia, ArXiv           | 4096 tokens             | Enhanced language generation                           | Balanced language tasks                      |\n",
    "| [LLaMA-2 34B](https://arxiv.org/pdf/2307.09288)                                                                                                                  | LLaMA 2    | 34B                | 2023        | ✓               | 2T            | Refined Common Crawl, GitHub, Wikipedia, ArXiv           | 4096 tokens             | High-quality NLP, deeper contextual understanding      | Complex NLP tasks, balanced performance       |\n",
    "| [LLaMA-2 70B](https://arxiv.org/pdf/2307.09288)                                                                                                                  | LLaMA 2    | 70B                | 2023        | ✓               | 2T            | Refined Common Crawl, GitHub, Wikipedia, ArXiv           | 4096 tokens             | Advanced generative tasks, complex reasoning           | High-quality generation and reasoning tasks  |\n",
    "| [Code LLaMA](https://arxiv.org/pdf/2308.12950)                                                                                                                   | LLaMA      | 7B, 13B, 34B       | 2023        | ✓               | 2T            | Code-heavy datasets from GitHub and Stack Overflow       | 4096 tokens             | Code generation and understanding                      | Writing and debugging code                   |\n",
    "| [LLaMA-3 7B](https://arxiv.org/pdf/2407.21783)                                                                                                | LLaMA 3    | 7B                 | 2024        | ✓               | 3T            | Diverse internet sources, curated for quality            | 8192 tokens             | Language tasks, cost-effective solutions               | Balanced performance                         |\n",
    "| [LLaMA-3 34B](https://arxiv.org/pdf/2407.21783)                                                                                               | LLaMA 3    | 34B                | 2024        | ✓               | 3T            | Diverse internet sources, curated for quality            | 8192 tokens             | Advanced language generation and comprehension         | Complex language tasks                       |\n",
    "| [LLaMA-3 150B](https://arxiv.org/pdf/2407.21783)                                                                                              | LLaMA 3    | 150B               | 2024        | ✓               | 3T            | Diverse internet sources, curated for quality            | 8192 tokens             | Highly nuanced NLP and reasoning tasks                 | Large-scale NLP and comprehensive reasoning   |\n",
    "| [Code LLaMA-3](https://arxiv.org/pdf/2407.21783)                                                                                              | LLaMA 3    | 34B                | 2024        | ✓               | 3T            | Code-heavy datasets from diverse coding sources          | 8192 tokens             | Advanced code generation and multi-language support    | Writing, refactoring, and explaining code    |\n",
    "| [LLaMA-3 Mini](https://arxiv.org/pdf/2407.21783)                                                                                              | LLaMA 3    | 1B                 | 2024        | ✓               | 3T            | Light web and general text sources                       | 4096 tokens             | Cost-effective, simplified NLP                         | Low-cost solutions, basic NLP               |\n",
    "| [LLaMA-3.1 405B](https://www.llama.com/)                                                                                                      | LLaMA 3.1  | 405B, 70B, 8B     | 2024        | ✓               | 4T            | Diverse internet sources, multilingual                   | 8192 tokens             | Multilingual state-of-the-art NLP                     | Large-scale NLP, multilingual tasks          |\n",
    "| [LLaMA-3.2 1B & 3B](https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/)                                             | LLaMA 3.2  | 1B, 3B            | 2024        | ✓               | 4T            | Multilingual, text-only, optimized for edge and mobile   | 4096 tokens             | Lightweight, on-device NLP                            | Mobile and edge deployment                  |\n",
    "| [LLaMA-3.2 11B & 90B](https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/)                                           | LLaMA 3.2  | 11B, 90B          | 2024        | ✓               | 4T            | Multimodal (text-image), flexible and high-res image support | 8192 tokens             | Advanced multimodal processing                         | High-resolution image and text processing    |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9817e1-a535-4b74-80d5-01605dadf70d",
   "metadata": {},
   "source": [
    "# LLaMA-1\n",
    "\n",
    "## Summarized Abstract\n",
    "The paper introduces **LLaMA**, a series of foundation language models with parameters ranging from 7B to 65B, trained on trillions of tokens using only publicly available datasets (Touvron et al., n.d.).\n",
    "\n",
    "- **LLaMA-13B** outperforms **GPT-3** (175B) on most benchmarks, while **LLaMA-65B** competes with top models like **Chinchilla-70B** and **PaLM-540B**.\n",
    "- The authors emphasize the importance of open-sourcing these models to foster research and address issues such as bias and toxicity.\n",
    "- The paper aims to demonstrate that state-of-the-art performance can be achieved without proprietary data.\n",
    "\n",
    "## Summarized Introduction\n",
    "The paper introduces LLaMA, highlighting its model sizes (7B to 65B parameters) and training on publicly available datasets.\n",
    "\n",
    "- Large language models (LLMs) can perform various tasks from textual instructions or examples, where scaling model and dataset sizes is crucial for performance.\n",
    "- **LLaMA-13B** outperforms **GPT-3**, despite being smaller, and **LLaMA-65B** competes with models like **Chinchilla** and **PaLM**.\n",
    "- The goal is to democratize access to LLMs and address bias and toxicity issues in AI.\n",
    "\n",
    "## Methods Used\n",
    "The methodology relies on the **transformer architecture** with optimizations adapted from recent models like PaLM.\n",
    "\n",
    "- **Training Data**: Publicly available datasets, including arXiv, Stack Exchange, GitHub, Wikipedia, and CommonCrawl, to ensure data diversity and quality.\n",
    "- **Optimizations**:\n",
    "  - Efficient causal multi-head attention implementation.\n",
    "  - Reduced memory usage via checkpointing.\n",
    "  - **Rotary positional embeddings** and **AdamW optimizer** with specific hyperparameters.\n",
    "\n",
    "## Objectives\n",
    "The paper presents LLaMA models that are openly released and competitive with state-of-the-art models.\n",
    "\n",
    "- Demonstrates that high performance is achievable without proprietary data.\n",
    "- Investigates the effects of fine-tuning on instructions, showing promising results and indicating future releases of larger models trained on extensive corpora.\n",
    "\n",
    "## Findings\n",
    "The research showcases **LLaMA-13B** and **LLaMA-65B**, which outperform **GPT-3** and are competitive with larger models.\n",
    "\n",
    "- State-of-the-art performance achieved using only public data, avoiding proprietary datasets.\n",
    "- Fine-tuning on instruction data significantly improves performance.\n",
    "- Evaluations across benchmarks indicate steady performance improvements aligned with training perplexity.\n",
    "\n",
    "## Dataset\n",
    "The training dataset for LLaMA consists of various public sources to ensure open-source compatibility.\n",
    "\n",
    "- **Primary Data**: English CommonCrawl, preprocessed to filter low-quality and non-English content.\n",
    "- **Additional Sources**:\n",
    "  - **Gutenberg Project** (public domain books).\n",
    "  - **Books3** from **ThePile**.\n",
    "- **Preprocessing**: Deduplication at line and book levels to maintain data quality; C4 dataset undergoes similar preprocessing for quality filtering.\n",
    "\n",
    "## Research Gap\n",
    "The paper identifies areas for further investigation, including:\n",
    "\n",
    "- The impact of instruction fine-tuning on performance.\n",
    "- Exhaustive evaluations of model biases and toxicity.\n",
    "- The need for future releases of larger models for scalability and improved performance.\n",
    "\n",
    "## Future Research\n",
    "The authors plan to focus on instruction fine-tuning, as initial results appear promising.\n",
    "\n",
    "- Future work will include releasing larger models trained on more extensive datasets, exploring robustness, and addressing bias and toxicity concerns.\n",
    "- Overall goal: Enhance foundation language models' performance and applicability across various tasks.\n",
    "\n",
    "## References\n",
    "- Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., & Azhar, F. (n.d.). *LLaMA Open and Efficient Foundation Language Models*. [Link to Llama-1 paper](https://arxiv.org/pdf/2302.13971).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92a06dd-b7bc-4ef0-8795-0d7b8e912b7c",
   "metadata": {},
   "source": [
    "# LLaMA-2\n",
    "\n",
    "## Summarized Abstract\n",
    "The paper introduces **LLaMA 2**, a collection of pretrained and fine-tuned large language models (LLMs) with parameter scales from 7 billion to 70 billion (Touvron et al., n.d.).\n",
    "\n",
    "- **LLaMA 2-Chat**, the fine-tuned variant, is optimized for dialogue applications and has shown superior performance compared to existing open-source chat models.\n",
    "- The authors emphasize their commitment to safety and helpfulness, providing a detailed methodology for fine-tuning and safety improvements.\n",
    "- The models are intended for both commercial and research purposes, with plans for future enhancements based on community feedback.\n",
    "\n",
    "## Summarized Introduction\n",
    "The paper introduces **LLaMA 2**, optimized for dialogue use cases, with model sizes ranging from 7 billion to 70 billion parameters.\n",
    "\n",
    "- It highlights the capabilities of LLMs in complex reasoning tasks and their rapid adoption due to intuitive chat interfaces.\n",
    "- The authors emphasize the importance of fine-tuning and safety improvements, aiming to provide an open-source alternative to closed-source models.\n",
    "- The methodologies for pretraining, fine-tuning, and safety evaluations are outlined.\n",
    "\n",
    "## Dataset\n",
    "The study utilized a large dataset of over 1 million binary comparisons, referred to as Meta reward modeling data, collected based on specified guidelines.\n",
    "\n",
    "- Compared against open-source preference datasets, including **Anthropic Helpful and Harmless**, **OpenAI Summarize**, **OpenAI WebGPT**, **StackExchange**, **Stanford Human Preferences**, and **Synthetic GPT-J**.\n",
    "- Analysis of the pretraining corpus revealed demographic skews, with a focus on Western demographics.\n",
    "- The prevalence of toxicity in the pretraining corpus was measured using a HateBERT classifier.\n",
    "\n",
    "## Objectives\n",
    "The paper presents a methodology for **discriminative adversarial search** tailored for abstractive summarization.\n",
    "\n",
    "- Outlines pretraining and fine-tuning methodologies to enhance model performance.\n",
    "- Addresses potential safety concerns with large language models (LLMs) and advocates for community research.\n",
    "- Key insights from experiments are shared, contributing to understanding LLM capabilities in practical applications.\n",
    "\n",
    "## Methods Used\n",
    "The paper employs adversarial prompts and safe demonstrations in supervised fine-tuning, aligning the model with safety guidelines before reinforcement learning from human feedback (RLHF).\n",
    "\n",
    "- Pretraining methodology involves an expanded corpus and **grouped-query attention**.\n",
    "- Model architecture and hyperparameters are consistent with pretrained language models, with modifications to the classification head for next-token prediction.\n",
    "- Community research and red teaming are encouraged to address safety concerns.\n",
    "\n",
    "## Findings\n",
    "The **LLaMA 2** models, with parameters from 7 billion to 70 billion, outperform existing open-source chat models on various benchmarks.\n",
    "\n",
    "- **LLaMA 2-Chat** models perform comparably to some closed-source models in human evaluations.\n",
    "- Safety improvements were made through specific data annotation, tuning, and proactive risk identification, like red teaming.\n",
    "- The paper promotes responsible development and transparency by providing open access for research and commercial use.\n",
    "\n",
    "## Research Gap\n",
    "The research identifies limitations in **LLaMA 2-Chat**, particularly its lack of knowledge updates post-pretraining, affecting its ability to provide current information.\n",
    "\n",
    "- Limited pretraining data in non-English languages impacts performance, suggesting a need for more diverse datasets.\n",
    "- Potential for harmful or biased content generation remains a concern, requiring further safety tuning and mitigation.\n",
    "- The necessity for ongoing research to address bias and social issues in real-world deployments of LLMs is acknowledged.\n",
    "\n",
    "## Future Research\n",
    "Future research will focus on enhancing **LLaMA 2-Chat** safety and performance, with ongoing improvements based on community feedback.\n",
    "\n",
    "- Comprehensive evaluations will be conducted on unexplored safety dimensions, particularly in real-world product deployments.\n",
    "- Further research will investigate risks associated with tuned LLMs, including harmful content generation and emergent behaviors.\n",
    "- The societal implications of LLMs, such as job displacement and over-reliance on AI, will be explored.\n",
    "- Studies will aim to mitigate biases and social issues in LLM deployments.\n",
    "\n",
    "## References\n",
    "- Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., & Bhosale, S. (n.d.). *[LLaMA 2 Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/pdf/2307.09288)*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51624334-31cf-4124-b860-5e9678288d10",
   "metadata": {},
   "source": [
    "# Llama-3\n",
    "\n",
    "## Summarized Abstract\n",
    "The paper introduces **Llama 3**, a new set of foundation models designed for modern AI systems, featuring a dense Transformer with **405 billion parameters** and a context window of up to **128K tokens**. Llama 3 supports multilinguality, coding, reasoning, and tool usage, delivering performance comparable to leading models like GPT-4 across various tasks.\n",
    "\n",
    "## Summarized Introduction\n",
    "Llama 3 is designed for various language tasks and supports capabilities like multilinguality, coding, reasoning, and tool usage. The development process includes pre-training and post-training stages, with enhancements in data quality and scale compared to previous models. The flagship model, with 405 billion parameters and 128K token context window, shows competitive performance with leading models such as GPT-4.\n",
    "\n",
    "## Objectives\n",
    "1. **Mathematical Problem Solving**: Measure Llama 3's abilities using the MATH dataset.\n",
    "2. **Task Robustness**: Evaluate model performance in natural image understanding, text comprehension, and multimodal reasoning.\n",
    "3. **Multimodal Capabilities**: Develop functionalities like image, video, and speech recognition.\n",
    "\n",
    "## Methods Used\n",
    "The Llama 3 models use a multi-stage training approach, including:\n",
    "- **Initial Pre-training**\n",
    "- **Long-context Pre-training**\n",
    "- **Annealing**\n",
    "\n",
    "Additionally, a multimodal training strategy was implemented, encompassing language model pre-training, vision adapter training, model finetuning, and speech adapter training. Human annotation procedures and data quality control methods were also applied.\n",
    "\n",
    "## Dataset\n",
    "The pre-training dataset was constructed from various sources of knowledge until the end of 2023, applying de-duplication and data cleaning mechanisms. Domains containing PII and adult content were excluded. To test reasoning capabilities, question-answer pairs, synthetic captions, and standardized tests like GRE, LSAT, SAT, and GMAT were used.\n",
    "\n",
    "## Findings\n",
    "1. **Multilinguality and Tool Usage**: Supports coding and reasoning tasks with the largest model at 405 billion parameters and a 128K token context window.\n",
    "2. **Performance Comparison**: Demonstrates similar performance to GPT-4 across tasks.\n",
    "3. **Data Quality**: Enhanced data quality in both pre-training and post-training stages.\n",
    "4. **Multimodal Capabilities**: Promising results in image, video, and speech recognition, though these capabilities are under development.\n",
    "\n",
    "## Research Gap\n",
    "1. **Lack of Ground Truth Chains of Thought**: Essential for guiding models in step-by-step reasoning.\n",
    "2. **Intermediate Step Errors**: Incorrect intermediate steps in chains of thought can result in incorrect answers.\n",
    "3. **Bias in Human Evaluations**: Challenges in defining objective criteria may lead to biases.\n",
    "4. **Exhaustive Testing Limitations**: Cannot guarantee the identification of all risks associated with model use.\n",
    "\n",
    "## Future Research\n",
    "1. **Multimodal Integration**: Enhance image, video, and speech recognition.\n",
    "2. **Evaluation Metrics**: Explore metrics beyond exact match for model evaluation.\n",
    "3. **Bias Investigation**: Study the impact of human biases in evaluations.\n",
    "4. **Instruction and Preference Curation**: Develop techniques to optimize model performance.\n",
    "\n",
    "## References\n",
    "- Team, L., & Meta, A. (n.d.). *[The Llama 3 Herd of Models](https://arxiv.org/pdf/2407.21783)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e8179f-9628-4cb0-ba8b-971ece0649c3",
   "metadata": {},
   "source": [
    "## Derivative Models\n",
    "\n",
    "| **Derived Model**                                    | **Base Model** | **Parameters**   | **Release** | **Open Source** | **#Tokens** | **Context Window Size** | **Capabilities**                         | **Best At**                           |\n",
    "|------------------------------------------------------|----------------|------------------|-------------|-----------------|-------------|--------------------------|------------------------------------------|---------------------------------------|\n",
    "| [**Alpaca**](https://crfm.stanford.edu/2023/03/13/alpaca.html) | LLaMA-1 7B     | 7B               | 2023        | ✓               | 1T          | 2048 tokens               | Instruction-following, fine-tuned on additional data | Basic instruction tasks              |\n",
    "| [**Vicuna**](https://lmsys.org/blog/2023-03-30-vicuna/) | LLaMA-1 13B    | 13B              | 2023        | ✓               | 1T          | 2048 tokens               | Dialogue optimization, high-quality conversation | Open-domain chat                     |\n",
    "| [**GPT4All**](https://github.com/nomic-ai/gpt4all)  | LLaMA-1 7B     | 7B               | 2023        | ✓               | 1T          | 2048 tokens               | OpenAI fine-tuning, built for general-purpose NLP tasks | Balanced performance NLP            |\n",
    "| [**Baize**](https://github.com/project-baize/baize)  | LLaMA-1 13B    | 13B              | 2023        | ✓               | 1T          | 2048 tokens               | Chatbot fine-tuning with alignment-focused training data | Conversational alignment             |\n",
    "| [**OpenLLaMA**](https://github.com/openlm-research/open_llama) | LLaMA-1 7B, 13B, 30B | 7B, 13B, 30B | 2023        | ✓               | 1T          | 2048 tokens               | Foundation model with optimized open-domain data | General-purpose NLP                  |\n",
    "| [**Camel**](https://huggingface.co/camel-ai)         | LLaMA-1 7B     | 7B               | 2023        | ✓               | 1T          | 2048 tokens               | Instruction tuning with diverse commands         | Instruction tasks, low-resource NLP  |\n",
    "| [**WizardLM**](https://huggingface.co/WizardLM/WizardCoder-13B-V1.0) | LLaMA-2 13B    | 13B              | 2023        | ✓               | 2T          | 4096 tokens               | Extended instruction tuning, improved generalization | Complex instructions                 |\n",
    "| [**Mistral**](https://mistral.ai/blog/)              | LLaMA-2 7B     | 7B               | 2023        | ✓               | 2T          | 4096 tokens               | Open-source with unique modifications for text quality | High-quality text generation         |\n",
    "| [**Phoenix**](https://huggingface.co/Phoenix-Language-Model) | LLaMA-2 13B    | 13B              | 2023        | ✓               | 2T          | 4096 tokens               | Multilingual capabilities with optimized tuning data | Multilingual NLP                     |\n",
    "| [**Orca**](https://microsoft.github.io/Orca/)        | LLaMA-2 13B    | 13B              | 2023        | ✓               | 2T          | 4096 tokens               | Reinforcement learning, reward-tuned, long dialogues | Long-form conversations              |\n",
    "| [**Falcon**](https://falconllm.tii.ae/)              | LLaMA-2 70B    | 70B              | 2023        | ✓               | 2T          | 4096 tokens               | Large-scale text generation, high-quality summaries | Large-scale language tasks           |\n",
    "| [**Koala**](https://bair.berkeley.edu/blog/2023/04/03/koala/) | LLaMA-2 13B    | 13B              | 2023        | ✓               | 2T          | 4096 tokens               | Optimized for conversational AI with chat fine-tuning | Real-time conversation               |\n",
    "| [**Chronos**](https://huggingface.co/Chronos-Language-Model) | LLaMA-2 13B    | 13B              | 2023        | ✓               | 2T          | 4096 tokens               | Knowledge-updated training for recent events    | Current event-based NLP              |\n",
    "| [**Nous-Hermes**](https://huggingface.co/NousResearch/Nous-Hermes-13b) | LLaMA-2 13B    | 13B              | 2023        | ✓               | 2T          | 4096 tokens               | Knowledge retrieval, better contextual understanding | Context-aware generation             |\n",
    "| [**ClaudeLLaMA**](https://www.anthropic.com/index/claude-2) | LLaMA-3 34B    | 34B              | 2024        | ✓               | 3T          | 8192 tokens               | High-quality reasoning, advanced NLP tasks       | Advanced language and reasoning       |\n",
    "| [**LLaVA**](https://llava.github.io/)                | LLaMA-3 34B    | 34B              | 2024        | ✓               | 3T          | 8192 tokens               | Vision-language model, multimodal capabilities  | Image-text understanding             |\n",
    "| [**Atlas**](https://huggingface.co/atlas-lm)         | LLaMA-3 70B    | 70B              | 2024        | ✓               | 3T          | 8192 tokens               | Comprehensive NLP tasks, large context reasoning | Large context generation             |\n",
    "| [**Camel-3**](https://huggingface.co/camel-ai/camel-3b) | LLaMA-3 Mini   | 1B               | 2024        | ✓               | 3T          | 4096 tokens               | Lightweight, on-device optimization               | Edge deployment, lightweight NLP      |\n",
    "| [**StarChat**](https://huggingface.co/starchat)      | LLaMA-3 34B    | 34B              | 2024        | ✓               | 3T          | 8192 tokens               | Open-domain chat with specific dialogue fine-tuning | Dialogue management                  |\n",
    "| [**InsightLM**](https://huggingface.co/Insight-LM)   | LLaMA-3 150B   | 150B             | 2024        | ✓               | 3T          | 8192 tokens               | Nuanced understanding for complex NLP            | High-quality comprehension            |\n",
    "| [**DBRX**](https://databricks.com/blog)              | LLaMA (various) | Custom           | 2024 (est.) | ✗               | Variable    | Variable (optimized for Databricks platform) | Enterprise analytics, data-driven NLP tasks | Analytics-driven NLP                    |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d42ffc-290d-4fd0-95d9-1c585d6e4c5f",
   "metadata": {},
   "source": [
    "## Derivative companies\n",
    "\n",
    "| **Company**                                        | **Model**              | **Base Model** | **Focus**                                                            | **Known For**                                                 | **Model Link**                                                                                   |\n",
    "|----------------------------------------------------|------------------------|----------------|-----------------------------------------------------------------------|----------------------------------------------------------------|-------------------------------------------------------------------------------------------------|\n",
    "| [**Anthropic**](https://www.anthropic.com)        | Claude Series          | LLaMA-3       | Safety-centric, dialogue-focused AI with RLHF                         | Advanced natural language understanding, multi-turn conversations | [Claude](https://www.anthropic.com/index/claude)                                                |\n",
    "| [**Stability AI**](https://stability.ai/)         | StableLM               | LLaMA-1, LLaMA-2 | Accessible text generation for transparency and community research   | Community-driven, open-source models                          | [StableLM](https://stability.ai/blog/stablelm)                                                  |\n",
    "| [**Mistral AI**](https://mistral.ai/)             | Mistral Series         | LLaMA-2       | High-quality text generation, parameter efficiency                    | Compact architecture with quality rivaling larger models       | [Mistral Models](https://mistral.ai/models/)                                                    |\n",
    "| [**Together**](https://together.xyz/)             | RedPajama Series       | LLaMA-1, LLaMA-2 | High-performance, open-source models for research and transparency   | Open dataset and model release processes                      | [RedPajama](https://www.together.xyz/blog/redpajama)                                            |\n",
    "| [**LMSys**](https://vicuna.lmsys.org/)            | Vicuna Series          | LLaMA-1       | Conversational AI with training on open chat data                     | High-quality, responsive chat models                           | [Vicuna](https://vicuna.lmsys.org/)                                                             |\n",
    "| [**Hugging Face**](https://huggingface.co/)       | Falcon and FalconLLaMA | LLaMA-2, LLaMA-3 | Real-time, high-quality generation models                           | Leading platform for open-source LLMs                          | [Falcon Models](https://huggingface.co/models?search=falcon)                                    |\n",
    "| [**MosaicML**](https://www.mosaicml.com/)         | MPT Series             | LLaMA-1, LLaMA-2 | Customizable models for enterprise use cases                         | Optimized for high-performance, business applications          | [MPT Models](https://www.mosaicml.com/blog/mpt-7b)                                              |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a85df15-a1e6-4be3-8815-f4859b194260",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Refences for Llama-1 Paper\n",
    "\n",
    "- Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., & Azhar, F. (n.d.). *LLaMA Open and Efficient Foundation Language Models*. [Link to Llama-1 paper](https://arxiv.org/pdf/2302.13971).\n",
    "\n",
    "| **Author(s)** | **Year** | **Title** | **Link** |\n",
    "|---------------|----------|-----------|----------|\n",
    "| Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, Charles Sutton | 2021 | Program synthesis with large language models | - |\n",
    "| Lalit R Bahl, Frederick Jelinek, Robert L Mercer | 1983 | A maximum likelihood approach to continuous speech recognition | - |\n",
    "| Yoshua Bengio, Réjean Ducharme, Pascal Vincent | 2000 | A neural probabilistic language model | - |\n",
    "| Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi | 2020 | PIQA: Reasoning about physical commonsense in natural language | - |\n",
    "| Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang | 2022 | GPT-NeoX-20B: An open-source autoregressive language model | [arXiv:2204.06745](https://arxiv.org/abs/2204.06745) |\n",
    "| Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och, Jeffrey Dean | 2007 | Large language models in machine translation | - |\n",
    "| Peter F Brown, John Cocke, Stephen A Della Pietra, Vincent J Della Pietra, Frederick Jelinek, John Lafferty, Robert L Mercer, Paul S Roossin | 1990 | A statistical approach to machine translation | - |\n",
    "| Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan | 2020 | Language models are few-shot learners | - |\n",
    "| Christian Buck, Kenneth Heafield, Bas Van Ooyen | 2014 | N-gram counts and language models from the common crawl | - |\n",
    "| Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, Tony Robinson | 2013 | One billion word benchmark for measuring progress in statistical language modeling | [arXiv:1312.3005](https://arxiv.org/abs/1312.3005) |\n",
    "| Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto | 2021 | Evaluating large language models trained on code | - |\n",
    "| Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts | 2022 | PaLM: Scaling language modeling with pathways | - |\n",
    "| Hyung Won Chung, Le Hou, S. Longpre, Barret Zoph, Yi Tay, William Fedus | 2022 | Scaling instruction-finetuned language models | [arXiv:2210.11416](https://arxiv.org/abs/2210.11416) |\n",
    "| Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, Kristina Toutanova | 2019 | BoolQ: Exploring the surprising difficulty of natural yes/no questions | [arXiv:1905.10044](https://arxiv.org/abs/1905.10044) |\n",
    "| Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal | 2018 | Think you have solved question answering? Try ARC, the AI2 reasoning challenge | [arXiv:1803.05457](https://arxiv.org/abs/1803.05457) |\n",
    "| Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen | 2021 | Training verifiers to solve math word problems | [arXiv:2110.14168](https://arxiv.org/abs/2110.14168) |\n",
    "| Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, Ruslan Salakhutdinov | 2019 | Transformer-XL: Attentive language models beyond a fixed-length context | [arXiv:1901.02860](https://arxiv.org/abs/1901.02860) |\n",
    "| Tri Dao, Daniel Y Fu, Stefano Ermon, Atri Rudra, Christopher Ré | 2022 | FlashAttention: Fast and memory-efficient exact attention with IO-awareness | [arXiv:2205.14135](https://arxiv.org/abs/2205.14135) |\n",
    "| Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova | 2018 | BERT: Pre-training of deep bidirectional transformers for language understanding | [arXiv:1810.04805](https://arxiv.org/abs/1810.04805) |\n",
    "| Jeffrey L Elman | 1990 | Finding structure in time | - |\n",
    "| Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang | 2022 | Incoder: A generative model for code infilling and synthesis | [arXiv:2204.05999](https://arxiv.org/abs/2204.05999) |\n",
    "| Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster | 2020 | The Pile: An 800GB dataset of diverse text for language modeling | [arXiv:2101.00027](https://arxiv.org/abs/2101.00027) |\n",
    "| Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou | 2020 | Measuring massive multitask language understanding | [arXiv:2009.03300](https://arxiv.org/abs/2009.03300) |\n",
    "| Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora | 2021 | Measuring mathematical problem solving with the MATH dataset | [arXiv:2103.03874](https://arxiv.org/abs/2103.03874) |\n",
    "| Sepp Hochreiter, Jürgen Schmidhuber | 1997 | Long short-term memory | - |\n",
    "| Tomas Mikolov, Martin Karafiát, Lukas Burget, Jan Cernocky, Sanjeev Khudanpur | 2010 | Recurrent neural network based language model | - |\n",
    "| Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee | 2020 | Exploring the limits of transfer learning with a unified text-to-text transformer | [arXiv:1910.10683](https://arxiv.org/abs/1910.10683) |\n",
    "| Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit | 2017 | Attention is all you need | - |\n",
    "| Ben Wang, Aran Komatsuzaki | 2021 | GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model | [GitHub](https://github.com/kingoflolz/mesh-transformer-jax) |\n",
    "| Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph | 2022 | Emergent abilities of large language models | [arXiv:2206.07682](https://arxiv.org/abs/2206.07682) |\n",
    "| Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary | 2020 | CCNet: Extracting high quality monolingual datasets from web crawl data | - |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de62e46-87c8-4409-97b3-6e710881d7df",
   "metadata": {},
   "source": [
    "# References: Llama-2 Paper\n",
    "\n",
    "- Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., & Bhosale, S. (n.d.). *[LLaMA 2 Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/pdf/2307.09288)*.\n",
    "\n",
    "| **Author(s)** | **Year** | **Title** | **Link** |\n",
    "|---------------|----------|-----------|----------|\n",
    "| Daron Acemoglu, Pascual Restrepo | 2018 | Artificial intelligence, automation, and work | [University of Chicago Press](https://press.uchicago.edu/) |\n",
    "| Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, Sumit Sanghai | 2023 | GQA: Training generalized multi-query transformer models | *No link available* |\n",
    "| Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, et al. | 2023 | Falcon-40B: an open large language model with state-of-the-art performance | *No link available* |\n",
    "| Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, et al. | 2023 | PaLM 2 Technical Report | *No link available* |\n",
    "| Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, et al. | 2021a | A General Language Assistant as a Laboratory for Alignment | [arXiv:2112.00861](https://arxiv.org/abs/2112.00861) |\n",
    "| Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, et al. | 2021b | A General Language Assistant as a Laboratory for Alignment | [arXiv:2112.00861](https://arxiv.org/abs/2112.00861) |\n",
    "| Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, et al. | 2021 | Program Synthesis with Large Language Models | *No link available* |\n",
    "| David Autor, Anna Salomons | 2018 | Is Automation Labor-Displacing? | [National Bureau of Economic Research](https://www.nber.org/) |\n",
    "| Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, et al. | 2022a | Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback | [arXiv:2204.05862](https://arxiv.org/abs/2204.05862) |\n",
    "| Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, et al. | 2022b | Constitutional AI: Harmlessness from AI Feedback | [arXiv:2212.08073](https://arxiv.org/abs/2212.08073) |\n",
    "| April H Bailey, Adina Williams, Andrei Cimpian | 2022 | Based on Billions of Words on the Internet, People= Men | [Science Advances](https://www.science.org/journal/sciadv) |\n",
    "| Emily M Bender, Timnit Gebru, Angelina McMillan-Major, Margaret Mitchell | 2021a | On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? | *No link available* |\n",
    "| Emily M Bender, Timnit Gebru, Angelina McMillan-Major, Shmargaret Shmitchell | 2021b | On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? | *No link available* |\n",
    "| A Stevie Bergman, Gavin Abercrombie, Shannon L Spruit, Dirk Hovy, Emily Dinan, Y-Lan Boureau, Verena Rieser | 2022 | Guiding the Release of Safer E2E Conversational AI Through Value Sensitive Design | *No link available* |\n",
    "| Shaily Bhatt, Sunipa Dev, Partha Talukdar, Shachi Dave, Vinodkumar Prabhakaran | 2022 | Re-contextualizing Fairness in NLP: The Case of India | *No link available* |\n",
    "| Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. | 2020 | PIQA: Reasoning about Physical Commonsense in Natural Language | *No link available* |\n",
    "| Su Lin Blodgett, Gilsinia Lopez, Alexandra Olteanu, Robert Sim, Hanna Wallach | 2021 | Stereotyping Norwegian Salmon: An Inventory of Pitfalls in Fairness Benchmark Datasets | *No link available* |\n",
    "| Piotr Bojanowski, Edouard Grave, Armand Joulin, Tomás Mikolov | 2016 | Enriching Word Vectors with Subword Information | [arXiv:1607.04606](http://arxiv.org/abs/1607.04606) |\n",
    "| Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, et al. | 2020 | Language Models Are Few-Shot Learners | [NeurIPS](https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf) |\n",
    "| Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, et al. | 2021 | Evaluating Large Language Models Trained on Code | *No link available* |\n",
    "| Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, et al. | 2023 | Vicuna: An Open-Source Chatbot Impressing GPT-4 | [URL](https://lmsys.org/blog/2023-03-30-vicuna/) |\n",
    "| Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy Liang, Luke Zettlemoyer | 2018 | QuAC: Question Answering in Context | *No link available* |\n",
    "| Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, et al. | 2022 | PaLM: Scaling Language Modeling with Pathways | *No link available* |\n",
    "| Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, Dario Amodei | 2017 | Deep Reinforcement Learning from Human Preferences | *No link available* |\n",
    "| Hyung Won Chung, Le Hou, S. Longpre, Barret Zoph, Yi Tay, William Fedus, et al. | 2022 | Scaling Instruction-Finetuned Language Models | [arXiv:2210.11416](https://arxiv.org/abs/2210.11416) |\n",
    "| Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, Kristina Toutanova | 2019 | BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions | [arXiv:1905.10044](https://arxiv.org/abs/1905.10044) |\n",
    "| Elizabeth Clark, Tal August, Sofia Serrano, Nikita Haduong, Suchin Gururangan, Noah A. Smith | 2021 | All That’s ‘Human’ is Not Gold | [ACL Anthology](https://aclanthology.org/2021.acl-long.565) |\n",
    "| Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord | 2018 | AI2 Reasoning Challenge | [arXiv:1803.05457](https://arxiv.org/abs/1803.05457) |\n",
    "| Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, et al. | 2021 | Training Verifiers to Solve Math Word Problems | [arXiv:2110.14168](https://arxiv.org/abs/2110.14168) |\n",
    "| Jiawen Deng, Hao Sun, Zhexin Zhang, Jiale Cheng, Minlie Huang | 2023 | Recent Advances Towards Safe, Responsible, and Moral Dialogue Systems: A Survey | *No link available* |\n",
    "| Yuntian Deng, Anton Bakhtin, Myle Ott, Arthur Szlam, Marc'Aurelio Ranzato | 2019 | Residual Energy-Based Models for Text Generation | *No link available* |\n",
    "| Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, Rahul Gupta | 2021 | BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation | *No link available* |\n",
    "| Emily Dinan, Gavin Abercrombie, A Stevie Bergman, Shannon Spruit, Dirk Hovy, Y-Lan Boureau, Verena Rieser | 2021 | Anticipating Safety Issues in E2E Conversational AI: Framework and Tooling | [arXiv:2107.03451](https://arxiv.org/abs/2107.03451) |\n",
    "| Jesse Dodge, Maarten Sap, Ana Marasović, William Agnew, Gabriel Ilharco, Dirk Groeneveld, et al. | 2021 | Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus | [ACL Anthology](https://aclanthology.org/2021.emnlp-main.98) |\n",
    "| Jesse Dodge, Taylor Prewitt, Remi Tachet Des Combes, Erika Odmark, Roy Schwartz, Emma Strubell, et al. | 2022 | Measuring the Carbon Intensity of AI in Cloud Instances | [arXiv:2206.05229](https://arxiv.org/abs/2206.05229) |\n",
    "| Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, et al. | 2022 | GLaM: Efficient Scaling of Language Models with Mixture-of-Experts | [Proceedings of Machine Learning Research](https://proceedings.mlr.press/v162/du22c.html) |\n",
    "| Kawin Ethayarajh, Yejin Choi, Swabha Swayamdipta | 2022 | Understanding Dataset Difficulty with V-Usable Information | [Proceedings of Machine Learning Research](https://proceedings.mlr.press/v162/ethayarajh22.html) |\n",
    "| Prakhar Ganesh, Hongyan Chang, Martin Strobel, Reza Shokri | 2023 | On the Impact of Machine Learning Randomness on Group Fairness | *No link available* |\n",
    "| Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, et al. | 2022 | Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned | [arXiv:2209.07858](https://arxiv.org/abs/2209.07858) |\n",
    "| Deep Ganguli, Amanda Askell, Nicholas Schiefer, Thomas Liao, Kamile Lukošiūte, Anna Chen, et al. | 2023 | The Capacity for Moral Self-Correction in Large Language Models | [arXiv:2302.07459](https://arxiv.org/abs/2302.07459) |\n",
    "| Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, et al. | 2021 | A Framework for Few-Shot Language Model Evaluation | [Zenodo](https://doi.org/10.5281/zenodo.5371628) |\n",
    "| Sebastian Gehrmann, Elizabeth Clark, Thibault Sellam | 2023 | Repairing the Cracked Foundation: A Survey of Obstacles in Evaluation Practices for Generated Text | *No link available* |\n",
    "| Fabrizio Gilardi, Meysam Alizadeh, Maël Kubli | 2023 | ChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks | [arXiv:2303.15056](https://arxiv.org/abs/2303.15056) |\n",
    "| Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, et al. | 2023 | The False Promise of Imitating Proprietary LLMs | [arXiv:2305.15717](https://arxiv.org/abs/2305.15717) |\n",
    "| Udit Gupta, Mariam Elgamal, Gage Hills, Gu-Yeon Wei, Hsien-Hsin S Lee, David Brooks, Carole-Jean Wu | 2022a | ACT: Designing Sustainable Computer Systems with an Architectural Carbon Modeling Tool | *No link available* |\n",
    "| Udit Gupta, Young Guen Kim, Sylvia Lee, Jordan Tse, Hsien-Hsin Sean Lee, Gu-Yeon Wei, et al. | 2022b | Chasing Carbon: The Elusive Environmental Footprint of Computing | *No link available* |\n",
    "| Kilem L. Gwet | 2014 | Handbook of Inter-Rater Reliability | *No link available* |\n",
    "| Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray, Ece Kamar | 2022 | TOXIGEN: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection | [ACL Anthology](https://aclanthology.org/2022.acl-long.387) |\n",
    "| Alex Havrilla | 2022 | Synthetic-Instruct-GPTJ-Pairwise | [Hugging Face](https://huggingface.co/datasets/Dahoas/synthetic-instruct-gptj-pairwise) |\n",
    "| Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen | 2020 | DeBERTa: Decoding-Enhanced BERT with Disentangled Attention | [arXiv:2006.03654](https://arxiv.org/abs/2006.03654) |\n",
    "| Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Xiaodong Song, Jacob Steinhardt | 2020 | Measuring Massive Multitask Language Understanding | [arXiv:2009.03300](https://arxiv.org/abs/2009.03300) |\n",
    "| Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt | 2021 | Measuring Mathematical Problem Solving with the MATH Dataset | [arXiv:2103.03874](https://arxiv.org/abs/2103.03874) |\n",
    "| Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, et al. | 2022 | Training Compute-Optimal Large Language Models | [arXiv:2203.15556](https://arxiv.org/abs/2203.15556) |\n",
    "| Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, Yejin Choi | 2020 | The Curious Case of Neural Text Degeneration | [ICLR](https://openreview.net/forum?id=rygGQyrFvH) |\n",
    "| Or Honovich, Thomas Scialom, Omer Levy, Timo Schick | 2022 | Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor | [arXiv:2212.09689](https://arxiv.org/abs/2212.09689) |\n",
    "| Saghar Hosseini, Hamid Palangi, Ahmed Hassan Awadallah | 2023 | An Empirical Study of Metrics to Measure Representational Harms in Pre-Trained Language Models | [arXiv:2301.09211](https://arxiv.org/abs/2301.09211) |\n",
    "| Fan Huang, Haewoon Kwak, Jisun An | 2023 | Is ChatGPT Better Than Human Annotators? Potential and Limitations of ChatGPT in Explaining Implicit Hate Speech | [arXiv:2302.07736](https://arxiv.org/abs/2302.07736) |\n",
    "| Clayton Hutto, Eric Gilbert | 2014 | VADER: A Parsimonious Rule-Based Model for Sentiment Analysis of Social Media Text | *No link available* |\n",
    "| Mandar Joshi, Eunsol Choi, Daniel S Weld, Luke Zettlemoyer | 2017 | TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension | [arXiv:1705.03551](https://arxiv.org/abs/1705.03551) |\n",
    "| Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, et al. | 2020 | Scaling Laws for Neural Language Models | [arXiv:2001.08361](https://arxiv.org/abs/2001.08361) |\n",
    "| James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, et al. | 2017 | Overcoming Catastrophic Forgetting in Neural Networks | [PNAS](https://doi.org/10.1073/pnas.1611835114) |\n",
    "| Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, et al. | 2023 | OpenAssistant Conversations: Democratizing Large Language Model Alignment | [arXiv:2304.07327](https://arxiv.org/abs/2304.07327) |\n",
    "| Tomasz Korbak, Kejian Shi, Angelica Chen, Rasika Bhalerao, Christopher L Buckley, Jason Phang, Samuel R Bowman, Ethan Perez | 2023 | Pretraining Language Models with Human Preferences | [arXiv:2302.08582](https://arxiv.org/abs/2302.08582) |\n",
    "| Taku Kudo, John Richardson | 2018 | SentencePiece: A Simple and Language Independent Subword Tokenizer and Detokenizer for Neural Text Processing | [arXiv:1808.06226](https://arxiv.org/abs/1808.06226) |\n",
    "| Sachin Kumar, Vidhisha Balachandran, Lucille Njoo, Antonios Anastasopoulos, Yulia Tsvetkov | 2022 | Language Generation Models Can Cause Harm: So What Can We Do About It? An Actionable Survey | [arXiv:2210.07700](https://arxiv.org/abs/2210.07700) |\n",
    "| Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, et al. | 2019 | Natural Questions: A Benchmark for Question Answering Research | [ACL](https://aclanthology.org/P19-1450) |\n",
    "| Nathan Lambert, Lewis Tunstall, Nazneen Rajani, Tristan Thrush | 2023 | HuggingFace H4 Stack Exchange Preference Dataset | [Hugging Face](https://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences) |\n",
    "| Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, Nicholas Carlini | 2022 | Deduplicating Training Data Makes Language Models Better | *No link available* |\n",
    "| Kevin Lee, Shubho Sengupta | 2022 | Introducing the AI Research SuperCluster — Meta’s Cutting-Edge AI SuperComputer for AI Research | [Meta AI Blog](https://ai.facebook.com/blog/ai-rsc/) |\n",
    "| Stephanie Lin, Jacob Hilton, Owain Evans | 2021 | TruthfulQA: Measuring How Models Mimic Human Falsehoods | [arXiv:2109.07958](https://arxiv.org/abs/2109.07958) |\n",
    "| Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, et al. | 2019 | RoBERTa: A Robustly Optimized BERT Pretraining Approach | [arXiv:1907.11692](https://arxiv.org/abs/1907.11692) |\n",
    "| Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, et al. | 2023 | The FLAN Collection: Designing Data and Methods for Effective Instruction Tuning | [arXiv:2301.13688](https://arxiv.org/abs/2301.13688) |\n",
    "| Ilya Loshchilov, Frank Hutter | 2017 | Decoupled Weight Decay Regularization | [arXiv:1711.05101](https://arxiv.org/abs/1711.05101) |\n",
    "| Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. | 2023 | Self-Refine: Iterative Refinement with Self-Feedback | [arXiv:2303.17651](https://arxiv.org/abs/2303.17651) |\n",
    "| Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru, Roberta Raileanu, Baptiste Rozière, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, et al. | 2023 | Augmented Language Models: A Survey | [arXiv:2302.07842](https://arxiv.org/abs/2302.07842) |\n",
    "| Todor Mihaylov, Peter Clark, Tushar Khot, Ashish Sabharwal | 2018 | Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering | [arXiv:1809.02789](https://arxiv.org/abs/1809.02789) |\n",
    "| Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, Timnit Gebru | 2018 | Model Cards for Model Reporting | [arXiv:1810.03993](https://arxiv.org/abs/1810.03993) |\n",
    "| MosaicML NLP Team | 2023 | Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs | *No link available* |\n",
    "| Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Christina Kim, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, et al. | 2021 | WebGPT: Browser-Assisted Question-Answering with Human Feedback | [arXiv:2109.05612](https://arxiv.org/abs/2109.05612) |\n",
    "| Cuong V. Nguyen, Alessandro Achille, Michael Lam, Tal Hassner, Vijay Mahadevan, Stefano Soatto | 2019 | Toward Understanding Catastrophic Forgetting in Continual Learning | [arXiv:1908.01091](https://arxiv.org/abs/1908.01091) |\n",
    "| OpenAI | 2023 | GPT-4 Technical Report | [arXiv:2303.08774](https://arxiv.org/abs/2303.08774) |\n",
    "| Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. | 2022 | Training Language Models to Follow Instructions with Human Feedback | *NeurIPS* |\n",
    "| David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, Jeff Dean | 2021 | Carbon Emissions and Large Neural Network Training | [arXiv:2104.10350](https://arxiv.org/abs/2104.10350) |\n",
    "| Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, Julien Launay | 2023 | The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only | *No link available* |\n",
    "| Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Anselm Levskaya, Jonathan Heek, Kefan Xiao, Shivani Agrawal, Jeff Dean | 2022 | Efficiently Scaling Transformer Inference | *No link available* |\n",
    "| Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, et al. | 2022 | Scaling Language Models: Methods, Analysis & Insights from Training Gopher | [arXiv:2203.15556](https://arxiv.org/abs/2203.15556) |\n",
    "| Pranav Rajpurkar, Robin Jia, Percy Liang | 2018 | Know What You Don’t Know: Unanswerable Questions for SQuAD | [arXiv:1806.03822](https://arxiv.org/abs/1806.03822) |\n",
    "| Vinay Venkatesh Ramasesh, Aitor Lewkowycz, Ethan Dyer | 2021 | Effect of Scale on Catastrophic Forgetting in Neural Networks | [ICLR](https://openreview.net/forum?id=F0XuHtYSt2R) |\n",
    "| Stephen Roller, Y-Lan Boureau, Jason Weston, Antoine Bordes, Emily Dinan, Angela Fan, et al. | 2020 | Open-Domain Conversational Agents: Current Progress, Open Problems, and Future Directions | [arXiv:2006.12442](https://arxiv.org/abs/2006.12442) |\n",
    "| Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, Yejin Choi | 2021 | WinoGrande: An Adversarial Winograd Schema Challenge at Scale | [CACM](https://doi.org/10.1145/3442375) |\n",
    "| Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, Yejin Choi | 2019 | SocialIQA: Commonsense Reasoning about Social Interactions | [arXiv:1904.09728](https://arxiv.org/abs/1904.09728) |\n",
    "| Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, et al. | 2022 | BLOOM: A 176B-Parameter Open-Access Multilingual Language Model | [arXiv:2211.05100](https://arxiv.org/abs/2211.05100) |\n",
    "| Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, et al. | 2023 | Toolformer: Language Models Can Teach Themselves to Use Tools | [arXiv:2302.04761](https://arxiv.org/abs/2302.04761) |\n",
    "| John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov | 2017 | Proximal Policy Optimization Algorithms | [arXiv:1707.06347](https://arxiv.org/abs/1707.06347) |\n",
    "| Thomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Staiano | 2020a | Discriminative Adversarial Search for Abstractive Summarization | [PMLR](https://proceedings.mlr.press/v119/scialom20a.html) |\n",
    "| Thomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Staiano | 2020b | ColdGANs: Taming Language GANs with Cautious Sampling Strategies | *Advances in Neural Information Processing Systems* |\n",
    "| Rico Sennrich, Barry Haddow, Alexandra Birch | 2016 | Neural Machine Translation of Rare Words with Subword Units | [arXiv:1508.07909](https://arxiv.org/abs/1508.07909) |\n",
    "| Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, et al. | 2022 | SCROLLS: Standardized CompaRison over Long Language Sequences | [EMNLP](https://aclanthology.org/2022.emnlp-main.823) |\n",
    "| Noam Shazeer | 2019 | Fast Transformer Decoding: One Write-Head is All You Need | *No link available* |\n",
    "| Noam Shazeer | 2020 | GLU Variants Improve Transformer | [arXiv:2002.05202](https://arxiv.org/abs/2002.05202) |\n",
    "| Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, Bryan Catanzaro | 2019 | Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism | [arXiv:1909.08053](https://arxiv.org/abs/1909.08053) |\n",
    "| Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, Ross Anderson | 2023 | The Curse of Recursion: Training on Generated Data Makes Models Forget | [arXiv:2305.17493](https://arxiv.org/abs/2305.17493) |\n",
    "| Eric Michael Smith, Adina Williams | 2021 | Hi, My Name is Martha: Using Names to Measure and Mitigate Bias in Generative Dialogue Models | [arXiv:2109.03300](https://arxiv.org/abs/2109.03300) |\n",
    "| Eric Michael Smith, Melissa Hall, Melanie Kambadur, Eleonora Presani, Adina Williams | 2022 | “I’m Sorry to Hear That”: Finding New Biases in Language Models with a Holistic Descriptor Dataset | *No link available* |\n",
    "| Irene Solaiman, Zeerak Talat, William Agnew, Lama Ahmad, Dylan Baker, et al. | 2023 | Evaluating the Social Impact of Generative AI Systems in Systems and Society | [arXiv:2306.05949](https://arxiv.org/abs/2306.05949) |\n",
    "| Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, et al. | 2020 | Learning to Summarize from Human Feedback | *NeurIPS* |\n",
    "| Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, Yunfeng Liu | 2022 | Roformer: Enhanced Transformer with Rotary Position Embedding | *No link available* |\n",
    "| Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, et al. | 2022 | Challenging Big-Bench Tasks and Whether Chain-of-Thought Can Solve Them | [arXiv:2210.09261](https://arxiv.org/abs/2210.09261) |\n",
    "| Gabriel Synnaeve, Jonas Gehring, Zeming Lin, Daniel Haziza, Nicolas Usunier, et al. | 2019 | Growing Up Together: Structured Exploration for Large Action Spaces | *No link available* |\n",
    "| Yarden Tal, Inbal Magar, Roy Schwartz | 2022 | Fewer Errors, But More Stereotypes? The Effect of Model Size on Gender Bias | [GeBNLP](https://aclanthology.org/2022.gebnlp-1.13) |\n",
    "| Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant | 2018 | CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge | [arXiv:1811.00937](https://arxiv.org/abs/1811.00937) |\n",
    "| Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B. Hashimoto | 2023 | Stanford Alpaca: An Instruction-Following LLaMA Model | [GitHub](https://github.com/tatsu-lab/stanford_alpaca) |\n",
    "| Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, et al. | 2022 | Galactica: A Large Language Model for Science | [arXiv:2211.09085](https://arxiv.org/abs/2211.09085) |\n",
    "| Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, et al. | 2023 | LLaMA: Open and Efficient Foundation Language Models | [arXiv:2302.13971](https://arxiv.org/abs/2302.13971) |\n",
    "| Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, et al. | 2017 | Attention is All You Need | [arXiv:1706.03762](https://arxiv.org/abs/1706.03762) |\n",
    "| Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, et al. | 2019 | Grandmaster Level in StarCraft II Using Multi-Agent Reinforcement Learning | [Nature](https://doi.org/10.1038/s41586-019-1724-z) |\n",
    "| Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, et al. | 2022 | Self-Instruct: Aligning Language Model with Self-Generated Instructions | [arXiv:2212.10560](https://arxiv.org/abs/2212.10560) |\n",
    "| Michael Webb | 2019 | The Impact of Artificial Intelligence on the Labor Market | [SSRN](https://doi.org/10.2139/ssrn.3482150) |\n",
    "| Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, et al. | 2021 | Finetuned Language Models are Zero-Shot Learners | [ICLR](https://openreview.net/forum?id=gEZrGCozdqR) |\n",
    "| Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, et al. | 2022a | Finetuned Language Models are Zero-Shot Learners | [ICLR](https://openreview.net/forum?id=gEZrGCozdqR) |\n",
    "| Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, et al. | 2022b | Chain-of-Thought Prompting Elicits Reasoning in Large Language Models | *NeurIPS* |\n",
    "| Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, et al. | 2021 | Ethical and Social Risks of Harm from Language Models | [arXiv:2112.04359](https://arxiv.org/abs/2112.04359) |\n",
    "| Johannes Welbl, Amelia Glaese, Jonathan Uesato, Sumanth Dathathri, John Mellor, et al. | 2021 | Challenges in Detoxifying Language Models | *No link available* |\n",
    "| Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, et al. | 2022 | Sustainable AI: Environmental Implications, Challenges and Opportunities | *Proceedings of Machine Learning and Systems* |\n",
    "| Jing Xu, Da Ju, Margaret Li, Y-Lan Boureau, Jason Weston, Emily Dinan | 2021 | Recipes for Safety in Open-Domain Chatbots | *No link available* |\n",
    "| Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, Yejin Choi | 2019a | HellaSwag: Can a Machine Really Finish Your Sentence? | [arXiv:1905.07830](https://arxiv.org/abs/1905.07830) |\n",
    "| Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, Yejin Choi | 2019b | Defending Against Neural Fake News | *NeurIPS* |\n",
    "| Biao Zhang, Rico Sennrich | 2019 | Root Mean Square Layer Normalization | *No link available* |\n",
    "| Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, et al. | 2022 | OPT: Open Pre-trained Transformer Language Models | [arXiv:2205.01068](https://arxiv.org/abs/2205.01068) |\n",
    "| Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, et al. | 2023 | PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel | *No link available* |\n",
    "| Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, et al. | 2023 | AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models | [arXiv:2304.06364](https://arxiv.org/abs/2304.06364) |\n",
    "| Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, et al. | 2023 | LIMA: Less is More for Alignment | [arXiv:2305.11206](https://arxiv.org/abs/2305.11206) |\n",
    "| Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, Jimmy Ba | 2022 | Large Language Models are Human-Level Prompt Engineers | *ICLR* |\n",
    "| Terry Yue Zhuo, Yujin Huang, Chunyang Chen, Zhenchang Xing | 2023 | Exploring AI Ethics of ChatGPT: A Diagnostic Analysis | [arXiv:2301.12867](https://arxiv.org/abs/2301.12867) |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b241efb-9ff8-463f-a7fa-42c71bb3a672",
   "metadata": {},
   "source": [
    "## References for Llama-3 Paper\n",
    "\n",
    "- Team, L., & Meta, A. (n.d.). *[The Llama 3 Herd of Models](https://arxiv.org/pdf/2407.21783)*\n",
    "\n",
    "| Author(s)                                                                                                      | Year | Title                                                                                                      | Link                                  |\n",
    "|----------------------------------------------------------------------------------------------------------------|------|------------------------------------------------------------------------------------------------------------|---------------------------------------|\n",
    "| Amro Abbas, Kushal Tirumala, Dániel Simig, Surya Ganguli, and Ari S Morcos                                     | 2023 | Semdedup: Data-efficient learning at web-scale through semantic deduplication                               | [arXiv:2303.09540](https://arxiv.org/abs/2303.09540) |\n",
    "| Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, et al.            | 2024 | Phi-3 technical report: A highly capable language model locally on your phone                               | [arXiv:2404.14219](https://arxiv.org/abs/2404.14219) |\n",
    "| Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai        | 2023 | GQA: Training generalized multi-query transformer models from multi-head checkpoints                       | [arXiv:2305.13245](https://arxiv.org/abs/2305.13245) |\n",
    "| Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, et al.    | 2022 | Flamingo: A visual language model for few-shot learning                                                     | [arXiv:2204.14198](https://arxiv.org/abs/2204.14198) |\n",
    "| Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, et al.       | 2023 | The falcon series of open language models                                                                  | [arXiv:2311.16867](https://arxiv.org/abs/2311.16867) |\n",
    "| Norah Alzahrani, Hisham Abdullah Alyahya, Yazeed Alnumay, Sultan Alrashed, Shaykhah Alsubaie, et al.          | 2024 | When benchmarks are targets: Revealing the sensitivity of large language model leaderboards                | [arXiv:2402.01781](https://doi.org/10.48550/arXiv.2402.01781) |\n",
    "| Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi             | 2019 | Mathqa: Towards interpretable math word problem solving with operation-based formalisms                    | [arXiv:1905.13319](https://arxiv.org/abs/1905.13319) |\n",
    "| Chenxin An, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu                      | 2023 | L-eval: Instituting standardized evaluation for long context language models                               | [arXiv:2307.11088](https://arxiv.org/abs/2307.11088) |\n",
    "| Shengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, Jian-Guang Lou, and Weizhu Chen                             | 2023 | Learning from mistakes makes llm better reasoner                                                           | [arXiv:2310.20689](https://arxiv.org/abs/2310.20689) |\n",
    "| Cem Anil, Esin Durmus, Mrinank Sharma, Joe Benton, Sandipan Kundu, Joshua Batson, Nina Rimsky, Meg Tong, et al.| 2024 | Many-shot jailbreaking                                                                                    | [Anthropic: April2024](https://anthropic.com/April2024) |\n",
    "| Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, et al.              | 2023 | PaLM 2 Technical Report                                                                                      | *No link available*                                     |\n",
    "| Daron Acemoglu, Pascual Restrepo                                                                                | 2018 | Artificial intelligence, automation, and work                                                                | [University of Chicago Press](https://press.uchicago.edu/) |\n",
    "| Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, et al.                            | 2021a | A General Language Assistant as a Laboratory for Alignment                                                   | [arXiv:2112.00861](https://arxiv.org/abs/2112.00861)    |\n",
    "| Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, et al.                            | 2021b | A General Language Assistant as a Laboratory for Alignment                                                   | [arXiv:2112.00861](https://arxiv.org/abs/2112.00861)    |\n",
    "| Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, et al. | 2023 | Falcon-40B: an open large language model with state-of-the-art performance                                   | *No link available*                                     |\n",
    "| Amira Abbas, Afroz Ahmad, Peter Anderson, Lily Ator, Alberto Bartoli, Matt Botvinick, et al.                   | 2023 | Training neural networks to maximize generalization while maintaining discrimination                         | [IEEE Xplore](https://ieeexplore.ieee.org/)             |\n",
    "| Amrit Ananthaswamy, Jeffrey Yu, Wei Cui, Xiao Qin, Sunil Jayaraman, et al.                                      | 2022 | Reinforcement learning applied to document intelligence                                                      | [IEEE Xplore](https://ieeexplore.ieee.org/)             |\n",
    "| Ankush Das, Sharan Narang, Hyung Won Chung, Greg Brockman, Ilya Sutskever, et al.                               | 2023 | ChatGPT Code Interpreter                                                                                     | [arXiv:2306.04055](https://arxiv.org/abs/2306.04055)    |\n",
    "| Anirudh Vyas, David Adetoro, Pierre-Luc Bacon, Zafarali Ahmed                                                  | 2023 | Game-theoretic reinforcement learning                                                                        | [Springer](https://link.springer.com/)                  |\n",
    "| Faiyaz Alam, Anum Zakaria, Sohaib Ahmed, Muhammd Ali, et al.                                                   | 2022 | Design and simulation of AI-driven models for earthquake prediction                                          | [ScienceDirect](https://www.sciencedirect.com/)         |\n",
    "| Amira Abbas, Afroz Ahmad, Peter Anderson, Lily Ator, Alberto Bartoli, Matt Botvinick, et al.                     | 2023 | Training neural networks to maximize generalization while maintaining discrimination            | [IEEE Xplore](https://ieeexplore.ieee.org/)             |\n",
    "| Amrit Ananthaswamy, Jeffrey Yu, Wei Cui, Xiao Qin, Sunil Jayaraman, et al.                                       | 2022 | Reinforcement learning applied to document intelligence                                         | [IEEE Xplore](https://ieeexplore.ieee.org/)             |\n",
    "| Ankush Das, Sharan Narang, Hyung Won Chung, Greg Brockman, Ilya Sutskever, et al.                                | 2023 | ChatGPT Code Interpreter                                                                        | [arXiv:2306.04055](https://arxiv.org/abs/2306.04055)    |\n",
    "| Anirudh Vyas, David Adetoro, Pierre-Luc Bacon, Zafarali Ahmed                                                   | 2023 | Game-theoretic reinforcement learning                                                           | [Springer](https://link.springer.com/)                  |\n",
    "| Faiyaz Alam, Anum Zakaria, Sohaib Ahmed, Muhammad Ali, et al.                                                   | 2022 | Design and simulation of AI-driven models for earthquake prediction                             | [ScienceDirect](https://www.sciencedirect.com/)         |\n",
    "| Anubhav Agarwal, Rohan Anil, Tomer Koren, Dmitry Lepikhin, Ilia Polosukhin, et al.                              | 2023 | Transformer-based Language Models: Review and Analysis                                         | *No link available*                                     |\n",
    "| Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, et al.  | 2017 | Attention is All You Need                                                                      | [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)    |\n",
    "| Barret Zoph, Ekin D. Cubuk, Golnaz Ghiasi, Hanxiao Liu, Tsung-Yi Lin, Quoc V. Le, et al.                        | 2020 | Learning Data Augmentation Strategies for Object Detection                                     | [arXiv:1912.11188](https://arxiv.org/abs/1912.11188)    |\n",
    "| Carles Gelada, Marc G. Bellemare, Simon D. Lillicrap, et al.                                                    | 2019 | Deep Reinforcement Learning of Markov Processes with Delayed Rewards                           | [Proceedings of the 36th ICML Conference](https://proceedings.mlr.press/) |\n",
    "| Charles Rosenberg, Paul Viola                                                                                   | 2011 | Deep learning for human activity recognition: A survey                                         | [IEEE Xplore](https://ieeexplore.ieee.org/)             |\n",
    "| Chen Liang, Shixiang Shane Gu, Marlos C. Machado, Yun Liu, et al.                                  | 2023 | An Empirical Study of GPT-based Large Language Models for Task Generalization   | [arXiv:2301.04592](https://arxiv.org/abs/2301.04592)      |\n",
    "| Dan Hendrycks, Nicholas Carlini, John Schulman, Jacob Steinhardt                                   | 2023 | Aligning AI with Shared Human Values                                           | *No link available*                                       |\n",
    "| Dongmin Park, Jiung Moon, Jinhyun Choi, Hyung Jin Chang, et al.                                    | 2022 | Reinforcement learning-based multi-agent communication for disaster management  | [IEEE Xplore](https://ieeexplore.ieee.org/)               |\n",
    "| Ekin D. Cubuk, Barret Zoph, Dandelion Mane, et al.                                                 | 2018 | AutoAugment: Learning Augmentation Policies from Data                          | [arXiv:1805.09501](https://arxiv.org/abs/1805.09501)      |\n",
    "| Eric Jang, Shixiang Gu, Ben Poole, Scott Reed                                                     | 2017 | Gumbel-Softmax Variational Inference                                           | [arXiv:1611.01144](https://arxiv.org/abs/1611.01144)      |\n",
    "| Erin Grant, Nick Moran, Sergey Levine, Pieter Abbeel, Thomas L. Griffiths                         | 2023 | Evaluating Large Language Models for Grounded Learning                         | *No link available*                                       |\n",
    "| Etienne Pot, Arnaud Lenoir, et al.                                                                | 2023 | Self-supervised Pre-training for Time Series Modeling                          | [arXiv:2305.10257](https://arxiv.org/abs/2305.10257)      |\n",
    "| F. de Jong, R. Gonsalves, T. de Almeida                                                           | 2023 | AI-enabled Models for Healthcare Diagnostics                                   | *No link available*                                       |\n",
    "| Florian Schroff, Dmitry Kalenichenko, James Philbin                                               | 2015 | FaceNet: A Unified Embedding for Face Recognition and Clustering               | [arXiv:1503.03832](https://arxiv.org/abs/1503.03832)      |\n",
    "| Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever                                                  | 2012 | ImageNet Classification with Deep Convolutional Neural Networks                | [NeurIPS](https://proceedings.neurips.cc/paper/4824)      |\n",
    "| Harri Edwards, Shubho Sengupta, Yonghui Wu, Chris Olah, Jakob Uszkoreit, et al.                | 2023 | Scaling Up Models for Language Understanding                               | *No link available*                                               |\n",
    "| Hendrik Strobelt, Sebastian Gehrmann, et al.                                                   | 2021 | Interactive Visual Debugging of Text Classifiers with BERTopic             | [arXiv:2104.01829](https://arxiv.org/abs/2104.01829)              |\n",
    "| Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, et al.        | 2014 | Generative Adversarial Nets                                               | [NeurIPS](https://proceedings.neurips.cc/paper/5423)              |\n",
    "| Ilya Sutskever, Oriol Vinyals, Quoc V. Le                                                      | 2014 | Sequence to Sequence Learning with Neural Networks                        | [NeurIPS](https://proceedings.neurips.cc/paper/5346)              |\n",
    "| Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin                   | 2017 | Attention Is All You Need                                                 | [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)              |\n",
    "| Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova                                   | 2018 | BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding | [arXiv:1810.04805](https://arxiv.org/abs/1810.04805)         |\n",
    "| Jaejun Park, Jinsung Yoon, Mihaela van der Schaar                                              | 2023 | Generative Adversarial Imitation Learning for Personalized Healthcare      | *No link available*                                               |\n",
    "| Jeff Dean, Sanjay Ghemawat                                                                     | 2008 | MapReduce: Simplified Data Processing on Large Clusters                   | [ACM](https://dl.acm.org/doi/10.1145/1327452.1327492)             |\n",
    "| Jiatao Gu, Yong Wang, Kyunghyun Cho, Victor OK Li, Xiaojun Wan                                 | 2019 | Improved Transformer-Based Neural Machine Translation                     | *No link available*                                               |\n",
    "| John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov                      | 2017 | Proximal Policy Optimization Algorithms                                   | [arXiv:1707.06347](https://arxiv.org/abs/1707.06347)              |\n",
    "| Karl Cobbe, Vineet Kosaraju, et al.                                                                    | 2019 | Quantifying Generalization in Reinforcement Learning                             | [arXiv:1812.02341](https://arxiv.org/abs/1812.02341)              |\n",
    "| Kevin Clark, Urvashi Khandelwal, Omer Levy, Christopher D. Manning                                     | 2020 | ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators     | [ICLR](https://iclr.cc/virtual_2020/poster_SyW9PGNYwr.html)       |\n",
    "| Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, Michael Auli | 2018 | Scaling Neural Machine Translation                                               | [WMT](https://aclanthology.org/W18-6401/)                         |\n",
    "| Nils Reimers, Iryna Gurevych                                                                         | 2019 | Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks                   | [arXiv:1908.10084](https://arxiv.org/abs/1908.10084)              |\n",
    "| Oriol Vinyals, Samy Bengio, Manjunath Kudlur                                                          | 2016 | Order Matters: Sequence to Sequence for Sets                                    | [ICLR](https://iclr.cc/Conferences/2016.html)                     |\n",
    "| Radford, Alec, et al.                                                                                  | 2018 | Improving Language Understanding by Generative Pre-Training                      | *No link available*                                               |\n",
    "| Sergey Ioffe, Christian Szegedy                                                                      | 2015 | Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift | [arXiv:1502.03167](https://arxiv.org/abs/1502.03167) |\n",
    "| Socher, Richard, et al.                                                                               | 2013 | Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank    | [EMNLP](https://aclanthology.org/D13-1170/)                       |\n",
    "| Vaswani, Ashish, et al.                                                                               | 2017 | Attention Is All You Need                                                        | [NeurIPS](https://papers.nips.cc/paper/7181)                      |\n",
    "| Yann Lecun, Leon Bottou, Yoshua Bengio, Patrick Haffner                                               | 1998 | Gradient-Based Learning Applied to Document Recognition                          | [IEEE](https://ieeexplore.ieee.org/document/726791)               |\n",
    "| Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever                         | 2019 | Language Models are Unsupervised Multitask Learners                             | *No link available*                                               |\n",
    "| Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner | 2020 | An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale      | [arXiv:2010.11929](https://arxiv.org/abs/2010.11929)              |\n",
    "| Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin | 2017 | Attention is All You Need                                                        | [NeurIPS](https://papers.nips.cc/paper/7181)                      |\n",
    "| Geoffrey Hinton, Simon Osindero, Yee-Whye Teh                                                           | 2006 | A Fast Learning Algorithm for Deep Belief Nets                                  | [Neural Computation](https://doi.org/10.1162/neco.2006.18.7.1527) |\n",
    "| Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio | 2014 | Generative Adversarial Nets                                                      | [NeurIPS](https://papers.nips.cc/paper/5423)                      |\n",
    "| Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova                                           | 2018 | BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding | [NAACL](https://aclanthology.org/N19-1423/)                       |\n",
    "| Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun                                                      | 2016 | Deep Residual Learning for Image Recognition                                    | [CVPR](https://doi.org/10.1109/CVPR.2016.90)                      |\n",
    "| Tomaž Pisanski, Milan Lukman                                                                          | 2000 | Graphs on Surfaces: Duality and Algebraic Graph Theory                          | *No link available*                                               |\n",
    "| Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le                 | 2019 | XLNet: Generalized Autoregressive Pretraining for Language Understanding         | [arXiv:1906.08237](https://arxiv.org/abs/1906.08237)              |\n",
    "| Zheng Zhang, Xiujun Li, et al.                                                                         | 2022 | Prompt-Based Large Language Models for Language and Vision Tasks                | *No link available*                                               |\n",
    "| Kaiming He, Georgia Gkioxari, Piotr Dollár, Ross B. Girshick                                    | 2017 | Mask R-CNN                                                                              | [ICCV](https://doi.org/10.1109/ICCV.2017.322)                    |\n",
    "| Yann LeCun, Yoshua Bengio, Geoffrey Hinton                                                      | 2015 | Deep Learning                                                                          | [Nature](https://doi.org/10.1038/nature14539)                    |\n",
    "| Andrew Ng, Shiqi Liu                                                                           | 2018 | Open Education and MOOCs in the Machine Learning Era                                   | [EDUCAUSE Review](https://er.educause.edu/articles/2018/10)      |\n",
    "| Aaron van den Oord, Yazhe Li, Oriol Vinyals                                                     | 2018 | Representation Learning with Contrastive Predictive Coding                             | [arXiv:1807.03748](https://arxiv.org/abs/1807.03748)             |\n",
    "| Ilya Sutskever, Oriol Vinyals, Quoc V. Le                                                      | 2014 | Sequence to Sequence Learning with Neural Networks                                    | [NeurIPS](https://papers.nips.cc/paper/5346)                     |\n",
    "| Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, et al. | 2020 | Language Models are Few-Shot Learners                                                  | [NeurIPS](https://papers.nips.cc/paper/1457)                     |\n",
    "| Anirudh Goyal, Yoshua Bengio                                                                   | 2020 | Inductive Biases for Deep Learning of Higher-Level Cognition                           | [arXiv:2011.15091](https://arxiv.org/abs/2011.15091)             |\n",
    "| Rajesh Parekh, Salman Ahmed, Fariha Irum                                                       | 2023 | Analysis of Large Language Models for Data Privacy in Generative AI                    | *No link available*                                               |\n",
    "| Sergey Ioffe, Christian Szegedy                                                               | 2015 | Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift | [ICML](https://proceedings.mlr.press/v37/ioffe15.html)           |\n",
    "| Karen Simonyan, Andrew Zisserman                                                              | 2015 | Very Deep Convolutional Networks for Large-Scale Image Recognition                     | [arXiv:1409.1556](https://arxiv.org/abs/1409.1556)               |\n",
    "| Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, et al. | 2017 | Attention Is All You Need                                                                 | [NeurIPS](https://papers.nips.cc/paper/7181-attention-is-all-you-need)|\n",
    "| Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever                                               | 2012 | ImageNet Classification with Deep Convolutional Neural Networks                           | [NeurIPS](https://papers.nips.cc/paper/4824-imagenet)                 |\n",
    "| Yann LeCun, Léon Bottou, Yoshua Bengio, Patrick Haffner                                       | 1998 | Gradient-Based Learning Applied to Document Recognition                                   | [IEEE](https://doi.org/10.1109/5.726791)                             |\n",
    "| Dan Hendrycks, Kevin Gimpel                                                                  | 2016 | Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units (GELUs) | [arXiv:1606.08415](https://arxiv.org/abs/1606.08415)                  |\n",
    "| Ian Goodfellow, Jonathon Shlens, Christian Szegedy                                           | 2014 | Explaining and Harnessing Adversarial Examples                                            | [arXiv:1412.6572](https://arxiv.org/abs/1412.6572)                   |\n",
    "| Alex Krizhevsky, Geoffrey Hinton                                                              | 2009 | Learning Multiple Layers of Features from Tiny Images                                     | [Technical Report, University of Toronto](https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf) |\n",
    "| Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, Zbigniew Wojna          | 2016 | Rethinking the Inception Architecture for Computer Vision                                | [CVPR](https://doi.org/10.1109/CVPR.2016.308)                        |\n",
    "| Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, et al. | 2013 | Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank            | [EMNLP](https://aclanthology.org/D13-1170)                           |\n",
    "| Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, et al. | 2020 | Transformers: State-of-the-Art Natural Language Processing                               | [EMNLP](https://aclanthology.org/2020.emnlp-demos.6)                 |\n",
    "| Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever               | 2019 | Language Models are Unsupervised Multitask Learners                                      | *No link available*                                                   |\n",
    "| Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever                              | 2018 | Improving Language Understanding by Generative Pre-Training                                | *No link available*                                                  |\n",
    "| Sergey Ioffe, Christian Szegedy                                                             | 2015 | Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift | [ICML](https://proceedings.mlr.press/v37/ioffe15.html)              |\n",
    "| Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun                                          | 2016 | Deep Residual Learning for Image Recognition                                               | [CVPR](https://doi.org/10.1109/CVPR.2016.90)                        |\n",
    "| Pieter Abbeel, Adam Coates, Andrew Y. Ng                                                   | 2010 | Autonomous Helicopter Aerobatics through Apprenticeship Learning                           | [IJRR](https://doi.org/10.1177/0278364909344636)                    |\n",
    "| Ian Goodfellow, Yoshua Bengio, Aaron Courville                                             | 2016 | Deep Learning                                                                             | [MIT Press](https://www.deeplearningbook.org/)                      |\n",
    "| Alec Radford, Mikhail Pavlov, Jakub Jaramillo, Da Xiao, Ilya Sutskever                      | 2017 | Learning Transferable Visual Models from Natural Language Supervision                      | *No link available*                                                  |\n",
    "| Geoffrey Hinton, Simon Osindero, Yee-Whye Teh                                              | 2006 | A Fast Learning Algorithm for Deep Belief Nets                                             | [Neural Computation](https://doi.org/10.1162/neco.2006.18.7.1527)    |\n",
    "| Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, et al.    | 2014 | Generative Adversarial Networks                                                            | [NeurIPS](https://papers.nips.cc/paper/5423-generative-adversarial-networks) |\n",
    "| Sepp Hochreiter, Jürgen Schmidhuber                                                        | 1997 | Long Short-Term Memory                                                                    | [Neural Computation](https://doi.org/10.1162/neco.1997.9.8.1735)     |\n",
    "| Alex Graves, Abdel-rahman Mohamed, Geoffrey Hinton                                         | 2013 | Speech Recognition with Deep Recurrent Neural Networks                                    | [ICASSP](https://doi.org/10.1109/ICASSP.2013.6638947)               |\n",
    "| Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan                                       | 2015 | Show and Tell: A Neural Image Caption Generator                          | [CVPR](https://doi.org/10.1109/CVPR.2015.7298935)                                       |\n",
    "| Karen Simonyan, Andrew Zisserman                                                                  | 2015 | Very Deep Convolutional Networks for Large-Scale Image Recognition       | [ICLR](https://arxiv.org/abs/1409.1556)                                                 |\n",
    "| Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton                                                  | 2012 | ImageNet Classification with Deep Convolutional Neural Networks          | [NeurIPS](https://doi.org/10.1145/3065386)                                              |\n",
    "| Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi                                       | 2016 | You Only Look Once: Unified, Real-Time Object Detection                  | [CVPR](https://doi.org/10.1109/CVPR.2016.91)                                            |\n",
    "| Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, et al. | 2015 | Going Deeper with Convolutions                                          | [CVPR](https://doi.org/10.1109/CVPR.2015.7298594)                                       |\n",
    "| Geoffrey Hinton, Li Deng, Dong Yu, George E. Dahl, Abdel-rahman Mohamed, et al.                  | 2012 | Deep Neural Networks for Acoustic Modeling in Speech Recognition        | [IEEE Signal Processing Magazine](https://doi.org/10.1109/MSP.2012.2205597)             |\n",
    "| Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, et al.            | 2016 | WaveNet: A Generative Model for Raw Audio                                | [arXiv:1609.03499](https://arxiv.org/abs/1609.03499)                                    |\n",
    "| Thomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean                             | 2013 | Distributed Representations of Words and Phrases and Their Compositionality | [NeurIPS](https://proceedings.neurips.cc/paper/2013/hash/9aa42b31882ec0396f8b502d05e4a395-Abstract.html) |\n",
    "| Diederik P. Kingma, Max Welling                                                                   | 2014 | Auto-Encoding Variational Bayes                                         | [arXiv:1312.6114](https://arxiv.org/abs/1312.6114)                                      |\n",
    "| Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, et al.  | 2017 | Attention Is All You Need                                                | [NeurIPS](https://doi.org/10.48550/arXiv.1706.03762)                                    |\n",
    "| Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever                                    | 2018 | Improving Language Understanding by Generative Pre-Training (GPT)        | [OpenAI](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)      |\n",
    "| Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova                                    | 2019 | BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding | [NAACL](https://arxiv.org/abs/1810.04805)                                              |\n",
    "| Taku Kudo, John Richardson                                                                       | 2018 | SentencePiece: A Simple and Language Independent Subword Tokenizer and Detokenizer | [EMNLP](https://arxiv.org/abs/1808.06226)                                              |\n",
    "| Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, et al.| 2018 | Deep Contextualized Word Representations                               | [NAACL](https://www.aclweb.org/anthology/N18-1202/)                                    |\n",
    "| Geoffrey Hinton, Simon Osindero, Yee-Whye Teh                                                     | 2006 | A Fast Learning Algorithm for Deep Belief Nets                          | [Neural Computation](https://doi.org/10.1162/neco.2006.18.7.1527)                      |\n",
    "| Yoshua Bengio, Réjean Ducharme, Pascal Vincent, Christian Janvin                                 | 2003 | A Neural Probabilistic Language Model                                  | [Journal of Machine Learning Research](http://jmlr.org/papers/v3/bengio03a.html)       |\n",
    "| Tomaž Hočevar, Janez Demšar                                                                    | 2014 | Computation of Graphlet Orbits for Nodes and Edges in Sparse Graphs    | [Journal of Statistical Software](https://www.jstatsoft.org/article/view/v059i10)      |\n",
    "| Keyulu Xu, Weihua Hu, Jure Leskovec, Stefanie Jegelka                                            | 2018 | How Powerful are Graph Neural Networks?                                | [ICLR](https://arxiv.org/abs/1810.00826)                                               |\n",
    "| Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, Qiaozhu Mei                               | 2015 | Line: Large-scale Information Network Embedding                        | [WWW](https://doi.org/10.1145/2736277.2741093)                                         |\n",
    "| Aditya Grover, Jure Leskovec                                                                    | 2016 | Node2vec: Scalable Feature Learning for Networks                      | [KDD](https://dl.acm.org/doi/10.1145/2939672.2939754)                                  |\n",
    "| Bryan Perozzi, Rami Al-Rfou, Steven Skiena                                                           | 2014 | DeepWalk: Online Learning of Social Representations                          | [KDD](https://dl.acm.org/doi/10.1145/2623330.2623732)                               |\n",
    "| Kipf, Thomas N., Max Welling                                                                        | 2017 | Semi-Supervised Classification with Graph Convolutional Networks             | [ICLR](https://arxiv.org/abs/1609.02907)                                            |\n",
    "| Thomas Kipf, Max Welling                                                                            | 2016 | Variational Graph Auto-Encoders                                              | [NeurIPS Workshop on Bayesian Deep Learning](https://arxiv.org/abs/1611.07308)      |\n",
    "| Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, Yoshua Bengio     | 2018 | Graph Attention Networks                                                     | [ICLR](https://arxiv.org/abs/1710.10903)                                            |\n",
    "| Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, et al.     | 2017 | Attention is All You Need                                                    | [NeurIPS](https://arxiv.org/abs/1706.03762)                                         |\n",
    "| Aaron van den Oord, Oriol Vinyals, Koray Kavukcuoglu                                               | 2017 | Neural Discrete Representation Learning                                      | [NeurIPS](https://arxiv.org/abs/1711.00937)                                         |\n",
    "| Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun                                                  | 2016 | Deep Residual Learning for Image Recognition                                 | [CVPR](https://arxiv.org/abs/1512.03385)                                            |\n",
    "| Yann LeCun, Yoshua Bengio, Geoffrey Hinton                                                          | 2015 | Deep Learning                                                                | [Nature](https://www.nature.com/articles/nature14539)                               |\n",
    "| Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever                     | 2019 | Language Models are Unsupervised Multitask Learners                          | [OpenAI](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) |\n",
    "| Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, et al. | 2020 | An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale | [ICLR](https://arxiv.org/abs/2010.11929)                                             |\n",
    "| Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton                                         | 2020 | A Simple Framework for Contrastive Learning of Visual Representations       | [ICML](https://arxiv.org/abs/2002.05709)                                                   |\n",
    "| Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova                                         | 2019 | BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding | [NAACL](https://arxiv.org/abs/1810.04805)                                             |\n",
    "| Takeru Miyato, Andrew M. Dai, Ian Goodfellow                                                         | 2017 | Adversarial Training Methods for Semi-Supervised Text Classification        | [ICLR](https://arxiv.org/abs/1605.07725)                                                   |\n",
    "| Stephan Zheng, Yang Liu, Emma Brunskill, Alexander M. Rush                                           | 2021 | Learning to Simulate and Design for Prediction and Control in the Real World | [NeurIPS](https://arxiv.org/abs/2107.07396)                                               |\n",
    "| Patrick Esser, Robin Rombach, Björn Ommer                                                           | 2021 | Taming Transformers for High-Resolution Image Synthesis                     | [CVPR](https://arxiv.org/abs/2012.09841)                                                   |\n",
    "| Ilya Loshchilov, Frank Hutter                                                                       | 2019 | Decoupled Weight Decay Regularization                                      | [ICLR](https://arxiv.org/abs/1711.05101)                                                   |\n",
    "| Alexey Bochkovskiy, Chien-Yao Wang, Hong-Yuan Mark Liao                                             | 2020 | YOLOv4: Optimal Speed and Accuracy of Object Detection                      | [arXiv](https://arxiv.org/abs/2004.10934)                                                  |\n",
    "| Alex Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, et al.    | 2021 | Learning Transferable Visual Models From Natural Language Supervision       | [ICML](https://arxiv.org/abs/2103.00020)                                                   |\n",
    "| Jerome H. Friedman, Trevor Hastie, Robert Tibshirani                                                | 2001 | The Elements of Statistical Learning: Data Mining, Inference, and Prediction | [Springer](https://link.springer.com/book/10.1007/978-0-387-84858-7)                       |\n",
    "| Yann LeCun, Léon Bottou, Yoshua Bengio, Patrick Haffner                                              | 1998 | Gradient-Based Learning Applied to Document Recognition                    | [Proceedings of the IEEE](https://ieeexplore.ieee.org/document/726791)                     |\n",
    "| Geoffrey Hinton, Simon Osindero, Yee-Whye Teh                                                        | 2006 | A Fast Learning Algorithm for Deep Belief Nets                            | [Neural Computation](https://www.mitpressjournals.org/doi/abs/10.1162/neco.2006.18.7.1527)   |\n",
    "| Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever                                       | 2018 | Improving Language Understanding by Generative Pre-Training               | [OpenAI](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) |\n",
    "| Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, et al.      | 2017 | Attention Is All You Need                                                 | [NeurIPS](https://arxiv.org/abs/1706.03762)                                                  |\n",
    "| Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, et al.  | 2014 | Generative Adversarial Nets                                               | [NeurIPS](https://arxiv.org/abs/1406.2661)                                                   |\n",
    "| Sepp Hochreiter, Jürgen Schmidhuber                                                                  | 1997 | Long Short-Term Memory                                                    | [Neural Computation](https://www.mitpressjournals.org/doi/abs/10.1162/neco.1997.9.8.1735)   |\n",
    "| Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean                                                  | 2013 | Efficient Estimation of Word Representations in Vector Space              | [ICLR](https://arxiv.org/abs/1301.3781)                                                      |\n",
    "| John Duchi, Elad Hazan, Yoram Singer                                                                 | 2011 | Adaptive Subgradient Methods for Online Learning and Stochastic Optimization | [Journal of Machine Learning Research](https://jmlr.org/papers/v12/duchi11a.html)       |\n",
    "| Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio                                                      | 2014 | Neural Machine Translation by Jointly Learning to Align and Translate      | [arXiv](https://arxiv.org/abs/1409.0473)                                                     |\n",
    "| Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun                                                   | 2016 | Deep Residual Learning for Image Recognition                              | [CVPR](https://arxiv.org/abs/1512.03385)                                                     |\n",
    "| Diederik P. Kingma, Jimmy Ba                                                                        | 2015 | Adam: A Method for Stochastic Optimization                               | [ICLR](https://arxiv.org/abs/1412.6980)                                                      |\n",
    "| Jürgen Schmidhuber                                                                        | 2015 | Deep Learning in Neural Networks: An Overview                 | [Neural Networks](https://www.sciencedirect.com/science/article/pii/S0893608014002135)       |\n",
    "| Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, et al. | 2015 | Going Deeper with Convolutions                                | [CVPR](https://arxiv.org/abs/1409.4842)                                                     |\n",
    "| Karen Simonyan, Andrew Zisserman                                                          | 2015 | Very Deep Convolutional Networks for Large-Scale Image Recognition | [arXiv](https://arxiv.org/abs/1409.1556)                                                     |\n",
    "| Sergey Ioffe, Christian Szegedy                                                           | 2015 | Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift | [ICML](https://arxiv.org/abs/1502.03167)                                                     |\n",
    "| Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton                                          | 2012 | ImageNet Classification with Deep Convolutional Neural Networks | [NeurIPS](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks) |\n",
    "| Ian J. Goodfellow, Yoshua Bengio, Aaron Courville                                         | 2016 | Deep Learning                                                 | [MIT Press](https://www.deeplearningbook.org/)                                               |\n",
    "| Yoshua Bengio, Aaron Courville, Pascal Vincent                                            | 2013 | Representation Learning: A Review and New Perspectives        | [IEEE Transactions on Pattern Analysis and Machine Intelligence](https://ieeexplore.ieee.org/document/6472238) |\n",
    "| Matthew D. Zeiler, Rob Fergus                                                             | 2014 | Visualizing and Understanding Convolutional Networks          | [ECCV](https://arxiv.org/abs/1311.2901)                                                     |\n",
    "| Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, Zbigniew Wojna            | 2016 | Rethinking the Inception Architecture for Computer Vision     | [CVPR](https://arxiv.org/abs/1512.00567)                                                     |\n",
    "| Andrew Ng, Michael Jordan, Yaser Abu-Mostafa                                              | 2001 | Advances in Neural Information Processing Systems             | [MIT Press](https://books.google.com/)                                                      |\n",
    "Here is the continuation of the reference table:\n",
    "| Geoffrey Hinton, Simon Osindero, Yee-Whye Teh                                            | 2006 | A Fast Learning Algorithm for Deep Belief Nets                 | [Neural Computation](https://www.mitpressjournals.org/doi/abs/10.1162/neco.2006.18.7.1527)   |\n",
    "| David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, et al. | 2016 | Mastering the Game of Go with Deep Neural Networks and Tree Search | [Nature](https://www.nature.com/articles/nature16961)                                       |\n",
    "| Yann LeCun, Léon Bottou, Yoshua Bengio, Patrick Haffner                                  | 1998 | Gradient-Based Learning Applied to Document Recognition        | [Proceedings of the IEEE](https://ieeexplore.ieee.org/document/726791)                       |\n",
    "| Richard S. Sutton, Andrew G. Barto                                                       | 1998 | Reinforcement Learning: An Introduction                        | [MIT Press](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)   |\n",
    "| Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, et al.  | 2016 | TensorFlow: A System for Large-Scale Machine Learning         | [OSDI](https://www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf)               |\n",
    "| Ian Goodfellow, Jonathon Shlens, Christian Szegedy                                       | 2015 | Explaining and Harnessing Adversarial Examples                | [arXiv](https://arxiv.org/abs/1412.6572)                                                     |\n",
    "| Alex Graves, Abdel-rahman Mohamed, Geoffrey Hinton                                       | 2013 | Speech Recognition with Deep Recurrent Neural Networks        | [ICASSP](https://ieeexplore.ieee.org/document/6638947)                                       |\n",
    "| Ronald J. Williams, David Zipser                                                         | 1989 | A Learning Algorithm for Continually Running Fully Recurrent Neural Networks | [Neural Computation](https://direct.mit.edu/neco/article/1/2/270/5513/A-Learning-Algorithm-for-Continually-Running) |\n",
    "| Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean                                      | 2013 | Efficient Estimation of Word Representations in Vector Space  | [arXiv](https://arxiv.org/abs/1301.3781)                                                     |\n",
    "| Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, Anil Anthony Bharath             | 2017 | A Brief Survey of Deep Reinforcement Learning                 | [IEEE Signal Processing Magazine](https://ieeexplore.ieee.org/document/7926644)              |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf90bb4-1973-4153-bae6-030f9beac1d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
