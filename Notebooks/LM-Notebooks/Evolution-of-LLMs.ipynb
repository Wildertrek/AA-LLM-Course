{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9943b0ba-cfe0-4262-b757-414b9459ae72",
   "metadata": {},
   "source": [
    "# Evolution of Large Language Models (LLMs) with NLP Lense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba50ebe-a50e-4411-820a-03eda6522dae",
   "metadata": {},
   "source": [
    "A timeline of key events in the evolution of Large Language Models (LLMs):\n",
    "\n",
    "---\n",
    "\n",
    "### 1950s – **Onset of Natural Language Processing (NLP)**\n",
    "- **1954**: Researchers at IBM and Georgetown University create the first machine translation system, translating Russian phrases into English.\n",
    "\n",
    "### 1960s – **Rule-Based Models**\n",
    "- **1966**: Joseph Weizenbaum at MIT introduces **Eliza**, the first chatbot, using pattern recognition and rule-based systems.\n",
    "- **1970**: Terry Winograd develops **SHRDLU**, an early NLP program capable of understanding and manipulating a block world in natural language.\n",
    "\n",
    "### 1970s – **Statistical Language Models**\n",
    "- **1971**: Introduction of the **Hidden Markov Model (HMM)** for sequence prediction tasks like part-of-speech tagging.\n",
    "- **1990s**: Emergence of **N-gram models**, which estimate the probability of a word based on its preceding words, laying the foundation for statistical approaches to language modeling.\n",
    "\n",
    "### 1980s-1990s – **Deep Learning and Neural Networks**\n",
    "- **1986**: **Recurrent Neural Networks (RNNs)** enable capturing sequential dependencies in language.\n",
    "- **1997**: **Long Short-Term Memory (LSTM)** networks are introduced, addressing the issue of long-range dependencies in RNNs.\n",
    "- **1998**: Emergence of **Convolutional Neural Networks (CNNs)**, which, though primarily for image processing, begin being applied to text classification tasks.\n",
    "\n",
    "### 2000s – **Neural Networks and Word Embeddings**\n",
    "- **2003**: Bengio et al. introduce the first neural language model, leading to the development of **Word Embeddings** like **Word2Vec** in 2013.\n",
    "\n",
    "### 2010s – **Transformer Models**\n",
    "- **2017**: Google introduces the **Transformer architecture** with the paper “Attention is All You Need,” which revolutionizes LLMs by using self-attention mechanisms.\n",
    "- **2018**: OpenAI introduces **GPT-1**, marking the beginning of generative pre-training transformer models.\n",
    "- **2018**: Google releases **BERT (Bidirectional Encoder Representations from Transformers)**, a breakthrough in NLP for understanding the context of words bidirectionally.\n",
    "- **2019**: OpenAI releases **GPT-2**, showcasing the power of large-scale generative models with 1.5 billion parameters.\n",
    "\n",
    "### 2020s – **Explosion of Large Language Models**\n",
    "- **2020**: OpenAI unveils **GPT-3** with 175 billion parameters, excelling in a wide variety of tasks.\n",
    "- **2021**: Introduction of **LoRA (Low-Rank Adaptation)**, an efficient method for fine-tuning large models.\n",
    "- **2023**: OpenAI releases **GPT-4**, a trillion-parameter multimodal model capable of processing 50 pages of text.\n",
    "- **2023**: Meta’s **LLaMA** model emerges as an open-source LLM, followed by models like Falcon, MPT, and Zephyr, pushing open-source development further.\n",
    "\n",
    "\n",
    "### Latest LLMs\n",
    "\n",
    "[Liquid Foundation Models](https://www.liquid.ai/liquid-foundation-models)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "This timeline gives a structured view of the evolution of LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04735123-14e6-4274-abaf-0f29d02c5144",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
