{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebcc5d51-cc58-4423-aea7-73c0a502ba00",
   "metadata": {},
   "source": [
    "# Generative Pre-Trainined Transformer\n",
    "\n",
    "[2018 GPT](https://en.wikipedia.org/wiki/GPT-1) The transformer architecture was used to create the first autoregressive model, GPT. It then evolved into [GPT-2 2019](https://huggingface.co/transformers/v2.8.0/model_doc/gpt2.html), a larger and more optimized version of GPT pre-trained on WebText, and [GPT-3 2020](https://arxiv.org/abs/2005.14165), a larger and more optimized version of GPT-2 pre-trained on Common Crawl."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0be422-62e4-41fd-80e4-6cbfbcf18430",
   "metadata": {},
   "source": [
    "## The Origin GPT-1\n",
    "\n",
    "Generative Pre-trained Transformer 1 (GPT-1) was the first of OpenAI's large language models following Google's invention of the transformer architecture in 2017. In June 2018, OpenAI released a paper entitled \"Improving Language Understanding by Generative Pre-Training\", in which they introduced that initial model along with the general concept of a generative pre-trained transformer.\n",
    "\n",
    "Up to that point, the best-performing neural NLP models primarily employed supervised learning from large amounts of manually labeled data. This reliance on supervised learning limited their use of datasets that were not well-annotated, in addition to making it prohibitively expensive and time-consuming to train extremely large models\n",
    "\n",
    "In contrast, a GPT's \"semi-supervised\" approach involved two stages: \n",
    "    - an unsupervised generative \"pre-training\" stage in which a language modeling objective was used to set initial parameters, \n",
    "    - and a supervised discriminative \"fine-tuning\" stage in which these parameters were adapted to a target task.\n",
    "\n",
    "The use of a transformer architecture, as opposed to previous techniques involving attention-augmented RNNs, provided GPT models with a more structured memory than could be achieved through recurrent mechanisms; this resulted in \"robust transfer performance across diverse tasks\".\n",
    "\n",
    "Read [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)\n",
    "\n",
    "### Reason for choosing BookCorpus\n",
    "\n",
    "BookCorpus was chosen as a training dataset partly because the long passages of continuous text helped the model learn to handle long-range information. It contained over 7,000 unpublished fiction books from various genres. The rest of the datasets available at the time, while being larger, lacked this long-range structure (being \"shuffled\" at a sentence level).\n",
    "\n",
    "The BookCorpus text was cleaned by the ftfy library to standardized punctuation and whitespace and then tokenized by spaCy.\n",
    "\n",
    "Architecture\n",
    "The GPT-1 architecture was a twelve-layer decoder-only transformer, using twelve masked self-attention heads, with 64-dimensional states each (for a total of 768). Rather than simple [stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent), the [Adam optimization algorithm](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam) was used. GPT-1 has 110 million parameters.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/5/51/Full_GPT_architecture.svg/2560px-Full_GPT_architecture.svg.png\" alt=\"GPT Architecture\" width=\"600\" height=\"600\">\n",
    "\n",
    "This breakdown of the GPT architecture focuses on each component in sequence, offering detailed explanations of how they contribute to the overall function of the model. Here's a cleaner version of the explanation:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Input Embedding in GPT:**\n",
    "   - **Purpose:** Converts words or tokens into numerical vectors (dense embeddings) so the model can process them. These embeddings capture semantic relationships between tokens.\n",
    "   - **Explanation:** Words, subwords, or characters are represented as vectors in a continuous space (embeddings). Similar words have similar embeddings, while different words are farther apart in the vector space. Embeddings are learned during training and are passed to the transformer layers.\n",
    "\n",
    "#### **Steps in Input Embedding Process:**\n",
    "1. **Tokenization:** Text is divided into tokens. GPT uses techniques like **Byte-Pair Encoding (BPE)**, which splits text into manageable tokens (e.g., \"running\" -> \"run\" and \"ing\").\n",
    "2. **Vocabulary Mapping:** Each token is mapped to an index in the model's vocabulary. For instance, \"run\" might map to index 563.\n",
    "3. **Embedding Lookup Table:** Each token index is converted into a high-dimensional vector (embedding) using an embedding matrix.\n",
    "4. **Dense Representation (Embedding):** The output is a sequence of vectors representing tokens, passed into the transformer layers for further processing.\n",
    "\n",
    "---\n",
    "\n",
    "### Example:\n",
    "For the sentence `\"The cat sat on the mat.\"`, tokens are mapped to indices and converted to embeddings like:\n",
    "\n",
    "```plaintext\n",
    "Tokenized: [The] [cat] [sat] [on] [the] [mat]\n",
    "Token Indices: [3] [102] [205] [87] [3] [490]\n",
    "Embeddings: [[0.5, -1.3, 0.2, ...], [-0.8, 0.3, 1.1, ...], ...]\n",
    "```\n",
    "\n",
    "These dense embeddings are then fed into the transformer layers for processing.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Positional Encoding:**\n",
    "   - **Explanation:** Since transformers don't inherently understand the order of tokens, positional encoding is added to the embeddings to provide information about token positions in the sequence.\n",
    "\n",
    "#### **How Positional Encoding Works:**\n",
    "1. **Combination of Position and Embedding:** Position is encoded using sinusoidal functions, ensuring each token's position is uniquely represented.\n",
    "2. **Mathematical Formula:**\n",
    "   - For even dimensions:\n",
    "     $$ PE(p, 2i) = \\sin(p / 10000^{2i/d_{\\text{model}}}) $$\n",
    "   - For odd dimensions:\n",
    "     $$ PE(p, 2i+1) = \\cos(p / 10000^{2i/d_{\\text{model}}}) $$\n",
    "\n",
    "Where $p$ is the position, and $d_{\\text{model}}$ is the dimension of the embedding. These positional encodings are added element-wise to the embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "### Example:\n",
    "For the sentence `\"The cat sat on the mat.\"`, positional encodings are added to the embeddings:\n",
    "\n",
    "```plaintext\n",
    "Embedding + Positional Encoding for \"The\": [0.51, -1.28, 0.23, ...]\n",
    "Embedding + Positional Encoding for \"cat\": [-0.78, 0.34, 1.16, ...]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Dropout Layer:**\n",
    "   - **Explanation:** Dropout helps prevent overfitting by randomly \"dropping\" (setting to zero) a percentage of neurons during training. This forces the network to learn more robust patterns.\n",
    "\n",
    "#### **Why Dropout Helps:**\n",
    "- **Prevents Co-Adaptation:** Dropout prevents the model from becoming too reliant on any specific neurons, forcing different neurons to collaborate more effectively.\n",
    "- **Improves Generalization:** Helps the network perform better on unseen data.\n",
    "- **Simulates an Ensemble:** By randomly dropping neurons, dropout behaves like training multiple models that share parameters, improving robustness.\n",
    "\n",
    "During training, dropout randomly sets neurons to zero with a specific probability, e.g., 20% dropout. During inference, dropout is turned off, and the weights are scaled.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Transformer Blocks:**\n",
    "   - **Explanation:** GPT's architecture consists of stacked transformer blocks, each containing self-attention mechanisms, feedforward networks, and normalization layers.\n",
    "\n",
    "#### **Components of a Transformer Block:**\n",
    "1. **Layer Normalization (LayerNorm):** Stabilizes training by normalizing the input to each layer, ensuring consistent activation scaling.\n",
    "2. **Multi-Head Self-Attention:** Allows the model to focus on different parts of the input sequence simultaneously. Each head attends to a different subspace of the input, capturing multiple relationships between tokens.\n",
    "   \n",
    "   - **Query, Key, and Value Matrices:** Self-attention works by creating query, key, and value matrices from the input.\n",
    "   - **Attention Scores:** Calculated by multiplying the query matrix with the transposed key matrix and normalizing with softmax.\n",
    "   - **Weighted Sum of Values:** The attention scores are used to compute a weighted sum of the value matrix, determining the final output of the attention mechanism.\n",
    "   \n",
    "3. **Feedforward Neural Network:** A two-layer network that further transforms the output of the self-attention mechanism. It applies a **GELU activation function** to introduce non-linearity.\n",
    "4. **Residual Connection (+):** Adds the input of a layer back to its output, preventing vanishing gradient problems and stabilizing training.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **LayerNorm and Residual Connections:**\n",
    "   - **Explanation:** LayerNorm and residual connections occur twice in each transformer block, after both the self-attention and feedforward layers.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Feedforward Network (GELU Activation):**\n",
    "   - **Explanation:** The feedforward network transforms the output of the attention mechanism. The **GELU activation** introduces non-linearity, helping the model learn complex patterns.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **Final Output (Linear Layer and Softmax):**\n",
    "   - **Explanation:** After passing through all transformer blocks, the final output is processed by a linear layer and a softmax function to generate probabilities over the vocabulary, enabling next-token prediction during text generation.\n",
    "\n",
    "---\n",
    "\n",
    "This summary covers the key components of the GPT architecture and how each one contributes to the model's ability to process and generate text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff75ed6-6aae-4dd5-b7d8-4cafa166b255",
   "metadata": {},
   "source": [
    "| **Type** | **Model Name** | **#Parameters**  | **Release** | **Base Models** | **Open Source** | **#Tokens** | **Training Dataset** | **Context Window Size** |\n",
    "|-----------------|----------------|--------------------|-------------|-----------------|-----------------|---------------------------|-----------------|------------------|\n",
    "| **GPT Family**  | GPT-1           | 110M                | 2018   | ✓               | ✓               | 1.3B        | BooksCorpus, English Wikipedia |  |\n",
    "|                 | GPT-2           | 1.5B                | 2019   | ✓               | ✓               | 10B         | Reddit outbound                |  |\n",
    "|                 | GPT-3           | 6.7B, 13B, 175B     | 2020   | ×               | ×  | 300B | Common Crawl (filtered), WebText2, Books1, Books2, Wikipedia | |\n",
    "|                 | GPT-3.5         | 1.3B, 6B, 20B       | 2022   | ×               | ×               | 2.5T        | WebText, Common Crawl, Books, Wikipedia | |\n",
    "|                 | CODEX           | 12B                 | 2021   | GPT             | ✓               | -           | Public GitHub software repositories     | |\n",
    "|                 | WebGPT          | 760M, 13B, 175B     | 2021   | GPT-3           | ×               | -           | ELI5                                    | |\n",
    "|| [GPT-4](https://cdn.openai.com/papers/gpt-4.pdf)| 1.76T| 2023   | -               | ×               | 13T         | Diverse internet text | 128,000 tokens|\n",
    "|| [GPT-4o](https://platform.openai.com/docs/models/gpt-4o)| 220B (Experts)| 2024 | GPT-4 | ×  | 13T?                | Web, technical documents, and multimodal data |128,000 tokens |\n",
    "|| [GPT-4o Mini](https://platform.openai.com/docs/models/gpt-4o-mini)| 8B| 2024| GPT-4o| ×             | 13T?        | Cost-effective model for general API usage | 128,000 tokens|\n",
    "\n",
    "**Key updates**:\n",
    "1. **GPT-3.5** was introduced as an intermediate model between GPT-3 and GPT-4, with variants ranging from 1.3B to 20B parameters. It brought improvements in language generation and is used in ChatGPT's free version.\n",
    "2. **GPT-4o**, launched in 2024, uses a \"Mixture of Experts\" architecture with experts specialized in different tasks. It consists of multiple expert models (220B parameters per expert) that together sum up to a total of 1.76 trillion parameters.\n",
    "3. **GPT-4o Mini** is a lighter and more affordable version of GPT-4o, specifically aimed at smaller applications and businesses needing cost-effective AI solutions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66cc0f4-5867-4a9a-a903-c3d5915124ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
