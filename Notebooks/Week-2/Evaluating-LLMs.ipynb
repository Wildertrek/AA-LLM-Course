{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a88ce12c-f1a2-4616-a0a4-e0c98aa36ca5",
   "metadata": {},
   "source": [
    "# Evaluating LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975a7df2-739a-4ebb-8157-4c09bcfda74a",
   "metadata": {},
   "source": [
    "The table below provides a comprehensive overview of the numerous methods, tools, and datasets used to evaluate Large Language Models (LLMs) across various dimensions, reflecting the complexity and diversity of tasks these models can perform. Here’s a summary of the key categories and evaluation approaches:\n",
    "\n",
    "### 1. **General Evaluation Frameworks**\n",
    "   - **Tools** like **OpenAI’s Evals** and **DeepEval** provide versatile frameworks for custom LLM evaluations. These frameworks support metrics such as accuracy, F1, relevance, and hallucination detection, allowing for nuanced assessment of LLM outputs across multiple tasks. **LangChain** facilitates the creation of task-specific evaluation chains, which is particularly helpful for benchmarking complex workflows involving multiple components.\n",
    "\n",
    "### 2. **Benchmarking Suites**\n",
    "   - Suites like **EleutherAI’s LM Evaluation Harness** and **BIG-bench** offer extensive benchmarks that include a broad spectrum of open-ended and specialized tasks, challenging models with everything from basic text classification to intricate reasoning. These suites are essential for evaluating an LLM's general capabilities and adaptability to diverse scenarios.\n",
    "\n",
    "### 3. **Advanced Reasoning and Knowledge**\n",
    "   - Datasets such as **TruthfulQA**, **HellaSwag**, and **MMLU** assess models on their ability to generate truthful content, apply commonsense reasoning, and demonstrate cross-domain knowledge. These benchmarks are crucial for understanding how well models can reason and answer questions across disciplines, from the sciences to humanities.\n",
    "\n",
    "### 4. **Coding and Mathematical Reasoning**\n",
    "   - Tools like **HumanEval** and platforms like **Codeforces** evaluate LLMs on coding and problem-solving skills, crucial for models used in software development and technical fields. Benchmarks like **MATH** and **GSM8K** focus on the model’s ability to solve math problems, testing both computational accuracy and logical reasoning.\n",
    "\n",
    "### 5. **Task-Specific Evaluation Benchmarks**\n",
    "   - Datasets such as **SQuAD**, **GLUE**, and **SuperGLUE** provide robust evaluations for models on specific NLP tasks like reading comprehension, natural language inference, and sentiment analysis. **CoQA** and **MNLI** test conversational capabilities and inferential skills, respectively, making these benchmarks ideal for models geared towards interactive applications.\n",
    "\n",
    "### 6. **Evaluation Metrics**\n",
    "   - Metrics like **ROUGE** and **BLEU** are commonly used for summarization and translation tasks, measuring the overlap between generated and reference texts. **Perplexity** is often employed in language modeling to gauge a model’s predictive accuracy, while metrics like **Accuracy** offer a straightforward approach for assessing classification tasks.\n",
    "\n",
    "### 7. **Multilingual and Sentiment Analysis**\n",
    "   - Datasets such as **MARC** support the evaluation of multilingual sentiment analysis capabilities, useful for models designed for diverse linguistic contexts. This category is essential for assessing an LLM's ability to handle non-English data and perform sentiment analysis across languages.\n",
    "\n",
    "### 8. **Domain-Specific and Commonsense Reasoning**\n",
    "   - Datasets like **ARC** and **PIQA** are designed to evaluate models on domain-specific tasks and physical commonsense reasoning, respectively. These benchmarks measure how well models can apply knowledge to specific contexts and understand real-world interactions, providing insights into practical use cases.\n",
    "\n",
    "### Summary\n",
    "Together, these evaluation methods cover a wide range of LLM capabilities, including general language understanding, advanced reasoning, coding, and even sentiment analysis in multilingual contexts. They allow for a thorough analysis of model strengths, weaknesses, and specific areas where further training or fine-tuning may be necessary. By combining general frameworks, specialized benchmarks, and targeted metrics, researchers and developers can comprehensively assess LLM performance to ensure models meet the demands of varied applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746c7c38-57a2-455d-b92d-93c8574eb58e",
   "metadata": {},
   "source": [
    "| **Category**                     | **Tool**                                         | **Description**                                                                                                                                                 | **Tested LLMs**                 |\n",
    "|----------------------------------|--------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------|\n",
    "| **General Evaluation Frameworks** | [Evals by OpenAI](https://github.com/openai/evals) | A framework by OpenAI designed for evaluating the performance of LLMs on custom datasets, using quantitative metrics like accuracy and F1.                       | GPT-3, GPT-3.5, GPT-4, GPT-4o   |\n",
    "|                                  | [LangChain](https://github.com/hwchase17/langchain) | A library that supports evaluation by chaining together different components, making it easier to benchmark LLMs on diverse tasks.                               | GPT-3, GPT-3.5, GPT-4           |\n",
    "|                                  | [DeepEval](https://github.com/confident-ai/deepeval) | An open-source LLM evaluation framework similar to Pytest, designed for unit testing LLM outputs on metrics like G-Eval, hallucination, answer relevancy, and more. Supports fine-tuning and RAG workflows. | Compatible with all LLMs        |\n",
    "| **Benchmarking Suites**          | [EleutherAI's LM Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness) | A suite of tools for evaluating LLMs on a variety of NLP benchmarks, covering tasks like question answering, text classification, and more.                      | GPT-3, GPT-4                    |\n",
    "|                                  | [BIG-bench](https://github.com/google/BIG-bench)  | A large-scale benchmark from Google for testing LLMs on over 200 diverse tasks. It’s designed to stress-test models on open-ended, complex tasks.                | GPT-3, GPT-4, GPT-4o            |\n",
    "| **Advanced Reasoning and Knowledge** | [TruthfulQA](https://github.com/sylinrl/TruthfulQA) | Assesses a model’s ability to generate truthful responses and avoid misinformation.                                       | GPT-3.5, GPT-4, GPT-4o          |\n",
    "|                                  | [MMLU](https://github.com/hendrycks/test)        | Tests multitask knowledge across varied domains such as humanities, sciences, and technical fields.                                                              | GPT-3, GPT-4, GPT-4o            |\n",
    "|                                  | [HellaSwag](https://rowanzellers.com/hellaswag/) | Measures a model's ability to continue sentences in a way that makes sense, focusing on common sense reasoning.                                                 | GPT-3, GPT-3.5, GPT-4           |\n",
    "|                                  | [BBH](https://arxiv.org/abs/2208.03299)          | Evaluates human-level tasks like arithmetic, logical reasoning, and understanding concepts like dates and numbers.                                               | GPT-4, GPT-4o                    |\n",
    "| | [ANLI](https://github.com/facebookresearch/anli)        | Adversarially-created Natural Language Inference dataset, assessing complex reasoning with challenging cases.     | GPT-3.5, GPT-4                              |\n",
    "|                                  | [ARC](https://allenai.org/data/arc)                      | Science questions requiring sophisticated reasoning and background knowledge, aimed at school-level science exams. | GPT-3, GPT-4                                |\n",
    "| **Coding and Mathematical Reasoning** | [HumanEval](https://github.com/openai/human-eval) | Evaluates models' programming skills by generating code in response to prompts.                                            | GPT-3, GPT-3.5, GPT-4, GPT-4o   |\n",
    "|                                  | [MATH](https://github.com/hendrycks/math)        | Tests the model's ability to solve high school-level math problems.                                                                                             | GPT-4, GPT-4o                    |\n",
    "|                                  | [GSM8K](https://github.com/openai/grade-school-math) | Focuses on elementary-level math tasks, testing arithmetic and basic multi-step reasoning skills.                                                                 | GPT-4, GPT-4o                    |\n",
    "|                                  | [Codeforces](https://codeforces.com/)            | A competitive programming platform to test advanced coding and problem-solving skills.                                                                           | GPT-4, GPT-4o                    |\n",
    "| **Task-Specific Evaluation Benchmarks** | [CoQA](https://stanfordnlp.github.io/coqa/) | The Conversational Question Answering dataset used to evaluate a model's ability to answer questions in a conversational context, measuring F1 score.             | GPT-2, GPT-3                     |\n",
    "|                                  | [MNLI](https://cims.nyu.edu/~sbowman/multinli/)   | The Multi-Genre Natural Language Inference benchmark that tests a model’s ability to determine the relationship between pairs of sentences.                      | GPT-2, GPT-3, GPT-4              |\n",
    "|                                  | [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) | The Stanford Question Answering Dataset used for evaluating a model’s reading comprehension by answering questions based on Wikipedia articles.                  | GPT-2, GPT-3, GPT-4              |\n",
    "|                                  | [GLUE](https://gluebenchmark.com/)                | The General Language Understanding Evaluation benchmark suite that assesses LLMs on a variety of NLP tasks, such as sentiment analysis, paraphrase detection, and more. | GPT-2, GPT-3, GPT-4              |\n",
    "|                                  | [SuperGLUE](https://super.gluebenchmark.com/)     | An enhanced version of GLUE that includes more challenging NLP tasks, designed to evaluate cutting-edge language models on general language understanding.        | GPT-3, GPT-4                      |\n",
    "|                                  | [Winograd Schema Challenge](https://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/WS.html) | A task for evaluating a model's common sense reasoning ability by resolving ambiguities in pronoun references.                                                   | GPT-3, GPT-4                      |\n",
    "|                                  | [WikiText-2](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/) | A Wikipedia-based dataset with a large vocabulary, used for testing general language modeling abilities.                                                          | GPT-2, GPT-3                     |\n",
    "|                                  | [PTB (Penn Treebank)](https://catalog.ldc.upenn.edu/LDC99T42) | A language modeling dataset based on Wall Street Journal articles, known for its small vocabulary.                                                               | GPT-2, GPT-3                     |\n",
    "|                                  | [CBT (Children's Book Test)](https://research.fb.com/downloads/babi/) | Tests story comprehension by requiring models to fill in missing words in children's book passages.                                                              | GPT-2, GPT-3                     |\n",
    "|                                  | [WikiText-103](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/) | A larger version of WikiText-2, with a greater variety of topics and vocabulary.                                                                                | GPT-2, GPT-3                     |\n",
    "|                                  | [BookCorpus](https://yknzhu.wixsite.com/mbweb)     | A dataset of unpublished books providing narrative-rich text, challenging models with diverse styles.                                                            | GPT-2, GPT-3                     |\n",
    "|                                  | [1BW (One Billion Word)](http://www.statmt.org/lm-benchmark/) | Consists of sentences from news articles, used for large-scale language modeling.                                                                               | GPT-2, GPT-3                     |\n",
    "|                                  | [enwiki8 (Hutter Prize)](http://prize.hutter1.net/) | A highly compressed dataset based on Wikipedia text, commonly used for language model compression benchmarks.                                                    | GPT-2, GPT-3                     |\n",
    "|                                  | [OpenWebText](https://github.com/skeskinen/OpenWebText) | An open-source alternative to WebText, designed to replicate the dataset methodology using high-quality web content shared on Reddit.                            | GPT-2, GPT-3, GPT-4              |\n",
    "|  | [BoolQ](https://github.com/google-research-datasets/boolean-questions) | Binary (yes/no) question-answering based on factual content.                                               | GPT-3.5, GPT-4                              |\n",
    "|                                  | [OpenBookQA](https://allenai.org/data/open-book-qa)      | Open-book science QA on elementary-level science, requiring factual recall and knowledge application.             | GPT-4                                       |\n",
    "|                                  | [PIQA](https://yonatanbisk.com/piqa/)                    | Physical commonsense reasoning, focusing on real-world physical interactions and understanding.                   | GPT-3.5, GPT-4                              |\n",
    "| **Evaluation Metrics**           | [ROUGE](https://pypi.org/project/rouge-score/)   | A set of metrics for evaluating the quality of text summaries, based on overlapping n-grams, word sequences, and word pairs.                                    | GPT-3, GPT-4                    |\n",
    "|                                  | [BLEU](https://pypi.org/project/bleu/)           | Metric for evaluating text generation tasks, often used in machine translation by measuring the overlap between generated and reference translations.            | GPT-3, GPT-4                    |\n",
    "|                                  | [Perplexity](https://en.wikipedia.org/wiki/Perplexity) | Measures how well a language model predicts a sample, with lower values indicating better predictive performance. Commonly used for language modeling tasks.     | GPT-2, GPT-3, GPT-4, GPT-4o     |\n",
    "|                                  | [Accuracy](https://en.wikipedia.org/wiki/Accuracy_and_precision) | A basic metric for classification tasks, measuring the ratio of correctly predicted instances to the total instances.                                            | All LLMs                        |\n",
    "|                                  | [TREC](https://trec.nist.gov/)                            | Text retrieval benchmark for evaluating search and information retrieval tasks.                                   | Used for GPT-4 with retrieval capabilities  |\n",
    "| **Multilingual and Sentiment Analysis** | [MARC](https://registry.opendata.aws/amazon-reviews-ml/)   | Multilingual Amazon Reviews corpus, testing sentiment analysis across various languages.                        | GPT-4 with multilingual capabilities         |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f69fdd-2ea5-4397-bb92-58c0138291e2",
   "metadata": {},
   "source": [
    "# Exams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83647e51-f902-4de0-92a4-0e6d06b99232",
   "metadata": {},
   "source": [
    "The below table provides an extensive overview of Large Language Models' (LLMs) performance on a variety of academic and competitive exams, showcasing the versatility of models like GPT-4 and GPT-4o across different knowledge domains and skill sets. Here are the primary categories and a summary of LLM capabilities in each area:\n",
    "\n",
    "### 1. **Advanced Placement (AP) Exams**\n",
    "   - LLMs, particularly GPT-4 and GPT-4o, were tested on a range of AP subjects, from humanities like **AP English Literature** and **AP US History** to sciences like **AP Chemistry** and **AP Biology**. Generally, the models show moderate to strong performance, with scores ranging from 2 to 4 on a scale of 1 to 5, indicating their ability to understand and process complex material across disciplines. Notably, in subjects like **AP English Literature** and **AP Psychology**, GPT-4 achieved scores of 4, demonstrating advanced comprehension and analytical skills.\n",
    "\n",
    "### 2. **Standardized Tests**\n",
    "   - Performance on standardized tests such as the **SAT** and **GRE** highlights GPT-4’s proficiency in both mathematical and verbal reasoning. On the **SAT Math** section, GPT-4 scored close to perfect, while its **GRE Verbal** score of 165/170 reflects a strong grasp of vocabulary and critical thinking. These scores suggest that LLMs are capable of performing well on exams that require broad general knowledge and reasoning.\n",
    "\n",
    "### 3. **Math Competitions**\n",
    "   - In competitive math environments like the **AMC 10** and **AMC 12**, which test advanced high school math skills, GPT-4 demonstrated average performance with scores reflecting approximately 50-60% accuracy. This indicates that while LLMs can handle complex problem-solving to some extent, there may still be limitations in areas that require deep, abstract reasoning and higher-order math skills.\n",
    "\n",
    "### 4. **Law Exams**\n",
    "   - For legal exams like the **Uniform Bar Exam** and **LSAT**, which assess knowledge of law and legal reasoning, GPT-4 scored respectably. With a score of 276/400 on the Bar Exam, GPT-4 shows potential for applications in legal contexts, particularly for preliminary research or educational purposes. However, it may still require refinement for more specialized legal tasks.\n",
    "\n",
    "### 5. **Competitive Coding**\n",
    "   - On coding platforms like **Codeforces**, which rates programming skills based on problem-solving and algorithmic challenges, GPT-4 achieved a score range of 1300-1400, suggesting an intermediate level of coding proficiency. This indicates that while LLMs can assist with programming tasks, they may not yet rival top human coders in competitive settings.\n",
    "\n",
    "### 6. **Science Competitions**\n",
    "   - In science-oriented competitions such as the **USABO Semifinal**, which covers advanced biology topics, GPT-4’s performance averaged around 45%. This points to an understanding of scientific concepts but with room for improvement, particularly in more specialized or nuanced scientific inquiries.\n",
    "\n",
    "Overall, this table reflects that while LLMs like GPT-4 excel in generalized exams (such as AP and standardized tests), they face challenges in highly specialized fields, such as advanced mathematics, legal reasoning, and competitive programming. However, their consistent performance across such a diverse range of exams underscores their versatility and potential for educational and research applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba280c41-fe6f-4722-9a92-c1f808dfda52",
   "metadata": {},
   "source": [
    "| **Exam Category**       | **Exam Name**                 | **Description**                                                                                                  | **Typical LLM Performance**                |\n",
    "|-------------------------|-------------------------------|------------------------------------------------------------------------------------------------------------------|--------------------------------------------|\n",
    "| **Advanced Placement (AP)** | [AP Calculus BC](https://apstudents.collegeboard.org/courses/ap-calculus-bc)         | Covers advanced calculus concepts, including differential and integral calculus.                                  | GPT-4: 3, GPT-4o: 3                        |\n",
    "|                         | [AP English Literature](https://apstudents.collegeboard.org/courses/ap-english-literature-and-composition) | Exam assessing reading comprehension and literary analysis.                                                   | GPT-4: 4                                   |\n",
    "|                         | [AP English Language](https://apstudents.collegeboard.org/courses/ap-english-language-and-composition) | Exam covering English composition and rhetorical analysis.                                                  | GPT-4: 3.5, GPT-4o: 3                      |\n",
    "|                         | [AP Chemistry](https://apstudents.collegeboard.org/courses/ap-chemistry)               | Covers general chemistry principles.                                                                              | GPT-4: 2, GPT-4o: 2.5                      |\n",
    "|                         | [AP Physics 2](https://apstudents.collegeboard.org/courses/ap-physics-2-algebra-based)  | Covers topics in fluid dynamics, thermodynamics, and electromagnetism.                                           | GPT-4: 2, GPT-4o: 2                        |\n",
    "|                         | [AP Macroeconomics](https://apstudents.collegeboard.org/courses/ap-macroeconomics)     | Principles of macroeconomics, including economic indicators and policy.                                          | GPT-4: 3.5, GPT-4o: 3                      |\n",
    "|                         | [AP Microeconomics](https://apstudents.collegeboard.org/courses/ap-microeconomics)     | Covers microeconomic principles, such as market structures and consumer behavior.                                | GPT-4: 3, GPT-4o: 3                        |\n",
    "|                         | [AP Biology](https://apstudents.collegeboard.org/courses/ap-biology)                   | Covers topics in genetics, ecology, and cellular biology.                                                        | GPT-4: 3, GPT-4o: 3                        |\n",
    "|                         | [AP World History](https://apstudents.collegeboard.org/courses/ap-world-history-modern) | Covers global historical events and civilizations.                                                               | GPT-4: 3                                   |\n",
    "|                         | [AP US History](https://apstudents.collegeboard.org/courses/ap-united-states-history)   | Covers U.S. history from pre-Columbian times to present.                                                         | GPT-4: 3.5                                 |\n",
    "|                         | [AP US Government](https://apstudents.collegeboard.org/courses/ap-united-states-government-and-politics) | Exam covering U.S. government structures and political systems.                                                | GPT-4: 3                                   |\n",
    "|                         | [AP Psychology](https://apstudents.collegeboard.org/courses/ap-psychology)             | Covers psychological theories and concepts, including cognition and behavior.                                    | GPT-4: 4, GPT-4o: 3.5                      |\n",
    "|                         | [AP Art History](https://apstudents.collegeboard.org/courses/ap-art-history)           | Exam covering art movements, techniques, and historical pieces.                                                 | GPT-4: 3                                   |\n",
    "|                         | [AP Environmental Science](https://apstudents.collegeboard.org/courses/ap-environmental-science) | Covers human impact on the environment and sustainability.                                                       | GPT-4: 2.5                                 |\n",
    "|                         | [AP Statistics](https://apstudents.collegeboard.org/courses/ap-statistics)             | Exam covering probability, data interpretation, and statistical analysis.                                        | GPT-4: 2.5, GPT-4o: 2                      |\n",
    "| **Standardized Tests**  | [SAT Math](https://collegereadiness.collegeboard.org/sat)                             | Math section covering algebra, geometry, and trigonometry.                                                       | GPT-4: 700/800, GPT-4o: 680/800            |\n",
    "|                         | [SAT EBRW](https://collegereadiness.collegeboard.org/sat)                              | Reading and writing section covering grammar, vocabulary, and comprehension.                                     | GPT-4: 740/800, GPT-4o: 720/800            |\n",
    "|                         | [GRE Quantitative](https://www.ets.org/gre.html)                                       | Quantitative section covering math skills like algebra, geometry, and data analysis.                             | GPT-4: 153/170                             |\n",
    "|                         | [GRE Verbal](https://www.ets.org/gre.html)                                             | Verbal section covering vocabulary, reading comprehension, and critical thinking.                                | GPT-4: 165/170                             |\n",
    "|                         | [GRE Writing](https://www.ets.org/gre.html)                                            | Analytical writing section assessing argument analysis and writing skills.                                       | GPT-4: 3.5/6                               |\n",
    "| **Math Competitions**   | [AMC 12](https://www.maa.org/math-competitions/amc-1012)                               | Covers advanced high school math topics, including algebra, geometry, and combinatorics.                         | GPT-4: 50% average performance             |\n",
    "|                         | [AMC 10](https://www.maa.org/math-competitions/amc-1012)                               | High school math competition with introductory algebra and geometry.                                             | GPT-4: 60% average performance             |\n",
    "| **Law Exams**           | [Uniform Bar Exam](https://www.ncbex.org/exams/ube/)                                   | A general legal knowledge exam covering various legal topics required for law licensure in the U.S.              | GPT-4: 276/400, GPT-4o: 270/400            |\n",
    "|                         | [LSAT](https://www.lsac.org/lsat)                                                      | Law school admissions test covering logical and analytical reasoning and reading comprehension.                  | GPT-4: 150/180                             |\n",
    "| **Competitive Coding**  | [Codeforces Rating](https://codeforces.com/)                                           | Competitive programming platform with a rating system based on problem-solving skills.                           | GPT-4: 1300-1400                           |\n",
    "| **Science Competitions**| [USABO Semifinal 2020](https://www.usabo-trc.org/)                                     | Biology competition covering advanced biology topics.                                                            | GPT-4: 45% average performance             |\n",
    "| **Other AP Exams**      | [AP Microeconomics](https://apstudents.collegeboard.org/courses/ap-microeconomics)     | Covers microeconomic principles, including market structures and consumer theory.                                | GPT-4: 3, GPT-4o: 3                        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db6ef44-4e16-4355-81eb-0ec332d45b26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
