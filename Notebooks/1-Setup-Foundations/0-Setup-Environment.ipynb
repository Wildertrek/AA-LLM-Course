{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b669b815-cf4b-4061-a20e-13a64110ba13",
   "metadata": {},
   "source": [
    "# Set up your environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3deedde1-147e-41de-a6b1-33ba720e7774",
   "metadata": {},
   "source": [
    "Let's use a `.env` file and the `dotenv` library. This allows you to keep your API key secure and separated from your code.\n",
    "\n",
    "Here’s how you can setup to load the OpenAI API key using a `.env` file:\n",
    "\n",
    "### Step 1: Install Dependencies\n",
    "Make sure you have `python-dotenv` installed if you're using `.env` files:\n",
    "```bash\n",
    "pip install python-dotenv\n",
    "pip install -U langsmith\n",
    "pip install openai\n",
    "pip install langchain_openai\n",
    "```\n",
    "\n",
    "### Step 2: Create a `.env` File\n",
    "In your project directory, create a `.env` file with the following content:\n",
    "```\n",
    "OPENAI_API_KEY=your-api-key-here\n",
    "```\n",
    "\n",
    "### Step 3: Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "149817a3-07b4-49ee-b0be-1e445041e62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install openai --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a17abb2-21df-4045-ae49-098a5aee17f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain_openai --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4690e85c-83b2-4db6-aa2f-206b0fcbd6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79f2a737-18df-4b3e-b197-c183222c6b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from GPT-4o: Hello! How can I assist you today?\n",
      "Response from GPT-4o-mini: Hello, Joseph! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Function to load environment variables or raise an error if not found\n",
    "def get_env_var(var: str):\n",
    "    value = os.getenv(var)\n",
    "    if value is None:\n",
    "        raise ValueError(f\"{var} not found in environment variables. Make sure it is set in your .env file.\")\n",
    "    return value\n",
    "\n",
    "# Load API keys from the environment\n",
    "langchain_api_key = get_env_var(\"LANGCHAIN_API_KEY\")  # LangChain tracing (if applicable)\n",
    "langchain_tracing_v2 = get_env_var(\"LANGCHAIN_TRACING_V2\")  # LangChain tracing V2 (optional)\n",
    "openai_api_key = get_env_var(\"OPENAI_API_COURSE_KEY\")  # OpenAI API key\n",
    "tavily_api_key = get_env_var(\"TAVILY_API_KEY\")  # Other API key (if used)\n",
    "\n",
    "# Import and configure LangChain OpenAI integration\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize ChatOpenAI with the desired models and temperature settings\n",
    "gpt4o_chat = ChatOpenAI(model=\"gpt-4o\", temperature=0, openai_api_key=openai_api_key)\n",
    "gpt4o_mini_chat = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "# Import LangChain message classes\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Create a message using HumanMessage\n",
    "msg = HumanMessage(content=\"Hello world\", name=\"Joseph\")  # Add content and optional metadata\n",
    "\n",
    "# Create a list of messages\n",
    "messages = [msg]\n",
    "\n",
    "# Invoke the GPT-4o model with the message list\n",
    "response_gpt4o = gpt4o_chat.invoke(messages)\n",
    "print(\"Response from GPT-4o:\", response_gpt4o.content)\n",
    "\n",
    "# Invoke the GPT-4o-mini model with the message list\n",
    "response_gpt4o_mini = gpt4o_mini_chat.invoke(messages)\n",
    "print(\"Response from GPT-4o-mini:\", response_gpt4o_mini.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8aaa1a2c-ea45-4552-9075-198123903ce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 11, 'total_tokens': 21, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_703d4ff298', 'finish_reason': 'stop', 'logprobs': None}, id='run-e8f851bc-0d54-4bf2-b9dc-34788957bbe8-0', usage_metadata={'input_tokens': 11, 'output_tokens': 10, 'total_tokens': 21, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Create a message\n",
    "msg = HumanMessage(content=\"Hello world\", name=\"Joseph\")\n",
    "\n",
    "# Message list\n",
    "messages = [msg]\n",
    "\n",
    "# Invoke the model with a list of messages \n",
    "gpt4o_chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1acfc41-e0cc-404a-8c4e-c107b18884bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 9, 'total_tokens': 18, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_9faba9f038', 'finish_reason': 'stop', 'logprobs': None}, id='run-ec6d50a0-e1f6-49b3-8ce0-fcf45817c745-0', usage_metadata={'input_tokens': 9, 'output_tokens': 9, 'total_tokens': 18, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt4o_chat.invoke(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93bcdd11-9a59-4e39-8daf-cdff98ec116c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 9, 'total_tokens': 19, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None}, id='run-635e1c17-8cb6-45dd-850e-df172458e121-0', usage_metadata={'input_tokens': 9, 'output_tokens': 10, 'total_tokens': 19, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt4o_mini_chat.invoke(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7b20566-6fdf-4f6b-8700-21607f395f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install langchain_community --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf97e30f-a1ab-41c1-80f2-c5fd729a495d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Tavily using the API key from the environment\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "tavily_search = TavilySearchResults(max_results=3, api_key=tavily_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a31011e-cffb-4c8d-8482-b0b70d94771f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://langchain-ai.github.io/langgraph/',\n",
       "  'content': 'Overview¶. LangGraph is a library for building stateful, multi-actor applications with LLMs, used to create agent and multi-agent workflows. Compared to other LLM frameworks, it offers these core benefits: cycles, controllability, and persistence. LangGraph allows you to define flows that involve cycles, essential for most agentic architectures, differentiating it from DAG-based solutions.'},\n",
       " {'url': 'https://www.datacamp.com/tutorial/langgraph-tutorial',\n",
       "  'content': 'LangGraph is a library within the LangChain ecosystem designed to tackle these challenges head-on. LangGraph provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured manner.'},\n",
       " {'url': 'https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141',\n",
       "  'content': 'LangGraph is a versatile tool for building complex, stateful applications with LLMs. By understanding its core concepts and working through simple examples, beginners can start to leverage its'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform a search with Tavily\n",
    "search_docs = tavily_search.invoke(\"What is LangGraph?\")\n",
    "search_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9677a7af-8d33-4754-97ef-3bcc4c4a3f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain tracing started successfully.\n",
      "Agent's Response:\n",
      "content='Transformers have revolutionized natural language processing (NLP) by significantly improving the performance of various NLP tasks. Some of the key benefits of using transformers in NLP include:\\n\\n1. Enhanced performance: Transformers have shown superior performance compared to traditional NLP models, especially in tasks like language translation, text generation, and sentiment analysis.\\n\\n2. Attention mechanism: Transformers use an attention mechanism that allows the model to focus on relevant parts of the input sequence, making them more effective at capturing long-range dependencies in text data.\\n\\n3. Scalability: Transformers are highly scalable and can be easily adapted to different languages and tasks without the need for extensive retraining.\\n\\n4. Transfer learning: Transformers can be fine-tuned on specific tasks with relatively small amounts of data, making them ideal for transfer learning and domain adaptation.\\n\\n5. Interpretability: Transformers provide better interpretability compared to other deep learning models, as the attention mechanism allows users to understand which parts of the input sequence are most important for making predictions.\\n\\nOverall, transformers have become the go-to architecture for many NLP tasks due to their superior performance, scalability, and interpretability.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 227, 'prompt_tokens': 33, 'total_tokens': 260, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-71b89b77-12f2-4aba-95af-8745fefeb2fd-0' usage_metadata={'input_tokens': 33, 'output_tokens': 227, 'total_tokens': 260, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "Follow-up Response:\n",
      "content='Certainly! GPT (Generative Pre-trained Transformer) and BERT (Bidirectional Encoder Representations from Transformers) are both popular models in the field of natural language processing (NLP). \\n\\nThe main difference between GPT and BERT lies in their architecture and training objectives. GPT is a generative model that predicts the next word in a sequence of text, while BERT is a discriminative model that is trained to understand the context of a word in a sentence by considering both the words before and after it.\\n\\nIn simpler terms, GPT is better suited for tasks that require generating text, such as language translation or text completion, while BERT is more suitable for tasks that require understanding the meaning of a sentence, such as sentiment analysis or question answering.\\n\\nOverall, both GPT and BERT have their own strengths and weaknesses, and the choice between them depends on the specific NLP task at hand.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 185, 'prompt_tokens': 33, 'total_tokens': 218, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-86588597-237e-45c4-8d77-6d31dbbdec58-0' usage_metadata={'input_tokens': 33, 'output_tokens': 185, 'total_tokens': 218, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "\n",
      "Total Tokens Used: 478\n",
      "Total Cost (USD): $0.0006510000000000001\n",
      "LangChain tracing ended successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Function to load environment variables or raise an error if not found\n",
    "def get_env_var(var: str):\n",
    "    value = os.getenv(var)\n",
    "    if value is None:\n",
    "        raise ValueError(f\"{var} not found in environment variables. Make sure it is set in your .env file.\")\n",
    "    return value\n",
    "\n",
    "# Load API keys from the environment\n",
    "openai_api_key = get_env_var(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Initialize the OpenAI LLM via LangChain\n",
    "gpt_chat = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "# Define a simple prompt template for conversation\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"user_input\"],\n",
    "    template=\"You are a helpful assistant. User asks: {user_input}. How would you respond?\"\n",
    ")\n",
    "\n",
    "# Instead of using RunnableSequence, directly chain the prompt and model using the | operator\n",
    "conversation_chain = prompt_template | gpt_chat\n",
    "\n",
    "try:\n",
    "    # Start callback for tracing OpenAI usage\n",
    "    with get_openai_callback() as cb:\n",
    "        print(\"LangChain tracing started successfully.\")\n",
    "\n",
    "        # Example conversation - LangChain will trace this operation\n",
    "        user_input = \"What are the benefits of using transformers in natural language processing?\"\n",
    "        response = conversation_chain.invoke({\"user_input\": user_input})\n",
    "\n",
    "        print(\"Agent's Response:\")\n",
    "        print(response)\n",
    "\n",
    "        # You can extend this conversation further\n",
    "        follow_up = \"Can you explain the difference between GPT and BERT?\"\n",
    "        follow_up_response = conversation_chain.invoke({\"user_input\": follow_up})\n",
    "\n",
    "        print(\"Follow-up Response:\")\n",
    "        print(follow_up_response)\n",
    "\n",
    "        # Output callback information\n",
    "        print(f\"\\nTotal Tokens Used: {cb.total_tokens}\")\n",
    "        print(f\"Total Cost (USD): ${cb.total_cost}\")\n",
    "\n",
    "    print(\"LangChain tracing ended successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error with LangChain tracing: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ea413de-2c56-4684-9133-519dd94fe26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain tracing started successfully.\n",
      "Agent's Response:\n",
      "content=\"Transformers have revolutionized natural language processing (NLP) due to several key benefits:\\n\\n1. **Parallelization**: Unlike traditional recurrent neural networks (RNNs), which process sequences sequentially, transformers allow for parallel processing of data. This significantly speeds up training times and makes it feasible to work with large datasets.\\n\\n2. **Long-Range Dependencies**: Transformers use self-attention mechanisms that enable them to capture relationships between words regardless of their distance in the text. This is particularly useful for understanding context and meaning in long sentences or documents.\\n\\n3. **Scalability**: Transformers can be scaled up effectively. Larger models, such as BERT and GPT, have shown that increasing the model size and training data can lead to substantial improvements in performance across various NLP tasks.\\n\\n4. **Transfer Learning**: Pre-trained transformer models can be fine-tuned on specific tasks with relatively small amounts of data. This transfer learning capability allows for efficient use of resources and can lead to high performance even in low-data scenarios.\\n\\n5. **Flexibility**: Transformers can be adapted for a wide range of NLP tasks, including text classification, translation, summarization, and question answering. Their architecture is versatile and can be modified to suit different applications.\\n\\n6. **Contextualized Representations**: Transformers generate contextual embeddings for words, meaning that the representation of a word can change based on its context in a sentence. This leads to a better understanding of nuances in language.\\n\\n7. **State-of-the-Art Performance**: Many transformer-based models have set new benchmarks on various NLP tasks, demonstrating their effectiveness and reliability in real-world applications.\\n\\n8. **Rich Attention Mechanisms**: The attention mechanism allows the model to focus on relevant parts of the input when making predictions, which enhances the model's ability to understand and generate human-like text.\\n\\nOverall, the introduction of transformers has led to significant advancements in NLP, making it possible to tackle complex language tasks more effectively than ever before.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 399, 'prompt_tokens': 33, 'total_tokens': 432, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None} id='run-482257e9-28e5-4054-97ca-324f4d2ecc29-0' usage_metadata={'input_tokens': 33, 'output_tokens': 399, 'total_tokens': 432, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "Follow-up Response:\n",
      "content='Certainly! GPT (Generative Pre-trained Transformer) and BERT (Bidirectional Encoder Representations from Transformers) are both models based on the Transformer architecture, but they are designed for different purposes and have distinct characteristics.\\n\\n### Key Differences:\\n\\n1. **Architecture**:\\n   - **GPT**: GPT is a unidirectional model, meaning it processes text in a left-to-right manner. It generates text by predicting the next word in a sequence based on the words that come before it. This makes it particularly well-suited for tasks that involve text generation.\\n   - **BERT**: BERT, on the other hand, is a bidirectional model. It looks at the entire context of a word by considering both the words that come before and after it. This allows BERT to understand the context of words more effectively, making it ideal for tasks that require understanding the meaning of text, such as question answering and sentiment analysis.\\n\\n2. **Training Objective**:\\n   - **GPT**: GPT is trained using a language modeling objective, specifically next-token prediction. It learns to predict the next word in a sentence given the previous words.\\n   - **BERT**: BERT is trained using a masked language modeling objective. During training, some words in the input are masked (hidden), and the model learns to predict these masked words based on the surrounding context. This helps BERT capture deeper contextual relationships.\\n\\n3. **Use Cases**:\\n   - **GPT**: Due to its generative nature, GPT is often used for tasks like text generation, dialogue systems, and creative writing. It excels in generating coherent and contextually relevant text.\\n   - **BERT**: BERT is primarily used for understanding tasks, such as text classification, named entity recognition, and question answering. It is effective in scenarios where understanding the nuances of language is crucial.\\n\\n4. **Fine-tuning**:\\n   - **GPT**: While GPT can be fine-tuned for specific tasks, it is often used in a zero-shot or few-shot manner, where it generates responses based on prompts without extensive task-specific training.\\n   - **BERT**: BERT is typically fine-tuned on specific downstream tasks, allowing it to adapt its learned representations to perform well on those tasks.\\n\\n### Summary:\\nIn summary, GPT is a unidirectional model focused on text generation, while BERT is a bidirectional model designed for understanding and interpreting text. Their different architectures and training objectives make them suitable for different types of natural language processing tasks.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 512, 'prompt_tokens': 32, 'total_tokens': 544, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None} id='run-bfef4051-d5a8-4f07-b135-af72fb55865b-0' usage_metadata={'input_tokens': 32, 'output_tokens': 512, 'total_tokens': 544, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "\n",
      "Total Tokens Used: 976\n",
      "Total Cost (USD): $0.0005563499999999999\n",
      "LangChain tracing ended successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Function to load environment variables or raise an error if not found\n",
    "def get_env_var(var: str):\n",
    "    value = os.getenv(var)\n",
    "    if value is None:\n",
    "        raise ValueError(f\"{var} not found in environment variables. Make sure it is set in your .env file.\")\n",
    "    return value\n",
    "\n",
    "# Load API keys from the environment\n",
    "openai_api_key = get_env_var(\"OPENAI_API_COURSE_KEY\")\n",
    "\n",
    "# Initialize the OpenAI LLM via LangChain with GPT-4o-mini\n",
    "gpt4o_mini_chat = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "# Define a simple prompt template for conversation\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"user_input\"],\n",
    "    template=\"You are a helpful assistant. User asks: {user_input}. How would you respond?\"\n",
    ")\n",
    "\n",
    "# Chain the prompt and model using the | operator\n",
    "conversation_chain = prompt_template | gpt4o_mini_chat\n",
    "\n",
    "try:\n",
    "    # Start callback for tracing OpenAI usage\n",
    "    with get_openai_callback() as cb:\n",
    "        print(\"LangChain tracing started successfully.\")\n",
    "\n",
    "        # Example conversation - LangChain will trace this operation\n",
    "        user_input = \"What are the benefits of using transformers in natural language processing?\"\n",
    "        response = conversation_chain.invoke({\"user_input\": user_input})\n",
    "\n",
    "        print(\"Agent's Response:\")\n",
    "        print(response)\n",
    "\n",
    "        # Extend the conversation further\n",
    "        follow_up = \"Can you explain the difference between GPT and BERT?\"\n",
    "        follow_up_response = conversation_chain.invoke({\"user_input\": follow_up})\n",
    "\n",
    "        print(\"Follow-up Response:\")\n",
    "        print(follow_up_response)\n",
    "\n",
    "        # Output callback information\n",
    "        print(f\"\\nTotal Tokens Used: {cb.total_tokens}\")\n",
    "        print(f\"Total Cost (USD): ${cb.total_cost}\")\n",
    "\n",
    "    print(\"LangChain tracing ended successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error with LangChain tracing: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b7eb07e-c9f5-4945-9163-405db7cdaa39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cockney Seller's Response: Well, 'ello there, mate! You’re lookin’ for a bit o’ fruit, eh? Well, let me tell ya, when it comes to apples, they’re the bees' knees, they are! A proper juicy bit o’ Adam and Eve, perfect for a munch or a pie, innit? \n",
      "\n",
      "You can have ‘em green or red, but I reckon a nice rosy one’s the way to go! Just the ticket for a sweet treat, or chuck ‘em in a crumble for a right old knees-up! What d’ya say, fancy a few?\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Function to retrieve environment variables\n",
    "def get_env_var(var: str):\n",
    "    value = os.getenv(var)\n",
    "    if value is None:\n",
    "        raise ValueError(f\"{var} not found in environment variables. Ensure it is set in your .env file.\")\n",
    "    return value\n",
    "\n",
    "# Load the OpenAI API key from .env\n",
    "openai_api_key = get_env_var(\"OPENAI_API_COURSE_KEY\")\n",
    "\n",
    "# Initialize the OpenAI model with LangChain\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", openai_api_key=openai_api_key)\n",
    "\n",
    "# Define a custom prompt template\n",
    "template = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    You are a cockney fruit and vegetable seller.\n",
    "    Your role is to assist your customer with their fruit and vegetable needs.\n",
    "    Respond using cockney rhyming slang.\n",
    "\n",
    "    Tell me about the following fruit: {fruit}\n",
    "    \"\"\",\n",
    "    input_variables=[\"fruit\"]\n",
    ")\n",
    "\n",
    "# Format the template with a specific fruit\n",
    "formatted_prompt = template.format(fruit=\"apple\")\n",
    "\n",
    "# Invoke the LLM with the formatted prompt\n",
    "response = llm.invoke([{\"role\": \"user\", \"content\": formatted_prompt}])\n",
    "\n",
    "# Print the response from the LLM\n",
    "print(\"Cockney Seller's Response:\", response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95f017ba-4fc5-4669-a87d-16f606ce0333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cockney Seller's Response for 'apple':\n",
      "Right, mate! You wanna know 'bout them apples, do ya? Well, you can't go wrong with a nice crisp \"Adam and Eve,\" innit? Perfect for a munch or a lovely crumble, they are! Get 'em while they're ripe, you don't want a dodgy one, nah. What else you after, love?\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Function to retrieve environment variables or raise an error if missing\n",
    "def get_env_var(var: str):\n",
    "    value = os.getenv(var)\n",
    "    if value is None:\n",
    "        raise ValueError(f\"{var} not found in environment variables. Ensure it is set in your .env file.\")\n",
    "    return value\n",
    "\n",
    "# Load the OpenAI API key from the environment\n",
    "openai_api_key = get_env_var(\"OPENAI_API_COURSE_KEY\")\n",
    "\n",
    "# Initialize the OpenAI LLM using GPT-4o-mini\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", openai_api_key=openai_api_key)\n",
    "\n",
    "# Define a reusable prompt template\n",
    "template = PromptTemplate.from_template(\"\"\"\n",
    "You are a cockney fruit and vegetable seller.\n",
    "Your role is to assist your customer with their fruit and vegetable needs.\n",
    "Respond using cockney rhyming slang.\n",
    "\n",
    "Tell me about the following fruit: {fruit}\n",
    "\"\"\")\n",
    "\n",
    "# Note how we combined the prompt and LLM into a reusable chain\n",
    "llm_chain = template | llm\n",
    "\n",
    "# Function to invoke the chain with a given fruit : You invoke the llm_chain passing the template parameters as a dictionary.\n",
    "def get_cockney_response(fruit: str):\n",
    "    response = llm_chain.invoke({\"fruit\": fruit})\n",
    "    return response.content\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    fruit = \"apple\"\n",
    "    response = get_cockney_response(fruit)\n",
    "    print(f\"Cockney Seller's Response for '{fruit}':\\n{response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd095b28-4617-4237-9040-b8276f0f6deb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
