{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b669b815-cf4b-4061-a20e-13a64110ba13",
   "metadata": {},
   "source": [
    "# Set up your environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3deedde1-147e-41de-a6b1-33ba720e7774",
   "metadata": {},
   "source": [
    "Let's use a `.env` file and the `dotenv` library. This allows you to keep your API key secure and separated from your code.\n",
    "\n",
    "Here’s how you can setup to load the OpenAI API key using a `.env` file:\n",
    "\n",
    "### Step 1: Install Dependencies\n",
    "Make sure you have `python-dotenv` installed if you're using `.env` files:\n",
    "```bash\n",
    "pip install python-dotenv\n",
    "pip install -U langsmith\n",
    "pip install openai\n",
    "pip install langchain_openai\n",
    "```\n",
    "\n",
    "### Step 2: Create a `.env` File\n",
    "In your project directory, create a `.env` file with the following content:\n",
    "```\n",
    "OPENAI_API_KEY=your-api-key-here\n",
    "```\n",
    "\n",
    "### Step 3: Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "149817a3-07b4-49ee-b0be-1e445041e62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install openai --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a17abb2-21df-4045-ae49-098a5aee17f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain_openai --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94a134a4-878c-4270-a8b1-ca214909e237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Function to load environment variables or raise an error if not found\n",
    "def get_env_var(var: str):\n",
    "    value = os.getenv(var)\n",
    "    if value is None:\n",
    "        raise ValueError(f\"{var} not found in environment variables. Make sure it is set in your .env file.\")\n",
    "    return value\n",
    "\n",
    "# Load API keys from the environment\n",
    "langchain_api_key = get_env_var(\"LANGCHAIN_API_KEY\")\n",
    "langchain_tracing_v2 = get_env_var(\"LANGCHAIN_TRACING_V2\")\n",
    "openai_api_key = get_env_var(\"OPENAI_API_KEY\")\n",
    "tavily_api_key = get_env_var(\"TAVILY_API_KEY\")\n",
    "\n",
    "# Now, you can use the keys in our setup as needed\n",
    "from langchain_openai import ChatOpenAI\n",
    "gpt4o_chat = ChatOpenAI(model=\"gpt-4o\", temperature=0, openai_api_key=openai_api_key)\n",
    "gpt35_chat = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0, openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8aaa1a2c-ea45-4552-9075-198123903ce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 11, 'total_tokens': 20, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_9faba9f038', 'finish_reason': 'stop', 'logprobs': None}, id='run-c481a0f4-0c41-418b-990b-1c8b1f39fe52-0', usage_metadata={'input_tokens': 11, 'output_tokens': 9, 'total_tokens': 20, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Create a message\n",
    "msg = HumanMessage(content=\"Hello world\", name=\"Joseph\")\n",
    "\n",
    "# Message list\n",
    "messages = [msg]\n",
    "\n",
    "# Invoke the model with a list of messages \n",
    "gpt4o_chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1acfc41-e0cc-404a-8c4e-c107b18884bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 9, 'total_tokens': 18, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_9faba9f038', 'finish_reason': 'stop', 'logprobs': None}, id='run-ec6d50a0-e1f6-49b3-8ce0-fcf45817c745-0', usage_metadata={'input_tokens': 9, 'output_tokens': 9, 'total_tokens': 18, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt4o_chat.invoke(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93bcdd11-9a59-4e39-8daf-cdff98ec116c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 9, 'total_tokens': 18, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-fe164123-f4c9-4a75-a9e4-d421da088832-0', usage_metadata={'input_tokens': 9, 'output_tokens': 9, 'total_tokens': 18, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt35_chat.invoke(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7b20566-6fdf-4f6b-8700-21607f395f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install langchain_community --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf97e30f-a1ab-41c1-80f2-c5fd729a495d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Tavily using the API key from the environment\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "tavily_search = TavilySearchResults(max_results=3, api_key=tavily_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a31011e-cffb-4c8d-8482-b0b70d94771f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://www.datacamp.com/tutorial/langgraph-tutorial',\n",
       "  'content': 'LangGraph is a library within the LangChain ecosystem designed to tackle these challenges head-on. LangGraph provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured manner.'},\n",
       " {'url': 'https://towardsdatascience.com/from-basics-to-advanced-exploring-langgraph-e8c1cf4db787',\n",
       "  'content': \"LangGraph is a low-level framework that offers extensive customisation options, allowing you to build precisely what you need. Since LangGraph is built on top of LangChain, it's seamlessly integrated into its ecosystem, making it easy to leverage existing tools and components. However, there are areas where LangGrpah could be improved:\"},\n",
       " {'url': 'https://langchain-ai.github.io/langgraph/',\n",
       "  'content': 'Overview¶. LangGraph is a library for building stateful, multi-actor applications with LLMs, used to create agent and multi-agent workflows. Compared to other LLM frameworks, it offers these core benefits: cycles, controllability, and persistence. LangGraph allows you to define flows that involve cycles, essential for most agentic architectures, differentiating it from DAG-based solutions.'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform a search with Tavily\n",
    "search_docs = tavily_search.invoke(\"What is LangGraph?\")\n",
    "search_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9677a7af-8d33-4754-97ef-3bcc4c4a3f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain tracing started successfully.\n",
      "Agent's Response:\n",
      "content='Transformers have revolutionized natural language processing (NLP) by significantly improving the performance of various NLP tasks. Some of the key benefits of using transformers in NLP include:\\n\\n1. Enhanced performance: Transformers have shown superior performance compared to traditional NLP models, especially in tasks such as language translation, text generation, and sentiment analysis.\\n\\n2. Attention mechanism: Transformers use an attention mechanism that allows the model to focus on different parts of the input sequence, enabling better understanding and capturing of long-range dependencies in the text.\\n\\n3. Scalability: Transformers are highly scalable and can be easily adapted to different languages and tasks without the need for extensive modifications to the model architecture.\\n\\n4. Transfer learning: Transformers support transfer learning, allowing pre-trained models to be fine-tuned on specific tasks with limited data, leading to improved performance and faster deployment of NLP applications.\\n\\n5. Interpretability: Transformers provide better interpretability compared to traditional NLP models, making it easier to understand how the model makes predictions and identify potential biases or errors.\\n\\nOverall, transformers have become a cornerstone in modern NLP research and applications, offering significant advantages in terms of performance, scalability, and interpretability.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 236, 'prompt_tokens': 33, 'total_tokens': 269, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-c6f01b38-f4c2-4b4d-a929-329a28ac344a-0' usage_metadata={'input_tokens': 33, 'output_tokens': 236, 'total_tokens': 269, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "Follow-up Response:\n",
      "content='Certainly! GPT (Generative Pre-trained Transformer) and BERT (Bidirectional Encoder Representations from Transformers) are both popular models in the field of natural language processing (NLP). \\n\\nThe main difference between GPT and BERT lies in their architecture and training objectives. GPT is a generative model that is trained to predict the next word in a sequence of text, while BERT is a discriminative model that is trained to understand the context of a word in a sentence by considering both the left and right context.\\n\\nIn simpler terms, GPT is better at generating coherent and contextually relevant text, while BERT is better at understanding the meaning of individual words in a sentence. Both models have their own strengths and weaknesses, and the choice between them depends on the specific task at hand in NLP.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 165, 'prompt_tokens': 33, 'total_tokens': 198, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-7a775f20-a213-4e80-8ea6-51a55717c1c3-0' usage_metadata={'input_tokens': 33, 'output_tokens': 165, 'total_tokens': 198, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "\n",
      "Total Tokens Used: 467\n",
      "Total Cost (USD): $0.0006345000000000001\n",
      "LangChain tracing ended successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Function to load environment variables or raise an error if not found\n",
    "def get_env_var(var: str):\n",
    "    value = os.getenv(var)\n",
    "    if value is None:\n",
    "        raise ValueError(f\"{var} not found in environment variables. Make sure it is set in your .env file.\")\n",
    "    return value\n",
    "\n",
    "# Load API keys from the environment\n",
    "openai_api_key = get_env_var(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Initialize the OpenAI LLM via LangChain\n",
    "gpt_chat = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "# Define a simple prompt template for conversation\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"user_input\"],\n",
    "    template=\"You are a helpful assistant. User asks: {user_input}. How would you respond?\"\n",
    ")\n",
    "\n",
    "# Instead of using RunnableSequence, directly chain the prompt and model using the | operator\n",
    "conversation_chain = prompt_template | gpt_chat\n",
    "\n",
    "try:\n",
    "    # Start callback for tracing OpenAI usage\n",
    "    with get_openai_callback() as cb:\n",
    "        print(\"LangChain tracing started successfully.\")\n",
    "\n",
    "        # Example conversation - LangChain will trace this operation\n",
    "        user_input = \"What are the benefits of using transformers in natural language processing?\"\n",
    "        response = conversation_chain.invoke({\"user_input\": user_input})\n",
    "\n",
    "        print(\"Agent's Response:\")\n",
    "        print(response)\n",
    "\n",
    "        # You can extend this conversation further\n",
    "        follow_up = \"Can you explain the difference between GPT and BERT?\"\n",
    "        follow_up_response = conversation_chain.invoke({\"user_input\": follow_up})\n",
    "\n",
    "        print(\"Follow-up Response:\")\n",
    "        print(follow_up_response)\n",
    "\n",
    "        # Output callback information\n",
    "        print(f\"\\nTotal Tokens Used: {cb.total_tokens}\")\n",
    "        print(f\"Total Cost (USD): ${cb.total_cost}\")\n",
    "\n",
    "    print(\"LangChain tracing ended successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error with LangChain tracing: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7eb07e-c9f5-4945-9163-405db7cdaa39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
