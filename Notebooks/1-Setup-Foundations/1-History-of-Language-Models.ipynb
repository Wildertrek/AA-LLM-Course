{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "626f25d2-921f-400b-9698-d374c0195a5b",
   "metadata": {},
   "source": [
    "# History of Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27db47ef-df20-4d64-8c68-b13db2014f1c",
   "metadata": {},
   "source": [
    "## Model Timeline\n",
    "\n",
    "- [1954 Bag of Words (BOW)](https://en.wikipedia.org/wiki/Bag-of-words_model)\n",
    " The Bag of Words model was a basic approach that tallied word occurrences in manuscripts. Despite its simplicity, it could not consider word order or context.\n",
    "- Markov Chains? 1960s?\n",
    "- [1972 TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n",
    " TF-IDF expanded on BOW by giving more weight to rare words and less to common terms, improving the model’s ability to detect document relevancy. Nonetheless, it made no mention of word context.\n",
    "- [2013 Word2Vec](https://arxiv.org/abs/1301.3781)\n",
    " Word embeddings are high-dimensional vectors encapsulating semantic associations, as described by Word2Vec. This was a substantial advancement in capturing textual semantics.\n",
    "- [2014 RNNs in Encoder-Decoder architectures](https://en.wikipedia.org/wiki/Recurrent_neural_network)\n",
    " RNNs were a significant advancement, capable of computing document embeddings and adding word context. They grew to include LSTM (1997) for long-term dependencies and Bidirectional RNN (1997) for context understanding. Encoder-Decoder RNNs (2014) improved on this method.\n",
    "- [2017 Transformer](https://arxiv.org/abs/1706.03762)\n",
    " The transformer, with its attention mechanisms, greatly improved embedding computation and alignment between input and output, revolutionizing NLP tasks.\n",
    "- [2018 BERT](https://arxiv.org/abs/1810.04805)\n",
    " BERT, a bidirectional transformer, achieved impressive NLP results using global attention and combined training objectives.\n",
    "- [2018 GPT](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)\n",
    " The transformer architecture was used to create the first autoregressive model, GPT. It then evolved into [GPT-2 2019](), a larger and more optimized version of GPT pre-trained on WebText, and [GPT-3 2020](), a larger and more optimized version of GPT-2 pre-trained on Common Crawl.\n",
    "- [2019 CTRL](https://arxiv.org/abs/1909.05858)\n",
    " CTRL, similar to GPT, introduced control codes enabling conditional text generation. This feature enhanced control over the content and style of the generated text.\n",
    "- [2019 Transformer-XL](https://arxiv.org/abs/1901.02860)\n",
    " Transformer-XL innovated by reusing previously computed hidden states, allowing the model to maintain a longer contextual memory. This enhancement significantly improved the model’s ability to handle extended text sequences.\n",
    "- [2019 ALBERT](https://arxiv.org/abs/1909.11942)\n",
    " ALBERT offered a more efficient version of BERT by implementing Sentence Order Prediction instead of Next Sentence Prediction and employing parameter-reduction techniques. These changes resulted in lower memory usage and expedited training.\n",
    "- [2019 RoBERTa](https://arxiv.org/abs/1907.11692)\n",
    " RoBERTa improved upon BERT by introducing dynamic Masked Language Modeling, omitting the Next Sentence Prediction, using the BPE tokenizer, and employing better hyperparameters for enhanced performance.\n",
    "- [2019 XLM](https://arxiv.org/abs/1901.07291)\n",
    " XLM was a multilingual transformer, pre-trained using a variety of objectives, including Causal Language Modeling, Masked Language Modeling, and Translation Language Modeling, catering to multilingual NLP tasks.\n",
    "- [2019 XLNet](https://arxiv.org/abs/1906.08237)\n",
    " XLNet combined the strengths of Transformer-XL with a generalized autoregressive pretraining approach, enabling the learning of bidirectional dependencies and offering improved performance over traditional unidirectional models.\n",
    "- [2019 PEGASUS](https://arxiv.org/abs/1912.08777)\n",
    "PEGASUS featured a bidirectional encoder and a left-to-right decoder, pre-trained using objectives like Masked Language Modeling and Gap Sentence Generation, optimizing it for summarization tasks.\n",
    "- [2019 DistilBERT](https://arxiv.org/abs/1910.01108)\n",
    " DistilBERT presented a smaller, faster version of BERT, retaining over 95% of its performance. This model was trained using distillation techniques to compress the pre-trained BERT model.\n",
    "- [2019 XLM-RoBERTa](https://arxiv.org/pdf/1911.02116.pdf)\n",
    " XLM-RoBERTa was a multilingual adaptation of RoBERTa, trained on a diverse multilanguage corpus, primarily using the Masked Language Modeling objective, enhancing its multilingual capabilities.\n",
    "- [2019 BART](https://arxiv.org/abs/1910.13461)\n",
    " BART, with a bidirectional encoder and a left-to-right decoder, was trained by intentionally corrupting text and then learning to reconstruct the original, making it practical for a range of generation and comprehension tasks.\n",
    "- [2019 ConvBERT](https://arxiv.org/abs/2008.02496)\n",
    " ConvBERT innovated by replacing traditional self-attention blocks with modules incorporating convolutions, allowing for more effective handling of global and local contexts within the text.\n",
    "- [2020 Funnel Transformer](https://arxiv.org/abs/2006.03236)\n",
    " Funnel Transformer innovated by progressively compressing the sequence of hidden states into a shorter sequence, effectively reducing computational costs while maintaining performance.\n",
    "- [2020 Reformer](https://arxiv.org/abs/2001.04451)\n",
    " Reformer offered a more efficient version of the transformer. It utilized locality-sensitive hashing for attention mechanisms and axial position encoding, among other optimizations, to enhance efficiency.\n",
    "- [2020 T5](https://arxiv.org/abs/1910.10683)\n",
    " T5 approached NLP tasks as a text-to-text problem. It was trained using a mixture of unsupervised and supervised tasks, making it versatile for various applications.\n",
    "- [2020 Longformer](https://arxiv.org/abs/2004.05150)\n",
    " Longformer adapted the transformer architecture for longer documents. It replaced traditional attention matrices with sparse versions, improving training efficiency and better handling of longer texts.\n",
    "- [2020 ProphetNet](https://arxiv.org/abs/2001.04063)\n",
    " ProphetNet was trained using a Future N-gram Prediction objective, incorporating a unique self-attention mechanism. This model aimed to improve sequence-to-sequence tasks like summarization and question-answering.\n",
    "- [2020 ELECTRA](https://arxiv.org/abs/2003.10555)\n",
    " ELECTRA presented a novel approach, trained with a Replaced Token Detection objective. It offered improvements over BERT in efficiency and performance across various NLP tasks.\n",
    "- [2021 Switch Transformers](https://arxiv.org/abs/2101.03961)\n",
    " Switch Transformers introduced a sparsely-activated expert model, a new spin on the Mixture of Experts (MoE) approach. This design allowed the model to manage a broader array of tasks more efficiently, marking a significant step towards scaling up transformer models.\n",
    "\n",
    "Update to LLM Production Book:\n",
    "\n",
    "- [2021 LaMDA (Language Model for Dialogue Applications)](https://arxiv.org/abs/2201.08239) introduced by Google, focuses on generating more natural, open-ended conversations. It leverages fine-tuning techniques to produce more contextually accurate and engaging responses.\n",
    "- [2021 GLaM (Generalist Language Model)](https://arxiv.org/abs/2112.06905) by Google, uses a mixture of experts approach similar to Switch Transformers. It offers high performance with fewer computational resources by activating only a subset of the model's parameters for each input.\n",
    "- [2021 DALL-E](https://arxiv.org/abs/2102.12092) by OpenAI, a model capable of generating images from textual descriptions, showcasing the extension of transformer-based models beyond text to multimodal applications.\n",
    "- [2022 PaLM (Pathways Language Model)](https://arxiv.org/abs/2204.02311) by Google, a large-scale language model designed using the Pathways system to facilitate scaling across multiple tasks, improving efficiency and performance.\n",
    "- [2022 Chinchilla](https://arxiv.org/abs/2203.15556) by DeepMind, emphasizes the importance of data efficiency in training large models, showing that training smaller models on more data can outperform larger models trained on less data.\n",
    "- [2022 OPT (Open Pretrained Transformer)](https://arxiv.org/abs/2205.01068) by Meta AI, an open-access large-scale model designed to make research in language modeling more accessible, providing transparency in large-scale language model training.\n",
    "- [2022 BLOOM (BigScience Large Open-science Open-access Multilingual Language Model)](https://arxiv.org/abs/2211.05100), a multilingual LLM developed by the BigScience project, with a focus on inclusivity and ethical considerations in AI development.\n",
    "- [2023 GPT-4](https://cdn.openai.com/papers/gpt-4.pdf) A successor to GPT-3, with improved capabilities in understanding and generating complex text, as well as integrating multimodal input. It leverages advancements in architecture and training to enhance performance across diverse NLP tasks.\n",
    "- [2023 Claude](https://arxiv.org/abs/2301.04107) by Anthropic, an AI assistant designed with safety and alignment at its core, featuring improvements in interpretability and ethical considerations in response generation.\n",
    "\n",
    "## Recap\n",
    "The advancements in natural language processing, beginning with the essential Bag of Words model, led us to the advanced and highly sophisticated transformer-based models we have today. Large language models (LLMs) are powerful architectures trained on massive amounts of text data that can comprehend and generate writing that nearly resembles human language. Built on transformer designs, they excel at capturing long-term dependencies in language and producing text via an auto-regressive process.\n",
    "\n",
    "The years 2020 and 2021 were key moments in the advancement of Large Language Models (LLMs). Before this, language models’ primary goal was to generate coherent and contextually suitable messages. However, advances in LLMs throughout these years resulted in a paradigm shift.\n",
    "\n",
    "The journey from pre-trained language models to Large Language Models (LLMs) is marked by distinctive features of LLMs, such as the impact of scaling laws and the emergence of abilities like in-context learning, step-by-step reasoning techniques, and instruction following. These emergent abilities are central to the success of LLMs, showcased in scenarios like few-shots and augmented prompting. However, scaling also brings challenges like bias and toxicity, necessitating careful consideration.\n",
    "\n",
    "Emergent abilities in LLMs have shifted the focus towards general-purpose models, opening up new applications beyond traditional NLP research. The expansion of context windows also played a key role in this shift. Innovations like FlashAttention-2, which optimizes the attention layer’s speed and memory utilization, and LongNet, which introduced the “dilated attention” method, have paved the way for context windows to potentially grow to 1 billion tokens.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50838c62-a261-40c6-ade5-b8e58f97e350",
   "metadata": {},
   "source": [
    "## Introduction to LLMs\n",
    "\n",
    "### What Are Large Language Models?\n",
    "\n",
    "Large Language Models (LLMs) are advanced neural networks that have revolutionized the field of natural language processing (NLP). These models are defined by their vast number of parameters, often reaching into the billions, which allow them to excel at processing and generating text. By training on extensive datasets, LLMs learn to recognize various language patterns and structures. Their primary purpose is to interpret and produce human-like text, capturing the complexities of natural language. This includes understanding syntax (the arrangement of words) through tokenization and semantics (the meaning of words).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
