{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b669b815-cf4b-4061-a20e-13a64110ba13",
   "metadata": {},
   "source": [
    "# Set up your environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3deedde1-147e-41de-a6b1-33ba720e7774",
   "metadata": {},
   "source": [
    "Let's use a `.env` file and the `dotenv` library. This allows you to keep your API key secure and separated from your code.\n",
    "\n",
    "Here’s how you can setup to load the OpenAI API key using a `.env` file:\n",
    "\n",
    "### Step 1: Install Dependencies\n",
    "Make sure you have `python-dotenv` installed if you're using `.env` files:\n",
    "```bash\n",
    "pip install python-dotenv\n",
    "pip install -U langsmith\n",
    "pip install openai\n",
    "pip install langchain_openai\n",
    "```\n",
    "\n",
    "### Step 2: Create a `.env` File\n",
    "In your project directory, create a `.env` file with the following content:\n",
    "```\n",
    "OPENAI_API_KEY=your-api-key-here\n",
    "```\n",
    "\n",
    "### Step 3: Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "179b63b9-3a4b-4761-a52f-bd1000e009a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/jupyterlab/4.3.5/libexec/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d035e705-40f1-4c66-9387-b3c6e2e1fece",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langgraph openai langchain_openai google-generativeai langchain_anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf3c0d3-1e89-42ea-be9a-cb900d07afbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#updated\n",
    "\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "from openai import OpenAI  # ✅ Grok now uses OpenAI-compatible SDK\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from anthropic import Anthropic\n",
    "from langchain_core.messages import HumanMessage  \n",
    "from IPython.display import display, Markdown  \n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Function to load API keys safely\n",
    "def get_env_var(var: str):\n",
    "    value = os.getenv(var)\n",
    "    if value is None:\n",
    "        raise ValueError(f\"{var} not found in environment variables. Make sure it is set in your .env file.\")\n",
    "    return value\n",
    "\n",
    "# Load API keys\n",
    "openai_api_key = get_env_var(\"OPENAI_API_COURSE_KEY\")  \n",
    "gemini_api_key = get_env_var(\"GEMINI_API_KEY\")  \n",
    "anthropic_api_key = get_env_var(\"ANTHROPIC_API_KEY\")  \n",
    "xai_api_key = get_env_var(\"XAI_API_KEY\")  # ✅ Updated for Grok\n",
    "\n",
    "# Initialize OpenAI GPT models\n",
    "gpt4o_chat = ChatOpenAI(model=\"gpt-4o\", temperature=0, openai_api_key=openai_api_key)\n",
    "gpt4o_mini_chat = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "# Initialize Claude model\n",
    "claude_chat = ChatAnthropic(model=\"claude-3-5-sonnet-20241022\", temperature=0, anthropic_api_key=anthropic_api_key)\n",
    "\n",
    "# Initialize Google Gemini API Client\n",
    "genai.configure(api_key=gemini_api_key)  \n",
    "gemini_model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "\n",
    "# Initialize Grok (XAI API) Client\n",
    "grok_client = OpenAI(api_key=xai_api_key, base_url=\"https://api.x.ai/v1\")  # ✅ New Grok client\n",
    "\n",
    "# Create outputs directory if it doesn't exist\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "# Function to generate a safe filename from query text\n",
    "def sanitize_filename(query: str):\n",
    "    return re.sub(r'[^\\w\\s-]', '', query).strip().replace(' ', '_')[:50] + \".md\"\n",
    "\n",
    "# ======================================\n",
    "# Grok API Integration (Updated)\n",
    "# ======================================\n",
    "\n",
    "def query_grok(prompt: str):\n",
    "    \"\"\"\n",
    "    Query the latest Grok model using OpenAI-compatible SDK.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        completion = grok_client.chat.completions.create(\n",
    "            model=\"grok-2-latest\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are Grok, a chatbot inspired by the Hitchhiker's Guide to the Galaxy.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "        )\n",
    "        return completion.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error querying Grok: {e}\"\n",
    "\n",
    "\n",
    "# ======================================\n",
    "# Unified Response Comparison and Saving\n",
    "# ======================================\n",
    "\n",
    "def format_response(response: str) -> str:\n",
    "    \"\"\"\n",
    "    Standardize the formatting of the model's response for consistent readability.\n",
    "    \"\"\"\n",
    "    sections = response.split(\"\\n\")\n",
    "    formatted_response = []\n",
    "    for section in sections:\n",
    "        if section.strip().endswith(\":\"):\n",
    "            formatted_response.append(f\"\\n### {section.strip()}\")  # Markdown headers\n",
    "        else:\n",
    "            formatted_response.append(section.strip())\n",
    "    return \"\\n\".join(formatted_response)\n",
    "\n",
    "def save_comparison_to_markdown(prompt: str, results: dict, filename: str):\n",
    "    \"\"\"\n",
    "    Save the formatted output of model comparisons to a Markdown file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"# Prompt:\\n\\n{prompt}\\n\\n\")\n",
    "            f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "            \n",
    "            for model, response in results.items():\n",
    "                f.write(f\"## {model} Response\\n\\n\")\n",
    "                f.write(f\"{response}\\n\\n\")\n",
    "                f.write(\"-\" * 80 + \"\\n\\n\")\n",
    "        \n",
    "        print(f\"✅ Output saved to: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error saving to {filename}: {e}\")\n",
    "\n",
    "def compare_responses(prompt: str, save_dir=\"outputs\"):\n",
    "    \"\"\"\n",
    "    Compare responses from GPT-4o, GPT-4o-mini, Claude-3.5, Gemini-2.0, and Grok models for the same prompt.\n",
    "    Save to a Markdown file and display in Jupyter Notebook.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    # GPT-4o\n",
    "    gpt4_response = gpt4o_chat.invoke([HumanMessage(content=prompt)]).content\n",
    "    results[\"GPT-4o\"] = format_response(gpt4_response)\n",
    "\n",
    "    # GPT-4o-mini\n",
    "    gpt4o_mini_response = gpt4o_mini_chat.invoke([HumanMessage(content=prompt)]).content\n",
    "    results[\"GPT-4o-mini\"] = format_response(gpt4o_mini_response)\n",
    "\n",
    "    # Claude-3.5\n",
    "    claude_response = claude_chat.invoke(prompt).content\n",
    "    results[\"Claude-3.5-Sonnet\"] = format_response(claude_response)\n",
    "\n",
    "    # Gemini-2.0 Flash\n",
    "    gemini_response = gemini_model.generate_content(prompt).text\n",
    "    results[\"Gemini-2.0-Flash\"] = format_response(gemini_response)\n",
    "\n",
    "    # Grok-2-Latest\n",
    "    grok_response = query_grok(prompt)\n",
    "    results[\"Grok-2-Latest\"] = format_response(grok_response)\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Generate the output filename based on the query\n",
    "    output_filename = os.path.join(save_dir, sanitize_filename(prompt))\n",
    "\n",
    "    # Save comparison results\n",
    "    save_comparison_to_markdown(prompt, results, output_filename)\n",
    "\n",
    "    # Display formatted results in Jupyter Notebook\n",
    "    output_markdown = f\"\"\"# Query: {prompt}\n",
    "\n",
    "## GPT-4o Response:\n",
    "{results[\"GPT-4o\"]}\n",
    "\n",
    "## GPT-4o-mini Response:\n",
    "{results[\"GPT-4o-mini\"]}\n",
    "\n",
    "## Claude-3.5-Sonnet Response:\n",
    "{results[\"Claude-3.5-Sonnet\"]}\n",
    "\n",
    "## Gemini-2.0-Flash Response:\n",
    "{results[\"Gemini-2.0-Flash\"]}\n",
    "\n",
    "## Grok-2-Latest Response:\n",
    "{results[\"Grok-2-Latest\"]}\n",
    "\"\"\"\n",
    "\n",
    "    display(Markdown(output_markdown))\n",
    "\n",
    "# ======================================\n",
    "# Run the Comparison\n",
    "# ======================================\n",
    "\n",
    "query_text = \"Explain how AI works\"\n",
    "compare_responses(query_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8aaa1a2c-ea45-4552-9075-198123903ce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 11, 'total_tokens': 21, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f9f4fb6dbf', 'finish_reason': 'stop', 'logprobs': None}, id='run-85a7b956-c5ee-4dda-ab26-148b18160a46-0', usage_metadata={'input_tokens': 11, 'output_tokens': 10, 'total_tokens': 21, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Create a message\n",
    "msg = HumanMessage(content=\"Hello world\", name=\"Joseph\")\n",
    "\n",
    "# Message list\n",
    "messages = [msg]\n",
    "\n",
    "# Invoke the model with a list of messages \n",
    "gpt4o_chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1acfc41-e0cc-404a-8c4e-c107b18884bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 9, 'total_tokens': 19, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_523b9b6e5f', 'finish_reason': 'stop', 'logprobs': None}, id='run-72a0f82e-1d65-449d-8a24-870b30465c04-0', usage_metadata={'input_tokens': 9, 'output_tokens': 10, 'total_tokens': 19, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt4o_chat.invoke(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93bcdd11-9a59-4e39-8daf-cdff98ec116c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 9, 'total_tokens': 19, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_13eed4fce1', 'finish_reason': 'stop', 'logprobs': None}, id='run-b9599e15-53a0-4aaf-a71d-53a715e25807-0', usage_metadata={'input_tokens': 9, 'output_tokens': 10, 'total_tokens': 19, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt4o_mini_chat.invoke(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf97e30f-a1ab-41c1-80f2-c5fd729a495d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Tavily using the API key from the environment\n",
    "tavily_api_key = get_env_var(\"TAVILY_API_KEY\")\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "tavily_search = TavilySearchResults(max_results=3, api_key=tavily_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a31011e-cffb-4c8d-8482-b0b70d94771f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://www.datacamp.com/tutorial/langgraph-tutorial',\n",
       "  'content': 'LangGraph is a library within the LangChain ecosystem that provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured and efficient manner. By managing the flow of data and the sequence of operations, LangGraph allows developers to focus on the high-level logic of their applications rather than the intricacies of agent coordination. Whether you need a chatbot that can handle various types of user requests or a multi-agent system that performs complex tasks, LangGraph provides the tools to build exactly what you need. LangGraph significantly simplifies the development of complex LLM applications by providing a structured framework for managing state and coordinating agent interactions.'},\n",
       " {'url': 'https://cobusgreyling.medium.com/langgraph-from-langchain-explained-in-simple-terms-f7cd0c12cdbf',\n",
       "  'content': 'LangGraph is a method for creating state machines for conversational flow by defining them as graphs & it’s easier to understand than you might think. Prompt chaining can be described as a technique used in working with language models, where multiple prompts (nodes) are sequentially linked (via edges) together to guide the generative app through a series of related tasks or steps. LangGraph is a module built on top of LangChain to better enable creation of cyclical graphs, often needed for agent runtimes. from langgraph.graph.message import add_messages   Intro to LangGraph - LangGraph ------------------------------ ### Build language agents as graphs langchain-ai.github.io'},\n",
       " {'url': 'https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141',\n",
       "  'content': 'In this article, we’ll introduce LangGraph, walk you through its basic concepts, and share some insights and common points of confusion for beginners. While LangChain allows you to define chains of computation (Directed Acyclic Graphs or DAGs), LangGraph introduces the ability to add cycles, enabling more complex, agent-like behaviors where you can call an LLM in a loop, asking it what action to take next. Step 1: Define the Graph State First, we define the state structure for our graph. Step 4: Add Nodes to the Graph LangGraph is a versatile tool for building complex, stateful applications with LLMs. By understanding its core concepts and working through simple examples, beginners can start to leverage its power for their projects.'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform a search with Tavily\n",
    "search_docs = tavily_search.invoke(\"What is LangGraph?\")\n",
    "search_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9677a7af-8d33-4754-97ef-3bcc4c4a3f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain tracing started successfully.\n",
      "Agent's Response:\n",
      "content='Transformers have revolutionized natural language processing (NLP) due to several key benefits:\\n\\n1. **Parallelization**: Unlike traditional recurrent neural networks (RNNs), which process sequences sequentially, transformers allow for parallel processing of input data. This significantly speeds up training and inference times, making it feasible to work with large datasets.\\n\\n2. **Long-Range Dependencies**: Transformers use self-attention mechanisms that enable them to capture relationships between words regardless of their distance in the text. This is particularly beneficial for understanding context and meaning in long sentences or documents.\\n\\n3. **Scalability**: Transformers can be scaled up easily by increasing the number of layers and parameters, leading to improved performance on various NLP tasks. This scalability has led to the development of very large models, such as BERT and GPT, which achieve state-of-the-art results.\\n\\n4. **Transfer Learning**: Pre-trained transformer models can be fine-tuned on specific tasks with relatively small amounts of labeled data. This transfer learning capability allows for efficient use of resources and time, as models can leverage knowledge gained from large datasets.\\n\\n5. **Flexibility**: Transformers can be adapted for a wide range of NLP tasks, including text classification, translation, summarization, and question answering. Their architecture is versatile enough to handle different types of input and output formats.\\n\\n6. **Contextualized Representations**: Transformers generate contextual embeddings for words, meaning that the representation of a word can change based on its context in a sentence. This leads to a better understanding of nuances in language, such as polysemy and idiomatic expressions.\\n\\n7. **State-of-the-Art Performance**: Many transformer-based models have set new benchmarks on various NLP tasks, demonstrating their effectiveness and reliability in real-world applications.\\n\\nOverall, the introduction of transformers has led to significant advancements in NLP, enabling more accurate and efficient processing of natural language.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 383, 'prompt_tokens': 33, 'total_tokens': 416, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_13eed4fce1', 'finish_reason': 'stop', 'logprobs': None} id='run-40712bb5-ca15-46b1-8593-cf19d2767b01-0' usage_metadata={'input_tokens': 33, 'output_tokens': 383, 'total_tokens': 416, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "Follow-up Response:\n",
      "content='Certainly! GPT (Generative Pre-trained Transformer) and BERT (Bidirectional Encoder Representations from Transformers) are both models based on the Transformer architecture, but they are designed for different purposes and have distinct characteristics.\\n\\n### Key Differences:\\n\\n1. **Architecture**:\\n   - **GPT**: GPT is a unidirectional model, meaning it processes text in a left-to-right manner. It generates text by predicting the next word in a sequence based on the words that come before it. This makes it particularly well-suited for tasks that involve text generation.\\n   - **BERT**: BERT, on the other hand, is a bidirectional model. It looks at the entire context of a word by considering both the words that come before and after it. This allows BERT to understand the context of words more effectively, making it ideal for tasks that require understanding the meaning of text, such as question answering and sentiment analysis.\\n\\n2. **Training Objective**:\\n   - **GPT**: GPT is trained using a language modeling objective, specifically next-token prediction. It learns to predict the next word in a sentence given the previous words.\\n   - **BERT**: BERT is trained using a masked language modeling objective. During training, some words in the input are masked (hidden), and the model learns to predict these masked words based on the surrounding context. This helps BERT capture deeper contextual relationships.\\n\\n3. **Use Cases**:\\n   - **GPT**: Due to its generative nature, GPT is often used for tasks like text generation, dialogue systems, and creative writing. It excels in generating coherent and contextually relevant text.\\n   - **BERT**: BERT is primarily used for understanding tasks, such as text classification, named entity recognition, and question answering. It is effective in scenarios where understanding the context and relationships between words is crucial.\\n\\n4. **Fine-tuning**:\\n   - Both models can be fine-tuned for specific tasks, but the approach differs. BERT is typically fine-tuned on a specific downstream task with labeled data, while GPT can be fine-tuned for both generation and understanding tasks.\\n\\n### Summary:\\nIn summary, GPT is a unidirectional model focused on text generation, while BERT is a bidirectional model designed for understanding text. Their different training objectives and architectures make them suitable for different types of natural language processing tasks.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 484, 'prompt_tokens': 32, 'total_tokens': 516, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_13eed4fce1', 'finish_reason': 'stop', 'logprobs': None} id='run-71126dcd-8097-44c4-98cb-78b6e005b858-0' usage_metadata={'input_tokens': 32, 'output_tokens': 484, 'total_tokens': 516, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "\n",
      "Total Tokens Used: 932\n",
      "Total Cost (USD): $0.00052995\n",
      "LangChain tracing ended successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Function to load environment variables or raise an error if not found\n",
    "def get_env_var(var: str):\n",
    "    value = os.getenv(var)\n",
    "    if value is None:\n",
    "        raise ValueError(f\"{var} not found in environment variables. Make sure it is set in your .env file.\")\n",
    "    return value\n",
    "\n",
    "# Load API keys from the environment\n",
    "openai_api_key = get_env_var(\"OPENAI_API_COURSE_KEY\")\n",
    "\n",
    "# Initialize the OpenAI LLM via LangChain\n",
    "# gpt_chat = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0, openai_api_key=openai_api_key)\n",
    "gpt4o_mini_chat = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "# Define a simple prompt template for conversation\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"user_input\"],\n",
    "    template=\"You are a helpful assistant. User asks: {user_input}. How would you respond?\"\n",
    ")\n",
    "\n",
    "# Instead of using RunnableSequence, directly chain the prompt and model using the | operator\n",
    "# conversation_chain = prompt_template | gpt_chat\n",
    "conversation_chain = prompt_template | gpt4o_mini_chat\n",
    "\n",
    "try:\n",
    "    # Start callback for tracing OpenAI usage\n",
    "    with get_openai_callback() as cb:\n",
    "        print(\"LangChain tracing started successfully.\")\n",
    "\n",
    "        # Example conversation - LangChain will trace this operation\n",
    "        user_input = \"What are the benefits of using transformers in natural language processing?\"\n",
    "        response = conversation_chain.invoke({\"user_input\": user_input})\n",
    "\n",
    "        print(\"Agent's Response:\")\n",
    "        print(response)\n",
    "\n",
    "        # You can extend this conversation further\n",
    "        follow_up = \"Can you explain the difference between GPT and BERT?\"\n",
    "        follow_up_response = conversation_chain.invoke({\"user_input\": follow_up})\n",
    "\n",
    "        print(\"Follow-up Response:\")\n",
    "        print(follow_up_response)\n",
    "\n",
    "        # Output callback information\n",
    "        print(f\"\\nTotal Tokens Used: {cb.total_tokens}\")\n",
    "        print(f\"Total Cost (USD): ${cb.total_cost}\")\n",
    "\n",
    "    print(\"LangChain tracing ended successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error with LangChain tracing: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ea413de-2c56-4684-9133-519dd94fe26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain tracing started successfully.\n",
      "Agent's Response:\n",
      "content=\"Transformers have revolutionized natural language processing (NLP) due to several key benefits:\\n\\n1. **Parallelization**: Unlike traditional recurrent neural networks (RNNs), which process sequences sequentially, transformers allow for parallel processing of data. This significantly speeds up training times and makes it feasible to work with large datasets.\\n\\n2. **Long-Range Dependencies**: Transformers use self-attention mechanisms that enable them to capture relationships between words regardless of their distance in the text. This is particularly useful for understanding context and meaning in long sentences or documents.\\n\\n3. **Scalability**: Transformers can be scaled up effectively. Larger models, such as BERT and GPT, have shown that increasing the model size and training data can lead to substantial improvements in performance across various NLP tasks.\\n\\n4. **Transfer Learning**: Pre-trained transformer models can be fine-tuned on specific tasks with relatively small amounts of data. This transfer learning capability allows for efficient use of resources and can lead to high performance even in low-data scenarios.\\n\\n5. **Flexibility**: Transformers can be adapted for a wide range of NLP tasks, including text classification, translation, summarization, and question answering. Their architecture is versatile and can be modified to suit different applications.\\n\\n6. **Contextualized Representations**: Transformers generate contextual embeddings for words, meaning that the representation of a word can change based on its context in a sentence. This leads to a better understanding of nuances in language.\\n\\n7. **State-of-the-Art Performance**: Many transformer-based models have set new benchmarks on various NLP tasks, demonstrating their effectiveness and reliability in real-world applications.\\n\\n8. **Rich Attention Mechanisms**: The attention mechanism allows the model to focus on relevant parts of the input when making predictions, which enhances the model's ability to understand and generate human-like text.\\n\\nOverall, the introduction of transformers has led to significant advancements in NLP, making it possible to tackle complex language tasks more effectively than ever before.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 399, 'prompt_tokens': 33, 'total_tokens': 432, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None} id='run-482257e9-28e5-4054-97ca-324f4d2ecc29-0' usage_metadata={'input_tokens': 33, 'output_tokens': 399, 'total_tokens': 432, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "Follow-up Response:\n",
      "content='Certainly! GPT (Generative Pre-trained Transformer) and BERT (Bidirectional Encoder Representations from Transformers) are both models based on the Transformer architecture, but they are designed for different purposes and have distinct characteristics.\\n\\n### Key Differences:\\n\\n1. **Architecture**:\\n   - **GPT**: GPT is a unidirectional model, meaning it processes text in a left-to-right manner. It generates text by predicting the next word in a sequence based on the words that come before it. This makes it particularly well-suited for tasks that involve text generation.\\n   - **BERT**: BERT, on the other hand, is a bidirectional model. It looks at the entire context of a word by considering both the words that come before and after it. This allows BERT to understand the context of words more effectively, making it ideal for tasks that require understanding the meaning of text, such as question answering and sentiment analysis.\\n\\n2. **Training Objective**:\\n   - **GPT**: GPT is trained using a language modeling objective, specifically next-token prediction. It learns to predict the next word in a sentence given the previous words.\\n   - **BERT**: BERT is trained using a masked language modeling objective. During training, some words in the input are masked (hidden), and the model learns to predict these masked words based on the surrounding context. This helps BERT capture deeper contextual relationships.\\n\\n3. **Use Cases**:\\n   - **GPT**: Due to its generative nature, GPT is often used for tasks like text generation, dialogue systems, and creative writing. It excels in generating coherent and contextually relevant text.\\n   - **BERT**: BERT is primarily used for understanding tasks, such as text classification, named entity recognition, and question answering. It is effective in scenarios where understanding the nuances of language is crucial.\\n\\n4. **Fine-tuning**:\\n   - **GPT**: While GPT can be fine-tuned for specific tasks, it is often used in a zero-shot or few-shot manner, where it generates responses based on prompts without extensive task-specific training.\\n   - **BERT**: BERT is typically fine-tuned on specific downstream tasks, allowing it to adapt its learned representations to perform well on those tasks.\\n\\n### Summary:\\nIn summary, GPT is a unidirectional model focused on text generation, while BERT is a bidirectional model designed for understanding and interpreting text. Their different architectures and training objectives make them suitable for different types of natural language processing tasks.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 512, 'prompt_tokens': 32, 'total_tokens': 544, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None} id='run-bfef4051-d5a8-4f07-b135-af72fb55865b-0' usage_metadata={'input_tokens': 32, 'output_tokens': 512, 'total_tokens': 544, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "\n",
      "Total Tokens Used: 976\n",
      "Total Cost (USD): $0.0005563499999999999\n",
      "LangChain tracing ended successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Function to load environment variables or raise an error if not found\n",
    "def get_env_var(var: str):\n",
    "    value = os.getenv(var)\n",
    "    if value is None:\n",
    "        raise ValueError(f\"{var} not found in environment variables. Make sure it is set in your .env file.\")\n",
    "    return value\n",
    "\n",
    "# Load API keys from the environment\n",
    "openai_api_key = get_env_var(\"OPENAI_API_COURSE_KEY\")\n",
    "\n",
    "# Initialize the OpenAI LLM via LangChain with GPT-4o-mini\n",
    "gpt4o_mini_chat = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "# Define a simple prompt template for conversation\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"user_input\"],\n",
    "    template=\"You are a helpful assistant. User asks: {user_input}. How would you respond?\"\n",
    ")\n",
    "\n",
    "# Chain the prompt and model using the | operator\n",
    "conversation_chain = prompt_template | gpt4o_mini_chat\n",
    "\n",
    "try:\n",
    "    # Start callback for tracing OpenAI usage\n",
    "    with get_openai_callback() as cb:\n",
    "        print(\"LangChain tracing started successfully.\")\n",
    "\n",
    "        # Example conversation - LangChain will trace this operation\n",
    "        user_input = \"What are the benefits of using transformers in natural language processing?\"\n",
    "        response = conversation_chain.invoke({\"user_input\": user_input})\n",
    "\n",
    "        print(\"Agent's Response:\")\n",
    "        print(response)\n",
    "\n",
    "        # Extend the conversation further\n",
    "        follow_up = \"Can you explain the difference between GPT and BERT?\"\n",
    "        follow_up_response = conversation_chain.invoke({\"user_input\": follow_up})\n",
    "\n",
    "        print(\"Follow-up Response:\")\n",
    "        print(follow_up_response)\n",
    "\n",
    "        # Output callback information\n",
    "        print(f\"\\nTotal Tokens Used: {cb.total_tokens}\")\n",
    "        print(f\"Total Cost (USD): ${cb.total_cost}\")\n",
    "\n",
    "    print(\"LangChain tracing ended successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error with LangChain tracing: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b7eb07e-c9f5-4945-9163-405db7cdaa39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cockney Seller's Response: Alright, me old china! You wanna know about the apple, eh? Well, it's a right lovely Ruby Murray, all crisp and juicy, great for munchin’ or tossin’ in a pie! Perfect for a bit of a snack or even a cheeky cider, innit? Just don’t go roamin’ too far from your Adam and Eve, or you might miss out on the good stuff! What say you, fancy a few?\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Function to retrieve environment variables\n",
    "def get_env_var(var: str):\n",
    "    value = os.getenv(var)\n",
    "    if value is None:\n",
    "        raise ValueError(f\"{var} not found in environment variables. Ensure it is set in your .env file.\")\n",
    "    return value\n",
    "\n",
    "# Load the OpenAI API key from .env\n",
    "openai_api_key = get_env_var(\"OPENAI_API_COURSE_KEY\")\n",
    "\n",
    "# Initialize the OpenAI model with LangChain\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", openai_api_key=openai_api_key)\n",
    "\n",
    "# Define a custom prompt template\n",
    "template = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    You are a cockney fruit and vegetable seller.\n",
    "    Your role is to assist your customer with their fruit and vegetable needs.\n",
    "    Respond using cockney rhyming slang.\n",
    "\n",
    "    Tell me about the following fruit: {fruit}\n",
    "    \"\"\",\n",
    "    input_variables=[\"fruit\"]\n",
    ")\n",
    "\n",
    "# Format the template with a specific fruit\n",
    "formatted_prompt = template.format(fruit=\"apple\")\n",
    "\n",
    "# Invoke the LLM with the formatted prompt\n",
    "response = llm.invoke([{\"role\": \"user\", \"content\": formatted_prompt}])\n",
    "\n",
    "# Print the response from the LLM\n",
    "print(\"Cockney Seller's Response:\", response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95f017ba-4fc5-4669-a87d-16f606ce0333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cockney Seller's Response for 'apple':\n",
      "Ah, the apple, mate! That’s a right lovely bit of nosh, innit? We call it a \"Adam and Eve,\" see? Crisp and juicy, perfect for a munch or a cheeky pie. Ya fancy a few \"Adam and Eves,\" or are ya lookin' for summat else?\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Function to retrieve environment variables or raise an error if missing\n",
    "def get_env_var(var: str):\n",
    "    value = os.getenv(var)\n",
    "    if value is None:\n",
    "        raise ValueError(f\"{var} not found in environment variables. Ensure it is set in your .env file.\")\n",
    "    return value\n",
    "\n",
    "# Load the OpenAI API key from the environment\n",
    "openai_api_key = get_env_var(\"OPENAI_API_COURSE_KEY\")\n",
    "\n",
    "# Initialize the OpenAI LLM using GPT-4o-mini\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", openai_api_key=openai_api_key)\n",
    "\n",
    "# Define a reusable prompt template\n",
    "template = PromptTemplate.from_template(\"\"\"\n",
    "You are a cockney fruit and vegetable seller.\n",
    "Your role is to assist your customer with their fruit and vegetable needs.\n",
    "Respond using cockney rhyming slang.\n",
    "\n",
    "Tell me about the following fruit: {fruit}\n",
    "\"\"\")\n",
    "\n",
    "# Note how we combined the prompt and LLM into a reusable chain\n",
    "llm_chain = template | llm\n",
    "\n",
    "# Function to invoke the chain with a given fruit : You invoke the llm_chain passing the template parameters as a dictionary.\n",
    "def get_cockney_response(fruit: str):\n",
    "    response = llm_chain.invoke({\"fruit\": fruit})\n",
    "    return response.content\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    fruit = \"apple\"\n",
    "    response = get_cockney_response(fruit)\n",
    "    print(f\"Cockney Seller's Response for '{fruit}':\\n{response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd095b28-4617-4237-9040-b8276f0f6deb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
