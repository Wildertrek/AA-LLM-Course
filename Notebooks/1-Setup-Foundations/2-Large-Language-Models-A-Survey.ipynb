{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5af3f461-8ee2-4d01-8ad5-498043dfe805",
   "metadata": {},
   "source": [
    "# Large Language Models: A Survey\n",
    "_Data: 09 February 2024_\n",
    "\n",
    "_Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu Richard Socher, Xavier Amatriain, Jianfeng Gao_\n",
    "\n",
    "Reference:[Arvix](https://arxiv.org/abs/2402.06196), [PDF](https://arxiv.org/pdf/2402.06196.pdf), [Semantic Scholar](https://www.semanticscholar.org/paper/a1f76db91c0debcf93ae9889736bce8470902113), [Google Scholar](https://scholar.google.com/scholar?q=Large%20Language%20Models%3A%20A%20Survey)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19433321-ff4b-44e5-8c4e-cfd521496367",
   "metadata": {},
   "source": [
    "Abstract—Large Language Models (LLMs) have drawn a lot of attention due to their strong performance on a wide range of natural language tasks, since the release of ChatGPT in November 2022. LLMs’ ability of general-purpose language understanding and generation is acquired by training billions of model’s parameters on massive amounts of text data, as predicted by scaling laws [1], [2]. The research area of LLMs, while very recent, is evolving rapidly in many different ways. In this paper, we review some of the most prominent LLMs, including three popular LLM families (GPT, LLaMA, PaLM), and discuss their characteristics, contributions and limitations. We also give an overview of techniques developed to build, and augment LLMs. We then survey popular datasets prepared for LLM training, fine-tuning, and evaluation, review widely used LLM evaluation metrics, and compare the performance of several popular LLMs on a set of representative benchmarks. Finally, we conclude the paper by discussing open challenges and future research directions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabf96e4-59f5-434c-9b92-cfdf87e4f5ab",
   "metadata": {},
   "source": [
    "#### Table of Contents <a class=\"anchor\" id=\"LLMs_toc\"></a>\n",
    "\n",
    "* [Table of Contents](#LLMs_toc)\n",
    "    * [**I. Introduction**](#LLMs_page_1)\n",
    "        * [Language Modeling](#LLMs_page_1.1)\n",
    "            * [Statistical language models (SLMs)](#LLMs_page_1.2)\n",
    "            * [Neural language models (NLMs)](#LLMs_page_1.3)\n",
    "            * [Pre-trained language models (PLMs)](#LLMs_page_1.4)\n",
    "            * [Large language models (LLMs) ](#LLMs_page_1.5)\n",
    "            * [The Paper Structure ](#LLMs_page_1.6)\n",
    "    * [**II. Large Language Models**](#LLMs_page_2)\n",
    "        * [A. Early Pre-trained Neural Language Models](#LLMs_page_2.1)\n",
    "            * [1. Encoder-only PLMs ](#LLMs_page_2.2)\n",
    "            * [TABLE I: High-level Overview of Popular Language Models](#LLMs_page_2.3)\n",
    "            * [2. Decoder-only PLMs ](#LLMs_page_2.4)\n",
    "            * [3. Encoder-Decoder PLMs ](#LLMs_page_2.5)\n",
    "        * [B. Large Language Model Families ](#LLMs_page_2.6)\n",
    "            * [1. The GPT Family ](#LLMs_page_2.7)\n",
    "            * [2. The LLaMA Family ](#LLMs_page_2.8)\n",
    "            * [3. The PaLM Family ](#LLMs_page_2.9)\n",
    "         * [C. Other Representative LLMs](#LLMs_page_2.10)\n",
    "    * [**III. HOW LLMS ARE BUILT**](#LLMs_page_3)\n",
    "        * [A. Dominant LLM Architectures](#LLMs_page_3.1)\n",
    "        * [B. Data Cleaning](#LLMs_page_3.2)\n",
    "        * [C. Tokenizations](#LLMs_page_3.3)\n",
    "        * [D. Positional Encoding](#LLMs_page_3.4)\n",
    "        * [E. Model Pre-training](#LLMs_page_3.5)\n",
    "        * [F. Fine-tuning and Instruction Tuning](#LLMs_page_3.6)\n",
    "    * [**References**](#LLMs_page_8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f099ea-6610-4f25-b4ac-49ef36030d14",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">\n",
    "\n",
    "# **I. Introduction** <a class=\"anchor\" id=\"LLMs_page_1`\"></a>\n",
    "\n",
    "[Back to Top](#LLMs_toc)\n",
    "\n",
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321f0bbd-f8b2-49f3-b9d0-f1b198691911",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">\n",
    "\n",
    "## **Language Modeling** <a class=\"anchor\" id=\"LLMs_page_1\"></a>\n",
    "\n",
    "[Back to Top](#LLMs_toc)\n",
    "\n",
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acda1f23-6c94-4ba3-9f82-84a5ac60d752",
   "metadata": {},
   "source": [
    "Language modeling is a long-standing research topic, dating back to the 1950s with Shannon’s application of information theory to human language, where he measured how well simple n-gram language models predict or compress natural language text [3]. Since then, statistical language modeling became fundamental to many natural language understanding and generation tasks, ranging from speech recognition, machine translation, to information retrieval [4], [5], [6].\n",
    "\n",
    "The recent advances on transformer-based large language models (LLMs), pretrained on Web-scale text corpora, significantly extended the capabilities of language models (LLMs). For example, OpenAI’s ChatGPT and GPT-4 can be used not only for natural language processing, but also as general task solvers to power Microsoft’s Co-Pilot systems, for instance, can follow human instructions of complex new tasks performing multi-step reasoning when needed. LLMs are thus becoming the basic building block for the development of general-purpose AI agents or artificial general intelligence (AGI).\n",
    "\n",
    "As the field of LLMs is moving fast, with new findings, models and techniques being published in a matter of months or weeks [7], [8], [9], [10], [11], AI researchers and practitioners often find it challenging to figure out the best recipes to build LLM-powered AI systems for their tasks. This paper gives a timely survey of the recent advances on LLMs. We hope this survey will prove a valuable and accessible resource for students, researchers and developers.\n",
    "\n",
    "LLMs are large-scale, pre-trained, statistical language models based on neural networks. The recent success of LLMs is an accumulation of decodes of research and development of language models, which can be categorized into four waves that have different starting points and velocity: statistical language models, neural language models, pre-trained language models and LLMs.\n",
    "\n",
    "**Statistical language models (SLMs)** view text as a sequence of words, and estimate the probability of text as the product of their word probabilities. The dominating form of SLMs are Markov chain models known as the n-gram models, which compute the probability of a word conditioned on its immediate proceeding n − 1 words. Since word probabilities are estimated using word and n-gram counts collected from text corpora, the model needs to deal with data sparsity (i.e., assigning zero probabilities to unseen words or n-grams) by using smoothing, where some probability mass of the model is reserved for unseen n-grams [12]. N-gram models are widely used in many NLP systems. However, these models are incomplete in that they cannot fully capture the diversity and variability of natural language due to data sparsity.\n",
    "\n",
    "Early **neural language models (NLMs)** [13], [14], [15], [16] deal with data sparsity by mapping words to low-dimensional continuous vectors (embedding vectors) and predict the next word based on the aggregation of the embedding vectors of its proceeding words using neural networks. The embedding vectors learned by NLMs define a hidden space where the semantic similarity between vectors can be readily computed as their distance. \n",
    "This opens the door to computing semantic similarity of any two inputs regardless their forms (e.g., queries vs. documents in Web search [17], [18], sentences in different languages in machine translation [19], [20]) or modalities (e.g., image and text in image captioning [21], [22]). Early NLMs are task-specific models, in that they are trained on task-specific data and their learned hidden space is task-specific.\n",
    "\n",
    "**Pre-trained language models (PLMs)**, unlike early NLMs, are task-agnostic. This generality also extends to the learned hidden embedding space. The training and inference of PLMs follows the pre-training and fine-tuning paradigm, where language models with recurrent neural networks [23] or transformers [24], [25], [26] are pre-trained on Web-scale unlabeled text corpora for general tasks such as word prediction, and then finetuned to specific tasks using small amounts of (labeled) task-specific data. Recent surveys on PLMs include [8], [27], [28].\n",
    "\n",
    "**Large language models (LLMs)** mainly refer to transformer-based neural language models<sup>1</sup> that contain tens to hundreds of billions of parameters, which are pre- trained on massive text data, such as PaLM [31], LLaMA [32], and GPT-4 [33], as summarized in Table III. Compared to PLMs, LLMs are not only much larger in model size, but also exhibit stronger language understanding and generation abilities, and more importantly, emergent abilities that are not present in smaller-scale language models. As illustrated in Fig. 1, these emergent abilities include (1) in-context learning, where LLMs learn a new task from a small set of examples presented in the prompt at inference time, (2) instruction following, where LLMs, after instruction tuning, can follow the instructions for new types of tasks without using explicit examples, and (3) multi-step reasoning, where LLMs can solve a complex task by breaking down that task into intermediate reasoning steps as demonstrated in the chain-of-thought prompt [34]. LLMs can also be augmented by using external knowledge and tools [35], [36] so that they can effectively interact with users and environment [37], and continually improve itself using feedback data collected through interactions (e.g. via reinforcement learning with human feedback (RLHF)).\n",
    "\n",
    "<p style=\"text-align: center\">\n",
    "  <img  src=\"../LLM-Images/Fig1-LLM-Capabilities.png\" width=\"1600\" alt=\"Fig1-LLM-Capabilities\">\n",
    "</p>\n",
    "\n",
    "<sup>1</sup>Recently, several very promising non-transformer LLMs have been proposed, such as the LLMs based on structured state space models [29], [30]. See Section VII for more details.\n",
    "\n",
    "Through advanced usage and augmentation techniques, LLMs can be deployed as so-called AI agents: artificial entities that sense their environment, make decisions, and take actions. Previous research has focused on developing agents for specific tasks and domains. The emergent abilities demonstrated by LLMs make it possible to build general-purpose AI agents based on LLMs. While LLMs are trained to produce responses in static settings, AI agents need to take actions to interact with dynamic environment. Therefore, LLM-based agents often need to augment LLMs to e.g., obtain updated information from external knowledge bases, verify whether a system action produces the expected result, and cope with when things do not go as expected, etc. We will discuss in detail LLM-based agents in Section IV.\n",
    "\n",
    "In the rest of this paper, Section II presents an overview of state of the art of LLMs, focusing on three LLM families (GPT, LLaMA and PaLM) and other representative models. Section III discusses how LLMs are built. Section IV discusses how LLMs are used, and augmented for real-world applications Sections V and VI review popular datasets and benchmarks for evaluating LLMs, and summarize the reported LLM evaluation results. Finally, Section VII concludes the paper by summarizing the challenges and future research directions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206190ee-c9a8-4964-ba6b-6f13c5008b8b",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">\n",
    "\n",
    "### The Paper Structure <a class=\"anchor\" id=\"LLMs_page_1.6\"></a>\n",
    "\n",
    "[Back to Top](#LLMs_toc)\n",
    "\n",
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd4c6c8-1273-4a48-8539-f5b3b9fae8d2",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center\">\n",
    "  <img  src=\"../LLM-Images/Fig2-The-Paper-Structure.png\" width=\"1200\" alt=\"Fig2-The-Paper-Structure\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7293cf3-b728-4d78-8ef8-adf4d713c291",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">\n",
    "\n",
    "## **II. Large Language Models** <a class=\"anchor\" id=\"LLMs_page_2\"></a>\n",
    "\n",
    "[Back to Top](#LLMs_toc)\n",
    "\n",
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9f5e32-4647-4c51-aada-e30c469d302f",
   "metadata": {},
   "source": [
    "In this section we start with a review of early pre-trained neural language models as they are the base of LLMs, and then focus our discussion on three families of LLMs: GPT, LlaMA, and PaLM. Table I provides an overview of some of these models and their characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f651fcb4-d82a-41b0-a10e-ab279fae3a6c",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">\n",
    "\n",
    "### A. Early Pre-trained Neural Language Models <a class=\"anchor\" id=\"LLMs_page_2.1\"></a>\n",
    "\n",
    "[Back to Top](#LLMs_toc)\n",
    "\n",
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4776a478-cadd-4e61-8ed8-0a14f148f29d",
   "metadata": {},
   "source": [
    "Language modeling using neural networks was pioneered by [38], [39], [40]. Bengio et al. [13] developed one of the first neural language models (NLMs) that are comparable to n-gram models. Then, [14] successfully applied NLMs to machine translation. The release of RNNLM (an open source NLM toolkit) by Mikolov [41], [42] helped significantly popularize NLMs. Afterwards, NLMs based on recurrent neural networks (RNNs) and their variants, such as long short-term memory (LSTM) [19] and gated recurrent unit (GRU) [20], were widely used for many natural language applications including machine translation, text generation and text classification [43]. \n",
    "\n",
    "Then, the invention of the Transformer architecture [44] marks another milestone in the development of NLMs. By applying self-attention to compute in parallel for every word in a sentence or document an “attention score” to model the influence each word has on another, Transformers allow for much more parallelization than RNNs, which makes it possible to efficiently pre-train very big language models on large amounts of data on GPUs. These pre-trained language models (PLMs) can be fine-tuned for many downstream tasks.\n",
    "\n",
    "We group early popular Transformer-based PLMs, based on their neural architectures, into three main categories: encoder-only, decoder-only, and encoder-decoder models. Comprehensive surveys of early PLMs are provided in [43], [28]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a033c7bf-30e9-42e2-981e-5e54efdc1331",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">\n",
    "\n",
    "### Encoder-only PLMs <a class=\"anchor\" id=\"LLMs_page_2.2\"></a>\n",
    "\n",
    "[Back to Top](#LLMs_toc)\n",
    "\n",
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9a770f-465b-4c04-84b2-996b037687dc",
   "metadata": {},
   "source": [
    "1) Encoder-only PLMs: As the name suggests, the encoder-only models only consist of an encoder network. These models are originally developed for language understanding tasks, such as text classification, where the models need to predict a class label for an input text. Representative encoder-only mod- els include BERT and its variants, e.g., RoBERTa, ALBERT, DeBERTa, XLM, XLNet, UNILM, as to be described below.\n",
    "BERT (Birectional Encoder Representations from Trans- formers) [24] is one of the most widely used encoder-only language models. BERT consists of three modules: (1) an embedding module that converts input text into a sequence of embedding vectors, (2) a stack of Transformer encoders that converts embedding vectors into contextual representation vectors, and (3) a fully connected layer that converts the representation vectors (at the final layer) to one-hot vectors. BERT is pre-trained uses two objectives: masked language modeling (MLM) and next sentence prediction. The pre-trained BERT model can be fine-tuned by adding a classifier layer for many language understanding tasks, ranging from text classification, question answering to language inference. A high-level overview of BERT framework is shown in Fig 3. As BERT significantly improved state of the art on a wide range of language understanding tasks when it was published, the AI community was inspired to develop many similar encoder-only language models based on BERT.\n",
    "\n",
    "<p style=\"text-align: center\">\n",
    "  <img  src=\"../LLM-Images/Fig3-BERT-PreTraining-Fine-Tuning.png\" width=\"1600\" alt=\"Fig3-BERT-PreTraining-Fine-Tuning.png\">\n",
    "</p>\n",
    "\n",
    "Fig. 3: Overall pre-training and fine-tuning procedures for BERT. Courtesy of [24]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38361448-956e-4d33-a482-d1d5c5cada46",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">\n",
    "\n",
    "### TABLE I: High-level Overview of Popular Language Models <a class=\"anchor\" id=\"LLMs_page_2.3\"></a>\n",
    "\n",
    "[Back to Top](#LLMs_toc)\n",
    "\n",
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd27664-00c9-457a-a031-c24468ea3e79",
   "metadata": {},
   "source": [
    "\n",
    "| **Type**        | **Model Name** | **#Parameters**    | **Release** | **Base Models** | **Open Source** | **#Tokens** | **Training Dataset**                                                                 |\n",
    "|-----------------|----------------|--------------------|-------------|-----------------|-----------------|-------------|--------------------------------------------------------------------------------------|\n",
    "| **Encoder-Only**| BERT           | 110M, 340M         | 2018        | ✓               | ✓               | 137B        | BooksCorpus, English Wikipedia                                                       |\n",
    "|                 | RoBERTa        | 355M               | 2019        | ✓               | ✓               | 2.2T        | BooksCorpus, English Wikipedia, CC-NEWS, STORIES (a subset of Common Crawl), Reddit  |\n",
    "|                 | ALBERT         | 12M, 18M, 60M, 235M | 2019    | ✓           | ✓            | 137B    | BooksCorpus, English Wikipedia                                                        |\n",
    "|                 | DeBERTa        | 223M, 300M          | 2020    | ✓           | ✓            | -       | BooksCorpus, English Wikipedia, STORIES, Reddit content                               |\n",
    "|                 | XLNet          | 340M                | 2019    | ✓           | ✓            | 32.89B  | BooksCorpus, English Wikipedia, Giga5, Common Crawl, ClueWeb 2012-B                   |\n",
    "|-----------------|----------------|--------------------|-------------|-----------------|-----------------|-------------|--------------------------------------------------------------------------------------|\n",
    "| **Decoder-Only**    | GPT-1          | 110M                | 2018    | ✓           | ✓            | 1.3B    | BooksCorpus, English Wikipedia                                                        |\n",
    "|                 | GPT-2          | 1.5B                | 2019    | ✓           | ✓            | 10B     | Reddit outbound                                                                       |\n",
    "|-----------------|----------------|--------------------|-------------|-----------------|-----------------|-------------|--------------------------------------------------------------------------------------|\n",
    "| **Encoder-Decoder** | T5 (Base)      | 220M                | 2019    | ✓           | ✓            | 156B    | C4 (Colossal Clean Crawled Corpus)                                                    |\n",
    "|                 | MT5 (Base)     | 300M                | 2020    | ✓           | ✓            | -       | mC4 (multilingual Colossal Clean Crawled Corpus)                                      |\n",
    "|                 | BART (Base)    | 139M                | 2019    | ✓           | ✓            | -       | BooksCorpus, English Wikipedia, CC-NEWS, STORIES (a subset of Common Crawl), Reddit   |\n",
    "|-----------------|----------------|--------------------|-------------|-----------------|-----------------|-------------|--------------------------------------------------------------------------------------|\n",
    "| **GPT Family**      | GPT-3          | 6.7B, 13B, 175B     | 2020    | ×           | ×            | 300B    | Common Crawl (filtered), WebText2, Books1, Books2, Wikipedia                          |\n",
    "|                 | CODEX          | 12B                 | 2021    | GPT         | ✓            | -       | Public GitHub software repositories                                                   |\n",
    "|                 | WebGPT         | 760M, 13B, 175B     | 2021    | GPT3        | ×            | -       | ELI5                                                                                  |\n",
    "|                 | GPT-4          | 1.76T               | 2023    | -           | ×            | 13T     | Diverse internet text                                                                 |\n",
    "|-----------------|----------------|--------------------|-------------|-----------------|-----------------|-------------|--------------------------------------------------------------------------------------|\n",
    "| **LLaMA Family**    | LLaMA1         | 7B, 13B, 33B, 65B   | 2023    | -           | ✓            | 1T, 1.4T| Online Sources                                                                        |\n",
    "|                 | LLaMA2         | 7B, 13B, 34B, 70B   | 2023    | -           | ✓            | 2T      | Online sources                                                                        |\n",
    "|                 | Alpaca         | 7B                  | 2023    | LLaMA1      | ✓            | -       | GPT-3.5                                                                               |\n",
    "|                 | Vicuna-13B     | 13B                 | 2023    | LLaMA1      | ✓            | -       | GPT-3.5                                                                               |\n",
    "|                 | Koala          | 13B                 | 2023    | LLaMA       | ✓            | -       | Dialogue data                                                                         |\n",
    "|                 | Mistral-7B     | 7.3B                | 2023    | -           | ✓            | -       | -                                                                                     |\n",
    "|                 | Code Llama     | 34B                  | 2023    | LLaMA2      | ✓            | 500B    | Publicly available code                                                               |\n",
    "|                 | LongLLaMA      | 3B, 7B              | 2023    | OpenLLaMA   | ✓            | 1T      | -                                                                                     |\n",
    "|                 | LLaMA-Pro-8B   | 8.3B                | 2024    | LLaMA2-7B   | ✓            | 80B     | Code and math corpora                                                                 |\n",
    "|                | TinyLlama-1.1B | 1.1B                | 2024    | LLaMA1.1B   | ✓            | 3T      | SlimPajama, Starcoderdata                                                             |\n",
    "|-----------------|----------------|--------------------|-------------|-----------------|-----------------|-------------|--------------------------------------------------------------------------------------|\n",
    "| **PaLM Family**    | PaLM           | 8B, 62B, 540B       | 2022    | -           | x            | 780B    | Web documents, books, Wikipedia, conversations, GitHub code                           | \n",
    "|                | U-PaLM         | 8B, 62B, 540B       | 2022    | -           | x            | 1.3B    | Web documents, books, Wikipedia, conversations, GitHub code                           |\n",
    "|                | PaLM-2         | 340B                | 2023    | -           | ✓            | 3.6T    | Web documents, books, code, mathematics, conversational data                          |\n",
    "|                | Med-PaLM       | 540B                | 2022    | PaLM        | x            | 780B    | HealthSearchQA, MedicationQA, LiveQA                                                  |\n",
    "|                | Med-PaLM 2     | -                   | 2023    | PaLM 2      | x            | -       | MedQA, MedMCQA, HealthSearchQA, LiveQA, MedicationQA                                  |\n",
    "|-----------------|----------------|--------------------|-------------|-----------------|-----------------|-------------|--------------------------------------------------------------------------------------|\n",
    "| **Other**          | FLAN           | 137B                | 2021    | LaMDA-PT    | ✓            | -       | Web documents, code, dialog data, Wikipedia                                           |\n",
    "|  **Popular LLMs**  | Gopher         | 280B                | 2021    | -           | ×            | 300B    | MassiveText                                                                           |\n",
    "|                | ERNIE 4.0      | 10B                 | 2023    | -           | ×            | 4TB     | Chinese text                                                                          |\n",
    "|                | Retro          | 7.5B                | 2021    | -           | ×            | 600B    | MassiveText                                                                           |\n",
    "|                | LaMDA          | 137B                | 2022    | -           | ×            | 168B    | public dialog data and web documents                                                  |\n",
    "|                | ChinChilla     | 70B                 | 2022    | -           | ×            | 1.4T    | MassiveText                                                                           |\n",
    "|                | Galactia-120B  | 120B                | 2022    | -           | ×            | 450B    |                                                                                       |\n",
    "|                | CodeGen        | 16.1B               | 2022    | -           | ✓            | -       | THE PILE, BIGQUERY, BIGPYTHON                                                         |\n",
    "|                | BLOOM          | 176B                | 2022    | -           | ✓            | 366B    | ROOTS                                                                                 |\n",
    "|                | Zephyr         | 7.24B               | 2023    | Mistral-7B  | ✓            | 800B    | Synthetic data                                                                        |\n",
    "|                | Grok-0         | 33B                 | 2023    | -           | ×            | -       | Online source                                                                         |\n",
    "|                | ORCA-2         | 13B                 | 2023    | LLaMA2      | -            | 201B    | -                                                                                     |\n",
    "|                | StartCoder     | 15.5B               | 2023    | -           | ✓            | 35B     | GitHub                                                                                |\n",
    "|                | MPT            | 7B                  | 2023    | -           | ✓            | 1T      | RedPajama, m Common Crawl, S2ORC, Common Crawl                                        |\n",
    "|                | Mixtral-8x7B   | 46.7B               | 2023    | -           | ✓            | -       | Instruction dataset                                                                   |\n",
    "|                | Falcon         | 180B                | 2023    | -           | ✓            | 3.5T    | RefinedWeb                                                                            |\n",
    "|                | Gemini         | 1.8B, 3.25B         | 2023    | -           | ✓            | -       | Web documents, books, and code, image data, audio data, video data                    |\n",
    "|                | DeepSeek-Coder | 1.3B, 6.7B, 33B     | 2024    | -           | ✓            | 2T      | GitHub’s Markdown and StackExchange                                                   |\n",
    "|                | DocLLM         | 1B,7B               | 2024    | -           | ×            | 2T      | IIT-CDIP Test Collection 1.0, DocBank                                                 |\n",
    "|-----------------|----------------|--------------------|-------------|-----------------|-----------------|-------------|--------------------------------------------------------------------------------------|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9280af1-df47-4694-a611-10c6a04c14d8",
   "metadata": {},
   "source": [
    "RoBERTa [25] significantly improves the robustness of BERT using a set of model design choices and training strategies, such as modifying a few key hyperparameters, removing the next-sentence pre-training objective and training with much larger mini-batches and learning rates. ALBERT [45] uses two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT: (1) splitting the embedding matrix into two smaller matrices, and (2) using repeating layers split among groups. DeBERTa (Decoding- enhanced BERT with disentangled attention) [26] improves the BERT and RoBERTa models using two novel techniques. The first is the disentangled attention mechanism, where each word is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices on their contents and relative positions, respectively. Second, an enhanced mask de- coder is used to incorporate absolute positions in the decoding layer to predict the masked tokens in model pre-training. In addition, a novel virtual adversarial training method is used for fine-tuning to improve models’ generalization. ELECTRA [46] uses a new pre-training task, known as replaced token detection (RTD), which is empirically proven to be more sample-efficient than MLM. Instead of masking the input, RTD corrupts it by replacing some tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, a discriminative model is trained to predict whether a token in the corrupted input was replaced by a generated sample or not. RTD is more sample-efficient than MLM because the former is defined over all input tokens rather than just the small subset being masked out, as illustrated in Fig 4.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"../LLM-Images/Fig4-BERT-Comparison.png\" alt=\"Fig. 4: A comparison between replaced token detection and masked language modeling. Courtesy of [46].\" style=\"width:100%;\">\n",
    "  <figcaption style=\"text-align: center;\">Fig. 4: A comparison between replaced token detection and masked language modeling. Courtesy of [46]. <a href=\"\">[47]</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "XLMs [47] extended BERT to cross-lingual language models using two methods: (1) a unsupervised method that only relies on monolingual data, and (2) a supervised method that leverages parallel data with a new cross-lingual language model objective, as illustrated in Fig 5. XLMs had obtained state-of-the-art results on cross-lingual classification, unsupervised and supervised machine translation, at the time they were proposed.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"../LLM-Images/Fig5-Corss-Lingual-lang-model-pretraining.png\" alt=\"Fig. 5: Cross-lingual language model pretraining. The MLM objective is similar to BERT, but with continuous streams of text as opposed to sentence pairs. The TLM objective extends MLM to pairs of parallel sentences. To predict a masked English word, the model can attend to both the English sentence and its French translation, and is encouraged to align English and French representations.\" style=\"width:100%;\">\n",
    "  <figcaption style=\"text-align: center;\">Fig. 5: Cross-lingual language model pretraining. The MLM objective is similar to BERT, but with continuous streams of text as opposed to sentence pairs. The TLM objective extends MLM to pairs of parallel sentences. To predict a masked English word, the model can attend to both the English sentence and its French translation, and is encouraged to align English and French representations. Courtesy of <a href=\"\">[47]</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "There are also encoder-only language models that leverage the advantages of auto-regressive (decoder) models for model training and inference. Two examples are XLNet and UNILM. XLNet [48] is based on Transformer-XL, pre-trained using a generalized autoregressive method that enables learning bidi- rectional contexts by maximizing the expected likelihood over all permutations of the factorization order. UNILM (UNIfied pre-trained Language Model) [49] is pre-trained using three types of language modeling tasks: unidirectional, bidirectional, and sequence-to-sequence prediction. This is achieved by employing a shared Transformer network and utilizing specific self-attention masks to control what context the prediction is conditioned on, as illustrated in Fig 6. The pre-trained model can be fine-tuned for both natural language understanding and generation tasks.\n",
    "\n",
    "\n",
    "\n",
    "<figure>\n",
    "  <img src=\"../LLM-Images/Fig6-LM-Pretraining-Overview.png\" alt=\"Fig. 6: Overview of unified LM pre-training. The model parameters are shared across the LM objectives (i.e., bidirectional LM, unidirectional LM, and sequence-to-sequence LM).\" style=\"width:100%;\">\n",
    "  <figcaption style=\"text-align: center;\">Fig. 6: Overview of unified LM pre-training. The model parameters are shared across the LM objectives (i.e., bidirectional LM, unidirectional LM, and sequence-to-sequence LM).  Courtesy of <a href=\"\">[49]</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b349a7d-e0c0-45f7-bb97-a297aaddfde0",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">\n",
    "\n",
    "### 2. Decoder-only PLMs <a class=\"anchor\" id=\"LLMs_page_2.4\"></a>\n",
    "\n",
    "[Back to Top](#LLMs_toc)\n",
    "\n",
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e982a3b9-4f14-4749-84e8-b6f796a9f65e",
   "metadata": {},
   "source": [
    "2) Decoder-only PLMs: Two of the most widely used decoder-only PLMs are GPT-1 and GPT-2, developed by OpenAI. These models lay the foundation to more powerful LLMs subsequently, i.e., GPT-3 and GPT-4.\n",
    "\n",
    "GPT-1 [50] demonstrates for the first time that good performance over a wide range of natural language tasks can be obtained by Generative Pre-Training (GPT) of a decoder-only Transformer model on a diverse corpus of unlabeled text in a self-supervised learning fashion (i.e., next word/token prediction), followed by discriminative fine-tuning on each specific downstream task (with much fewer samples), as illustrated in Fig 7. GPT-1 paves the way for subsequent GPT models, with each version improving upon the architecture and achieving better performance on various language tasks.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"../LLM-Images/Fig7-High-level-overview-GPT.png\" alt=\"High-level overview of GPT pretraining, and fine-tuning steps.\" style=\"width:100%;\">\n",
    "  <figcaption style=\"text-align: center;\">Fig. 7: High-level overview of GPT pretraining, and fine-tuning steps. Courtesy of OpenAI.  <a href=\"https://openai.com/\">[24]</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "GPT-2 [51] shows that language models are able to learn to perform specific natural language tasks without any explicit supervision when trained on a large WebText dataset consisting of millions of webpages. The GPT-2 model follows the model designs of GPT-1 with a few modifications: Layer normal- ization is moved to the input of each sub-block, additional layer normalization is added after the final self-attention block, initialization is modified to account for the accumulation on the residual path and scaling the weights of residual layers, vocabulary size is expanded to 50,25, and context size is increased from 512 to 1024 tokens.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b4240c-b93a-49f4-b0e1-655542c75a80",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">\n",
    "\n",
    "### 3. Encoder-Decoder PLMs <a class=\"anchor\" id=\"LLMs_page_2.5\"></a>\n",
    "\n",
    "[Back to Top](#LLMs_toc)\n",
    "\n",
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd69133-1a20-4f85-b3ff-8d94b17c32b9",
   "metadata": {},
   "source": [
    "3) Encoder-Decoder PLMs: In [52], Raffle et al. shows that almost all NLP tasks can be cast as a sequence-to-sequence generation task. Thus, an encoder-decoder language model, by design, is a unified model in that it can perform all natural language understanding and generation tasks. Representative encoder-decoder PLMs we will review below are T5, mT5, MASS, and BART.\n",
    "\n",
    "T5 [52] is a Text-to-Text Transfer Transformer (T5) model, where transfer learning is effectively exploited for NLP via an introduction of a unified framework in which all NLP tasks are cast as a text-to-text generation task. mT5 [53] is a multilingual variant of T5, which is pre-trained on a new Common Crawl- based dataset consisting of texts in 101 languages.\n",
    "\n",
    "MASS (MAsked Sequence to Sequence pre-training) [54] adopts the encoder-decoder framework to reconstruct a sen- tence fragment given the remaining part of the sentence. The encoder takes a sentence with randomly masked fragment (several consecutive tokens) as input, and the decoder predicts the masked fragment. In this way, MASS jointly trains the encoder and decoder for language embedding and generation, respectively.\n",
    "\n",
    "BART [55] uses a standard sequence-to-sequence transla- tion model architecture. It is pre-trained by corrupting text with an arbitrary noising function, and then learning to reconstruct the original text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fb55ac-69ed-44bb-8286-8cbe6098fb0e",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">\n",
    "\n",
    "### B. Large Language Model Families <a class=\"anchor\" id=\"LLMs_page_2.6\"></a>\n",
    "\n",
    "[Back to Top](#LLMs_toc)\n",
    "\n",
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e20f278-8878-47a9-a111-e20ce680e1b6",
   "metadata": {},
   "source": [
    "B. Large Language Model Families\n",
    "Large language models (LLMs) mainly refer to transformer-based PLMs that contain tens to hundreds of billions of parameters. Compared to PLMs reviewed above, LLMs are not only much larger in model size, but also exhibit stronger language understanding and generation and emergent abilities that are not present in smaller-scale models. In what follows, we review three LLM families: GPT, LLaMA, and PaLM, as illustrated in Fig 8.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"../LLM-Images/Fig8-Popular-LLM-Families.png\" alt=\"Fig. 8: Popular LLM Families\" style=\"width:100%;\">\n",
    "  <figcaption style=\"text-align: center;\">Fig. 8: Popular LLM Families  <a href=\"\">[h]</a></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8e8b09-24a9-4f0f-87e3-99aed98c1818",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">\n",
    "\n",
    "### 1. The GPT Family <a class=\"anchor\" id=\"LLMs_page_2.7\"></a>\n",
    "\n",
    "[Back to Top](#LLMs_toc)\n",
    "\n",
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5701d5fc-c0c5-42a1-88bd-71657614e8b1",
   "metadata": {},
   "source": [
    "1) The GPT Family: Generative Pre-trained Transform- ers (GPT) are a family of decoder-only Transformer-based language models, developed by OpenAI. This family consists of GPT-1, GPT-2, GPT-3, InstrucGPT, ChatGPT, GPT-4, CODEX, and WebGPT. Although early GPT models, such as GPT-1 and GPT-2, are open-source, recent models, such as GPT-3 and GPT-4, are close-source and can only be accessed via APIs. GPT-1 and GPT-2 models have been discussed in the early PLM subsection. We start with GPT-3 below.\n",
    "\n",
    "GPT-3 [56] is a pre-trained autoregressive language model with 175 billion parameters. GPT-3 is widely considered as the first LLM in that it not only is much larger than previous PLMs, but also for the first time demonstrates emergent abilities that are not observed in previous smaller PLMs. GPT- 3 shows the emergent ability of in-context learning, which means GPT-3 can be applied to any downstream tasks without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieved strong performance on many NLP tasks, including translation, question-answering, and the cloze tasks, as well as several ones that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, 3-digit arithmetic. Fig 9 plots the performance of GPT-3 as a function of the number of examples in in-context prompts.\n",
    "\n",
    "CODEX [57], released by OpenAI in March 2023, is a general-purpose programming model that can parse natural language and generate code in response. CODEX is a de- scendant of GPT-3, fine-tuned for programming applications on code corpora collected from GitHub. CODEX powers Microsoft’s GitHub Copilot.\n",
    "\n",
    "WebGPT [58] is another descendant of GPT-3, fine-tuned to answer open-ended questions using a text-based web browser, facilitating users to search and navigate the web. Specifically, WebGPT is trained in three steps. The first is for WebGPT to learn to mimic human browsing behaviors using human demonstration data. Then, a reward function is learned to predict human preferences. Finally, WebGPT is refined to optimize the reward function via reinforcement learning and rejection sampling to enable LLMs to follow expected human instructions, InstructGPT [59] is proposed to align language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, a dataset of labeler demonstrations of the desired model behavior is collected. Then GPT-3 is fine-tuned on this dataset. Then, a dataset of human-ranked model outputs is collected to further fine-tune the model using reinforcement learning. The method is known Reinforcement Learning from Human Feedback\n",
    "\n",
    "<figure>\n",
    "  <img src=\"../LLM-Images/Fig9-GPT3.png\" alt=\"Fig. 9: GPT-3 shows that larger models make increasingly efficient use of in-context information. It shows in-context learning performance on a simple task requiring the model to remove random symbols from a word, both with and without a natural language task description. Courtesy of [56].\" style=\"width:100%;\">\n",
    "  <figcaption style=\"text-align: center;\">Fig. 9: GPT-3 shows that larger models make increasingly efficient use of in-context information. It shows in-context learning performance on a simple task requiring the model to remove random symbols from a word, both with and without a natural language task description. Courtesy of  <a href=\"\">[56]</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "[Language Models are Few-Shot Learners](https://papers.nips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf)\n",
    "\n",
    "\n",
    "(RLHF), as shown in Fig 10. The resultant InstructGPT models have shown improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"../LLM-Images/Fig10-RLHF.png\" alt=\"Fig. 10: The high-level overview of RLHF. Courtesy of [59].\" style=\"width:100%;\">\n",
    "  <figcaption style=\"text-align: center;\">Fig. 10: The high-level overview of RLHF. Courtesy of <a href=\"\">[59]</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "The most important milestone of LLM development is the launch of ChatGPT (Chat Generative Pre-trained Transformer) [60] on November 30, 2022. ChatGPT is chatbot that enables users to steer a conversation to complete a wide range of tasks such as question answering, information seeking, text summarization, and more. ChatGPT is powered by GPT-3.5 (and later by GPT-4), a sibling model to InstructGPT, which is trained to follow an instruction in a prompt and provide a detailed response.\n",
    "\n",
    "GPT-4 [33] is the latest and most powerful LLM in the GPT family. Launched in March, 2023, GPT-4 is a multi- modal LLM in that it can take image and text as inputs and produce text outputs. While still less capable than humans in some of the most challenging real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers, as shown in Fig 11. Like early GPT models, GPT-4 was first pre-trained to predict next tokens on large text corpora, and then fine-tuned with RLHF to align model behaviors with human-desired ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4657b338-a430-4ea2-90fb-b9f49cfa97f9",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">\n",
    "\n",
    "### 2. The LLaMA Family <a class=\"anchor\" id=\"LLMs_page_2.8\"></a>\n",
    "\n",
    "[Back to Top](#LLMs_toc)\n",
    "\n",
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c24b101-bef3-4367-bb14-35ae7893d8e1",
   "metadata": {},
   "source": [
    "2) The LLaMA Family: LLaMA is a collection of founda- tion language models, released by Meta. Unlike GPT models, LLaMA models are open-source, i.e., model weights are released to the research community under a noncommercial license. Thus, the LLaMA family grows rapidly as these models are widely used by many research groups to develop better open-source LLMs to compete the closed-source ones or to develop task-specific LLMs for mission-critical applications.\n",
    "The first set of LLaMA models [32] was released in Febru- ary 2023, ranging from 7B to 65B parameters. These models are pre-trained on trillions of tokens, collected from publicly available datasets. LLaMA uses the transformer architecture of GPT-3, with a few minor architectural modifications, including (1) using a SwiGLU activation function instead of ReLU, (2) using rotary positional embeddings instead of absolute positional embedding, and (3) using root-mean-squared layer- normalization instead of standard layer-normalization. The open-source LLaMA-13B model outperforms the proprietary GPT-3 (175B) model on most benchmarks, making it a good baseline for LLM research.\n",
    "\n",
    "The first set of LLaMA models [32] was released in Febru- ary 2023, ranging from 7B to 65B parameters. These models are pre-trained on trillions of tokens, collected from publicly available datasets. LLaMA uses the transformer architecture of GPT-3, with a few minor architectural modifications, including (1) using a SwiGLU activation function instead of ReLU, (2) using rotary positional embeddings instead of absolute positional embedding, and (3) using root-mean-squared layer- normalization instead of standard layer-normalization. The open-source LLaMA-13B model outperforms the proprietary GPT-3 (175B) model on most benchmarks, making it a good baseline for LLM research.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"../LLM-Images/Fig11-GPT3-4-Exam-results.png\" alt=\"Fig. 11: GPT-4 performance on academic and professional exams, compared with GPT 3.5. Courtesy of [33].\" style=\"width:100%;\">\n",
    "  <figcaption style=\"text-align: center;\">Fig. 11: GPT-4 performance on academic and professional exams, compared with GPT 3.5. Courtesy of <a href=\"\">[33]</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "In July 2023, Meta, in partnership with Microsoft, released the LLaMA-2 collection [61], which include both foundation language models and Chat models finetuned for dialog, known as LLaMA-2 Chat. The LLaMA-2 Chat models were reported to outperform other open-source models on many public benchmarks. Fig 12 shows the training process of LLaMA-2 Chat. The process begins with pre-training LLaMA-2 using publicly available online data. Then, an initial version of LLaMA-2 Chat is built via supervised fine-tuning. Subse- quently, the model is iteratively refined using RLHF, rejection sampling and proximal policy optimization. In the RLHF stage, the accumulation of human feedback for revising the reward model is crucial to prevent the reward model from being changed too much, which could hurt the stability of LLaMA model training.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"../LLM-Images/Fig12-Training-of-LLaMA-2-Chat.png\" alt=\"Fig. 12: Training of LLaMA-2 Chat. Courtesy of [61]\" style=\"width:100%;\">\n",
    "  <figcaption style=\"text-align: center;\">Fig. 12: Training of LLaMA-2 Chat. Courtesy of <a href=\"\">[61]</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "Alpaca [62] is fine-tuned from the LLaMA-7B model using 52K instruction-following demonstrations generated in the style of self-instruct using GPT-3.5 (text-davinci-003). Alpaca is very cost-effective for training, especially for academic research. On the self-instruct evaluation set, Alpaca performs similarly to GPT-3.5, despite that Alpaca is much smaller.\n",
    "The Vicuna team has developed a 13B chat model, Vicuna- 13B, by fine-tuning LLaMA on user-shared conversations collected from ShareGPT. Preliminary evaluation using GPT- 4 as a evaluator shows that Vicuna-13B achieves more than 90% quality of OpenAI’s ChatGPT, and Google’s Bard while outperforming other models like LLaMA and Stanford Alpaca in more than 90% of cases. 13 shows the relative response quality of Vicuna and a few other well-known models by GPT-4. Another advantage of Vicuna-13B is its relative limited computational demand for model training. The training cost of Vicuna-13B is merely $300.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"../LLM-Images/Fig13-Relative-Response-Quality-of-Vicuna.png\" alt=\"Fig. 13: Relative Response Quality of Vicuna and a few other well-known models by GPT-4. Courtesy of Vicuna Team.\n",
    "\" style=\"width:100%;\">\n",
    "  <figcaption style=\"text-align: center;\">Fig. 13: Relative Response Quality of Vicuna and a few other well-known models by GPT-4. Courtesy of Vicuna Team.\n",
    " <a href=\"\">[?]</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "Like Alpaca and Vicuna, the Guanaco models [63] are also finetuned LLaMA models using instruction-following data. But the finetuning is done very efficiently using QLoRA such that finetuning a 65B parameter model can be done on a single 48GB GPU. QLoRA back-propagates gradients through a frozen, 4-bit quantized pre-trained language model into Low Rank Adapters (LoRA). The best Guanaco model outperforms all previously released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of fine-tuning on a single GPU.\n",
    "\n",
    "Koala [64] is yet another instruction-following language model built on LLaMA, but with a specific focus on interaction data that include user inputs and responses generated by highly capable closed-source chat models such as ChatGPT. The Koala-13B model performs competitively with state-of-the-art chat models according to human evaluation based on real- world user prompts.\n",
    "Mistral-7B [65] is a 7B-parameter language model engi- neered for superior performance and efficiency. Mistral-7B outperforms the best open-source 13B model (LLaMA-2-13B) across all evaluated benchmarks, and the best open-source 34B model (LLaMA-34B) in reasoning, mathematics, and code generation. This model leverages grouped-query attention for faster inference, coupled with sliding window attention to effectively handle sequences of arbitrary length with a reduced inference cost.\n",
    "\n",
    "The LLaMA family is growing rapidly, as more instruction- following models have been built on LLaMA or LLaMA- 2, including Code LLaMA [66], Gorilla [67], Giraffe [68], Vigogne [69], Tulu 65B [70], Long LLaMA [71], and Stable Beluga2 [72], just to name a few.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"../LLM-Images/Fig12-Training-of-LLaMA-2-Chat.png\" alt=\"Fig. 14: Flan-PaLM finetuning consist of 473 datasets in above task categories. Courtesy of [74].\" style=\"width:100%;\">\n",
    "  <figcaption style=\"text-align: center;\">Fig. 14: Flan-PaLM finetuning consist of 473 datasets in above task categories. Courtesy of <a href=\"\">[74]</a></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893d57bd-5d7c-4072-9b80-2d59fef18c9b",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">\n",
    "\n",
    "### 3. The PaLM Family <a class=\"anchor\" id=\"LLMs_page_2.9\"></a>\n",
    "\n",
    "[Back to Top](#LLMs_toc)\n",
    "\n",
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207a17d7-48d6-4313-8646-30521a071bf1",
   "metadata": {},
   "source": [
    "3) The PaLM Family: The PaLM (Pathways Language Model) family are developed by Google. The first PaLM model [31] was announced in April 2022 and remained private until March 2023. It is a 540B parameter transformer-based LLM. The model is pre-trained on a high-quality text corpus consisting of 780 billion tokens that comprise a wide range of natural language tasks and use cases. PaLM is pre-trained on 6144 TPU v4 chips using the Pathways system, which enables highly efficient training across multiple TPU Pods. PaLM demonstrates continued benefits of scaling by achiev- ing state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. PaLM- 540B outperforms not only state-of-the-art fine-tuned models on a suite of multi-step reasoning tasks, but also on par with humans on the recently released BIG-bench benchmark.\n",
    "\n",
    "The U-PaLM models of 8B, 62B, and 540B scales are continually trained on PaLM with UL2R, a method of continue training LLMs on a few steps with UL2’s mixture-of-denoiser objective [73]. An approximately 2x computational savings rate is reported.\n",
    "\n",
    "U-PaLM is later instruction-finetuned as Flan-PaLM [74]. Compared to other instruction finetuning work mentioned above, Flan-PaLM’s finetuning is performed using a much larger number of tasks, larger model sizes, and chain-of- thought data. As a result, Flan-PaLM substantially outperforms previous instruction-following models. For instance, Flan- PaLM-540B, which is instruction-finetuned on 1.8K tasks, outperforms PaLM-540B by a large margin (+9.4% on av- erage). The finetuning data comprises 473 datasets, 146 task categories, and 1,836 total tasks, as illustrated in Fig 14.\n",
    "\n",
    "\n",
    "PaLM-2 [75] is a more compute-efficient LLM with bet- ter multilingual and reasoning capabilities, compared to its predecessor PaLM. PaLM-2 is trained using a mixture of objectives. Through extensive evaluations on English, multi- lingual, and reasoning tasks, PaLM-2 significantly improves the model performance on downstream tasks across different model sizes, while simultaneously exhibiting faster and more efficient inference than PaLM.\n",
    "\n",
    "Med-PaLM [76] is a domain-specific PaLM, and is de- signed to provide high-quality answers to medical questions. Med-PaLM is finetuned on PaLM using instruction prompt tuning, a parameter-efficient method for aligning LLMs to new domains using a few exemplars. Med-PaLM obtains very encouraging results on many healthcare tasks, although it is still inferior to human clinicians. Med-PaLM 2 improves Med- PaLM via med-domain finetuning and ensemble prompting\n",
    "[77]. Med-PaLM 2 scored up to 86.5% on the MedQA dataset (i.e., a benchmark combining six existing open ques- tion answering datasets spanning professional medical exams, research, and consumer queries), improving upon Med-PaLM by over 19% and setting a new state-of-the-art.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9cda55-dee3-4eca-9f7d-055e2280e728",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">\n",
    "\n",
    "### C. Other Representative LLMs <a class=\"anchor\" id=\"LLMs_page_2.10\"></a>\n",
    "\n",
    "[Back to Top](#LLMs_toc)\n",
    "\n",
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e732e69-5e1b-4474-9b98-470abc9aee3e",
   "metadata": {},
   "source": [
    "In addition to the models discussed in the previous sub- sections, there are other popular LLMs which do not belong to those three model families, yet they have achieved great performance and have pushed the LLMs field forward. We briefly describe these LLMs in this subsection.\n",
    "\n",
    "FLAN: In [78], Wei et al. explored a simple method for improving the zero-shot learning abilities of language models. They showed that instruction tuning language models on a collection of datasets described via instructions substantially improves zero-shot performance on unseen tasks. They take a 137B parameter pretrained language model and instruction tune it on over 60 NLP datasets verbalized via natural language instruction templates. They call this instruction-tuned model FLAN. Fig 15 provides a comparison of instruction tuning with pretrain–finetune and prompting.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"../LLM-Images/Fig9-GPT3.png\" alt=\"Fig. 15: comparison of instruction tuning with pre- train–finetune and prompting. Courtesy of [78].\" style=\"width:100%;\">\n",
    "  <figcaption style=\"text-align: center;\">Fig. 15: comparison of instruction tuning with pre- train–finetune and prompting. Courtesy of  <a href=\"\">[78]</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "[FINETUNED LANGUAGE MODELS ARE ZERO-SHOT LEARNERS](https://arxiv.org/pdf/2109.01652.pdf)\n",
    "\n",
    "\n",
    "Gopher: In [79], Rae et al. presented an analysis of Transformer-based language model performance across a wide range of model scales — from models with tens of millions of parameters up to a 280 billion parameter model called Gopher. These models were evaluated on 152 diverse tasks, achieving state-of-the-art performance across the majority. The number of layers, the key/value size, and other hyper-parameters of different model sizes are shown in Fig 16.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"../LLM-Images/Fig16-Model-architecture-details-of-Gopher.png\" alt=\"Fig. 16: Model architecture details of Gopher with different number of parameters. Courtesy of [78]\" style=\"width:100%;\">\n",
    "  <figcaption style=\"text-align: center;\">Fig. 16: Model architecture details of Gopher with different number of parameters. Courtesy of <a href=\"\">[78]</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "T0: In [80], Sanh et al. developed T0, a system for easily mapping any natural language tasks into a human-readable prompted form. They converted a large set of supervised datasets, each with multiple prompts with diverse wording. These prompted datasets allow for benchmarking the ability of a model to perform completely held-out tasks. Then, a T0 encoder-decoder model is developed to consume textual inputs and produces target responses. The model is trained on a multitask mixture of NLP datasets partitioned into different tasks.\n",
    "\n",
    "ERNIE 3.0: In [81], Sun et al. proposed a unified frame- work named ERNIE 3.0 for pre-training large-scale knowledge enhanced models. It fuses auto-regressive network and auto- encoding network, so that the trained model can be easily tai- lored for both natural language understanding and generation tasks using zero-shot learning, few-shot learning or fine-tuning. They have trained ERNIE 3.0 with 10 billion parameters on a 4TB corpus consisting of plain texts and a large-scale knowledge graph. Fig 17 illustrates the model architecture of Ernie 3.0.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"../LLM-Images/Fig17-High-level-model-architecture-of-ERNIE.png\" alt=\"Fig. 17: High-level model architecture of ERNIE 3.0. Courtesy of [81].\" style=\"width:100%;\">\n",
    "  <figcaption style=\"text-align: center;\">Fig. 17: High-level model architecture of ERNIE 3.0. Courtesy of <a href=\"\">[81]</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "RETRO: In [82], Borgeaud et al. enhanced auto-regressive language models by conditioning on document chunks re- trieved from a large corpus, based on local similarity with pre- ceding tokens. Using a 2-trillion-token database, the Retrieval- Enhanced Transformer (Retro) obtains comparable perfor- mance to GPT-3 and Jurassic-1 [83] on the Pile, despite using 25% fewer parameters. As shown in Fig 18, Retro combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"../LLM-Images/Fig18-Retro-architecture.png\" alt=\"Fig. 18: Retro architecture. Left: simplified version where a sequence of length n = 12 is split into l = 3 chunks of size m = 4. For each chunk, we retrieve k = 2 neighbours of r = 5 tokens each. The retrieval pathway is shown on top. Right: Details of the interactions in the CCA operator. Causality is maintained as neighbours of the first chunk only affect the last token of the first chunk and tokens from the second chunk. Courtesy of [82]..\" style=\"width:100%;\">\n",
    "  <figcaption style=\"text-align: center;\">Fig. 18: Retro architecture. Left: simplified version where a sequence of length n = 12 is split into l = 3 chunks of size m = 4. For each chunk, we retrieve k = 2 neighbours of r = 5 tokens each. The retrieval pathway is shown on top. Right: Details of the interactions in the CCA operator. Causality is maintained as neighbours of the first chunk only affect the last token of the first chunk and tokens from the second chunk. Courtesy of <a href=\"\">[82]</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "GLaM: In [84], Du et al. proposed a family of LLMs named GLaM (Generalist Language Model), which use a sparsely activated mixture-of-experts architecture to scale the model capacity while also incurring substantially less training cost compared to dense variants. The largest GLaM has 1.2 trillion parameters, which is approximately 7x larger than GPT- 3. It consumes only 1/3 of the energy used to train GPT-3 and requires half of the computation flops for inference, while still achieving better overall zero, one and few-shot performance across 29 NLP tasks. Fig 19 shows the high-level architecture of GLAM.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"../LLM-Images/Fig19-GLaM-model-architecture.png\" alt=\"Fig. 19: GLaM model architecture. Each MoE layer (the bottom block) is interleaved with a Transformer layer (the upper block). Courtesy of [84].\" style=\"width:100%;\">\n",
    "  <figcaption style=\"text-align: center;\">Fig. 19: GLaM model architecture. Each MoE layer (the bottom block) is interleaved with a Transformer layer (the upper block). Courtesy of <a href=\"\">[84]</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "LaMDA: In [85], Thoppilan et al. presented LaMDA, a family of Transformer-based neural language models specialized for dialog, which have up to 137B parameters and are pre-trained on 1.56T words of public dialog data and web text. They showed that fine-tuning with annotated data and enabling the model to consult external knowledge sources can lead to significant improvements towards the two key challenges of safety and factual grounding.\n",
    "\n",
    "OPT: In [86], Zhang et al. presented Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained trans- formers ranging from 125M to 175B parameters, which they share with researchers. The OPT models’ parameters are shown in 20\n",
    "\n",
    "<figure>\n",
    "  <img src=\"../LLM-Images/Fig20-Different-OPT-Models-architecture.png\" alt=\"Fig. 20: Different OPT Models’ architecture details. Courtesy of [86].\" style=\"width:100%;\">\n",
    "  <figcaption style=\"text-align: center;\">Fig. 20: Different OPT Models’ architecture details. Courtesy of [86]<a href=\"\">[86]</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "Chinchilla: In [2], Hoffmann et al. investigated the optimal model size and number of tokens for training a transformer language model under a given compute budget. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, they found that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. They tested this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4% more more data.\n",
    "\n",
    "Galactica: In [87], Taylor et al. introduced Galactica, a large language model that can store, combine and reason about scientific knowledge. They trained on a large scientific corpus of papers, reference material, knowledge bases and many other sources. Galactica performed well on reasoning, outperforming Chinchilla on mathematical MMLU by 41.3% to 35.7%, and PaLM 540B on MATH with a score of 20.4% versus 8.8%.\n",
    "\n",
    "CodeGen: In [88], Nijkamp et al. trained and released a family of large language models up to 16.1B parameters, called CODEGEN, on natural language and programming language data, and open sourced the training library JAX- FORMER. They showed the utility of the trained model by demonstrating that it is competitive with the previous state-of- the-art on zero-shot Python code generation on HumanEval. They further investigated the multi-step paradigm for program synthesis, where a single program is factorized into multi- ple prompts specifying sub-problems. They also constructed an open benchmark, Multi-Turn Programming Benchmark (MTPB), consisting of 115 diverse problem sets that are factorized into multi-turn prompts.\n",
    "\n",
    "AlexaTM: In [89], Soltan et al. demonstrated that mul- tilingual large-scale sequence-to-sequence (seq2seq) models, pre-trained on a mixture of denoising and Causal Language Modeling (CLM) tasks, are more efficient few-shot learners than decoder-only models on various task. They trained a 20 billion parameter multilingual seq2seq model called Alexa Teacher Model (AlexaTM 20B) and showed that it achieves state-of-the-art (SOTA) performance on 1-shot summarization tasks, outperforming a much larger 540B PaLM decoder model. AlexaTM consist of 46 encoder layers, 32 decoder layers, 32 attention heads, and dmodel = 4096.\n",
    "\n",
    "Sparrow: In [90], Glaese et al. presented Sparrow, an information-seeking dialogue agent trained to be more helpful, correct, and harmless compared to prompted language model baselines. They used reinforcement learning from human feed- back to train their models with two new additions to help human raters judge agent behaviour. The high-level pipeline of Sparrow model is shown in Fig 21.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"../LLM-Images/Fig21-Sparrow-pipeline.png\" alt=\"Fig. 21: Sparrow pipeline relies on human participation to continually expand a training set. Courtesy of [90].\" style=\"width:100%;\">\n",
    "  <figcaption style=\"text-align: center;\">Fig. 21: Sparrow pipeline relies on human participation to continually expand a training set. Courtesy of <a href=\"\">[90]</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "Minerva: In [91], Lewkowycz et al. introduced Minerva, a large language model pretrained on general natural language data and further trained on technical content, to tackle previous LLM struggle with quantitative reasoning (such as solving mathematics, science, and engineering problems).\n",
    "\n",
    "MoD: In [92], Tay et al. presented a generalized and unified perspective for self-supervision in NLP and show how different pre-training objectives can be cast as one another and how interpolating between different objectives can be effective. They proposed Mixture-of-Denoisers (MoD), a pre- training objective that combines diverse pre-training paradigms together. This framework is known as Unifying Language Learning (UL2). An overview of UL2 pretraining paradigm is shown in Fig 21.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"../LLM-Images/Fig22-An-overview-of-UL2-pretraining-paradigm.png\" alt=\"Fig. 22: An overview of UL2 pretraining paradigm. Courtesy of [92].\" style=\"width:100%;\">\n",
    "  <figcaption style=\"text-align: center;\">Fig. 22: An overview of UL2 pretraining paradigm. Courtesy of  <a href=\"\">[92]</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "BLOOM: In [93], Scao et al. presented BLOOM, a 176B- parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). An overview of BLOOM architecture is shown in Fig 23.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"../LLM-Images/Fig23-An-overview-of-BLOOM-architecture.png\" alt=\"Fig. 23: An overview of BLOOM architecture. Courtesy of [93].\" style=\"width:100%;\">\n",
    "  <figcaption style=\"text-align: center;\">Fig. 23: An overview of BLOOM architecture. Courtesy of <a href=\"\">[93]</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "GLM: In [94], Zeng et al. introduced GLM-130B, a bilingual (English and Chinese) pre-trained language model with 130 billion parameters. It was an attempt to open-source a 100B-scale model at least as good as GPT-3 (davinci) and unveil how models of such a scale can be successfully pre- trained.\n",
    "\n",
    "Pythia: In [95], Biderman et al. introduced Pythia, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study.\n",
    "\n",
    "Orca: In [96], Mukherjee et al. develop Orca, a 13-billion parameter model that learns to imitate the reasoning process of large foundation models. Orca learns from rich signals from GPT-4 including explanation traces; step-by-step thought processes; and other complex instructions, guided by teacher assistance from ChatGPT.\n",
    "\n",
    "StarCoder: In [97], Li et al. introduced StarCoder and StarCoderBase. They are 15.5B parameter models with 8K context length, infilling capabilities and fast large-batch in- ference enabled by multi-query attention. StarCoderBase is trained on one trillion tokens sourced from The Stack, a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process. They fine-tuned StarCoderBase on 35B Python tokens, resulting in the creation of StarCoder. They performed the most comprehensive evalu- ation of Code LLMs to date and showed that StarCoderBase outperforms every open Code LLM that supports multiple pro- gramming languages and matches or outperforms the OpenAI code-cushman-001 model.\n",
    "\n",
    "KOSMOS: In [98], Huang et al. introduced KOSMOS-1, a Multimodal Large Language Model (MLLM) that can per- ceive general modalities, learn in context (i.e., few-shot), and follow instructions (i.e. zero-shot). Specifically, they trained KOSMOS-1 from scratch on web-scale multi-modal corpora, including arbitrarily interleaved text and images, image-caption pairs, and text data. Experimental results show that KOSMOS- 1 achieves impressive performance on (i) language understand- ing, generation, and even OCR-free NLP (directly fed with document images), (ii) perception-language tasks, including multimodal dialogue, image captioning, visual question an- swering, and (iii) vision tasks, such as image recognition with descriptions (specifying classification via text instructions).\n",
    "\n",
    "Gemini: In [99], Gemini team introduced a new family of multimodal models, that exhibit promising capabilities across image, audio, video, and text understanding. Gemini family includes three versions: Ultra for highly-complex tasks, Pro for enhanced performance and deployability at scale, and Nano for on-device applications. Gemini architecture is built on top of Transformer decoders, and is trained to support 32k context length (via using efficient attention mechanisms).\n",
    "\n",
    "Some of the other popular LLM frameworks (or techniques used for efficient developments of LLMs) includes Inner- Monologue [100], Megatron-Turing NLG [101], LongFormer [102], OPT-IML [103], MeTaLM [104], Dromedary [105],\n",
    "Palmyra [106], Camel [107], Yalm [108], MPT [109], ORCA- 2 [110], Gorilla [67], PAL [111], Claude [112], CodeGen 2 [113], Zephyr [114], Grok [115], Qwen [116], Mamba [30], Mixtral-8x7B [117], DocLLM [118], DeepSeek-Coder [119], FuseLLM-7B [120], TinyLlama-1.1B [121], LLaMA-Pro-8B [122].\n",
    "\n",
    "Fig 24 provides an overview of some of the most representative LLM frameworks, and the relevant works that have contributed to the success of LLMs and helped to push the limits of LLMs.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"../LLM-Images/Fig24-Timeline-of-some-of-the-most-representative-LLM-frameworks.png\" alt=\"Fig. 24: Timeline of some of the most representative LLM frameworks (so far). In addition to large language models with our #parameters threshold, we included a few representative works, which pushed the limits of language models, and paved the way for their success (e.g. vanilla Transformer, BERT, GPT-1), as well as some small language models. ♣ shows entities that serve not only as models but also as approaches. ♦ shows only approaches..\" style=\"width:100%;\">\n",
    "  <figcaption style=\"text-align: center;\">Fig. 24: Timeline of some of the most representative LLM frameworks (so far). In addition to large language models with our #parameters threshold, we included a few representative works, which pushed the limits of language models, and paved the way for their success (e.g. vanilla Transformer, BERT, GPT-1), as well as some small language models. ♣ shows entities that serve not only as models but also as approaches. ♦ shows only approaches. <a href=\"\">[?]</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29162cef-17d9-49f9-9835-806c155861a5",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">\n",
    "\n",
    "## **III. HOW LLMS ARE BUILT** <a class=\"anchor\" id=\"LLMs_page_3\"></a>\n",
    "\n",
    "[Back to Top](#LLMs_toc)\n",
    "\n",
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc96832f-0138-4ead-a01a-7d59c873b563",
   "metadata": {},
   "source": [
    "In this section, we first review the popular architectures used for LLMs, and then discuss data and modeling techniques ranging from data preparation, tokenization, to pre-training, instruction tuning, and alignment.\n",
    "Once the model architecture is chosen, the major steps involved in training an LLM includes: data preparation (col- lection, cleaning, deduping, etc.), tokenization, model pre- training (in a self-supervised learning fashion), instruction tuning, and alignment. We will explain each of them in a separate subsection below. These steps are also illustrated in Fig 25.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"../LLM-Images/Fig25-This-figure-shows-different-components-of-LLMs.png\" alt=\"Fig. 25: This figure shows different components of LLMs.\" style=\"width:100%;\">\n",
    "  <figcaption style=\"text-align: center;\">Fig. 25: This figure shows different components of LLMs. <a href=\"\">[]</a></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc710d29-231e-4642-ad5c-7db9edb63942",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">\n",
    "\n",
    "## **A. Dominant LLM Architectures** <a class=\"anchor\" id=\"LLMs_page_3.1\"></a>\n",
    "\n",
    "[Back to Top](#LLMs_toc)\n",
    "\n",
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fb6fdb-194b-4507-b8fa-3a77a98dbcfc",
   "metadata": {},
   "source": [
    "The most widely used LLM architectures are encoder-only, decoder-only, and encoder-decoder. Most of them are based on Transformer (as the building block). Therefore we also review the Transformer architecture here.\n",
    "\n",
    "1) Transformer: in a ground-breaking work [44], Vaswani et al. proposed the Transformer framework, which was originally designed for effective parallel computing using GPUs. The heart of Transformer is the (self-)attention mechanism, which can capture long-term contextual information much more effectively using GPUs than the recurrence and convolution mechanisms. Fig 26 provides a high-level overview of transformer work. In this section we provide an overview of the main elements and variants, see [44], [123] for more details.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"../LLM-Images/Fig26-High-level-overview-of-transformer-work.png\" alt=\"Fig. 26: High-level overview of transformer work. Courtesy of [44].\" style=\"width:100%;\">\n",
    "  <figcaption style=\"text-align: center;\">Fig. 26: High-level overview of transformer work. Courtesy of <a href=\"\">[44]</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "The Transformer language model architecture, originally proposed for machine translation, consists of an encoder and a decoder. The encoder is composed of a stack of N = 6 identical Transformer layers. Each layer has two sub-layers. The first one is a multi-head self-attention layer, and the other one is a simple position-wise fully connected feed-forward network. The decoder is composed of a stack of 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder has a third sub-layer, which performs multi-head attention over the output of the encoder stack. The attention function can be described as mapping a query and a set of key- value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. Instead of performing a single attention function with dmodel dimensional keys, values and queries, it is found to be beneficial to linearly project the queries, keys and values h with different, learned linear projections to dk, dk and dv dimensions, respectively. Positional encoding is incorporated to fuse information about the relative or absolute position of the tokens in the sequence.\n",
    "\n",
    "2) Encoder-Only: For this family, at each stage, the atten- tion layers can access all the words in the initial sentence. The pre-training of these models usually consist of some- how corrupting a given sentence (for instance, by masking random words in it) and tasking the model with finding or reconstructing the initial sentence. Encoder models are great for tasks requiring an understanding of the full sequence, such as sentence classification, named entity recognition, and extractive question answering. One prominent encoder only model is BERT (Bidirectional Encoder Representations from Transformers), proposed in [24].\n",
    "\n",
    "3) Decoder-Only: For these models, at each stage, for any word, the attention layers can only access the words positioned before that in the sentence. These models are also sometimes called auto-regressive models. The pretraining of these models is usually formulated as predicting the next word (or token) in the sequence. The decoder-only models are best suited for tasks involving text generation. GPT models are prominent example of this model category.\n",
    "\n",
    "4) Encoder-Decoder: These models use both encoder and decoder, and are sometimes called sequence-to-sequence mod- els. At each stage, the attention layers of the encoder can access all the words in the initial sentence, whereas the attention layers of the decoder only accesses the words positioned before a given word in the input. These models are usually pre- trained using the objectives of encoder or decoder models, but usually involve something a bit more complex. For instance, some models are pretrained by replacing random spans of text (that can contain several words) with a single mask special word, and the objective is then to predict the text that this mask word replaces. Encoder-decoder models are best suited for tasks about generating new sentences conditioned on a given input, such as summarization, translation, or generative question answering.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefc446e-3e47-4dc8-9a94-cd3f7ee5f8d4",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">\n",
    "\n",
    "## **B. Data Cleaning** <a class=\"anchor\" id=\"LLMs_page_3.2\"></a>\n",
    "\n",
    "[Back to Top](#LLMs_toc)\n",
    "\n",
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1cc92a-7b0c-4ee6-aefb-01aa06d529dd",
   "metadata": {},
   "source": [
    "Data quality is crucial to the performance of language models trained on them. Data cleaning techniques such as filtering, deduplication, are shown to have a big impact on the model performance.\n",
    "As an example, in Falcon40B [124], Penedo et al. showed that properly filtered and deduplicated web data alone can lead to powerful models; even significantly outperforming models from the state-of-the-art trained on The Pile. Despite extensive filtering, they were able to obtain five trillion tokens from CommonCrawl. They also released an extract of 600 billion tokens from our REFINEDWEB dataset, and 1.3/7.5B param- eters language models trained on it. 27 shows the Refinement process of CommonCrawl data by this work.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"../LLM-Images/Fig27-Subsequent-stages.png\" alt=\"Fig. 27: Subsequent stages of Macrodata Refinement remove nearly 90% of the documents originally in CommonCrawl. Courtesy of [124].\" style=\"width:100%;\">\n",
    "  <figcaption style=\"text-align: center;\">Fig. 27: Subsequent stages of Macrodata Refinement remove nearly 90% of the documents originally in CommonCrawl. Courtesy of <a href=\"\">[124]</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "1) Data Filtering: Data filtering aims to enhance the qual- ity of training data and the effectiveness of the trained LLMs. Common data filtering techniques include:\n",
    "Removing Noise: refers to eliminating irrelevant or noisy data that might impact the model’s ability to generalize well. As an example, one can think of removing false information from the training data, to lower the chance of model generating false responses. Two mainstream approaches for quality filter- ing includes: classifier-based, and heuristic-based frameworks.\n",
    "\n",
    "    - Handling Outliers: Identifying and handling outliers or anomalies in the data to prevent them from disproportionately influencing the model.\n",
    "    - Addressing Imbalances: Balancing the distribution of classes or categories in the dataset to avoid biases and ensure fair representation. This is specially useful for responsible model training and evaluation.\n",
    "    - Text Preprocessing: Cleaning and standardizing text data by removing stop words, punctuation, or other elements that may not contribute significantly to the model’s learning.\n",
    "    - Dealing with Ambiguities: Resolving or excluding am- biguous or contradictory data that might confuse the model during training. This can help the model to provide more definite and reliable answers.\n",
    "2) Deduplication: De-duplication refers to the process of removing duplicate instances or repeated occurrences of the same data in a dataset. Duplicate data points can introduce biases in the model training process and reduce the diversity, as the model may learn from the same examples multiple times, potentially leading to overfitting on those particular instances. Some works [125] have shown that de-duplication improves models’ ability to generalize to new, unseen data.\n",
    "\n",
    "The de-duplication process is particularly important when dealing with large datasets, as duplicates can unintentionally inflate the importance of certain patterns or characteristics. This is especially relevant in NLP tasks, where diverse and representative training data is crucial for building robust lan- guage models.\n",
    "\n",
    "The specific de-duplication method can vary based on the nature of the data and the requirements of the particular language model being trained. It may involve comparing entire data points or specific features to identify and eliminate duplicates. At the document level, existing works mainly rely on the overlap ratio of high-level features (e.g. n-grams overlap) between documents to detect duplicate samples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0aac49-b14f-4cc8-b536-9773dee2d009",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">\n",
    "\n",
    "## **C. Tokenizations** <a class=\"anchor\" id=\"LLMs_page_3.3\"></a>\n",
    "\n",
    "[Back to Top](#LLMs_toc)\n",
    "\n",
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49900846-01b4-4166-a0c4-2af84d80450b",
   "metadata": {},
   "source": [
    "Tokenization referes to the process of converting a sequence of text into smaller parts, known as tokens. While the simplest tokenization tool simply chops text into tokens based on white space, most tokenization tools rely on a word dictionary. However, out-of-vocabulary (OOV) is a problem in this case because the tokenizer only knows words in its dictionary. To increase the coverage of dictionaries, popular tokenizers used for LLMs are based on sub-words, which can be combined to form a large number of words, including the words unseen in training data or words in different languages. In what follows, we describe three popular tokenizers.\n",
    "\n",
    "1) BytePairEncoding: BytePairEncoding is originally a type of data compression algorithm that uses frequent patterns at byte level to compress the data. By definition, this algorithm mainly tries to keep the frequent words in their original form and break down ones that are not common. This simple paradigm keeps the vocabulary not very large, but also good enough to represent common words at the same time. Also morphological forms of the frequent words can be represented very well if suffix or prefix is also commonly presented in the training data of the algorithm.\n",
    "\n",
    "2) WordPieceEncoding: This algorithm is mainly used for very well-known models such as BERT and Electra. At the beginning of training, the algorithm takes all the alphabet from the training data to make sure that nothing will be left as UNK or unknown from the training dataset. This case happens when the model is given an input that can not be tokenized by the tokenizer. It mostly happens in cases where some characters are not tokenizable by it. Similar to BytePairEncoding, it tries to maximize the likelihood of putting all the tokens in vocabulary based on their frequency.\n",
    "\n",
    "3) SentencePieceEncoding: Although both tokenizers described before are strong and have many advantages compared to white-space tokenization, they still take assumption of words being always separated by white-space as granted. This assumption is not always true, in fact in some languages, words can be corrupted by many noisy elements such as unwanted spaces or even invented words. SentencePieceEncoding tries to address this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec469c8a-fea2-4485-8999-1b82d3778cf2",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">\n",
    "\n",
    "## **D. Positional Encoding** <a class=\"anchor\" id=\"LLMs_page_3.4\"></a>\n",
    "\n",
    "[Back to Top](#LLMs_toc)\n",
    "\n",
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a36ff9a-d4aa-4317-8374-9ddf00ef37e4",
   "metadata": {},
   "source": [
    "1) Absolute Positional Embeddings: (APE) [44] has been used in the original Transformer model to preserve the infor- mation of sequence order. Therefore, the positional information of words is added to the input embeddings at the bottom of both the encoder and decoder stacks. There are various options for positional encodings, either learned or fixed. In the vanilla Transformer, sine and cosine functions are employed for this purpose. The main drawback of using APE in Transformers is the restriction to a certain number of tokens. Additionally, APE fails to account for the relative distances between tokens.\n",
    "\n",
    "2) Relative Positional Embeddings: (RPE) [126] involves extending self-attention to take into account the pairwise links between input elements. RPE is added to the model at two levels: first as an additional component to the keys, and subsequently as a sub-component of the values matrix. This approach looks at the input as a fully-connected graph with labels and directed edges. In the case of linear sequences, edges can capture information about the relative position differences between input elements. A clipping distance, represented as k 2 ≤ k ≤ n − 4, specifies the maximum limit on relative lo- cations. This allows the model to make reasonable predictions for sequence lengths that are not part of the training data.\n",
    "\n",
    "3) Rotary Position Embeddings: Rotary Positional Em- bedding (RoPE) [127] tackles problems with existing ap- proaches. Learned absolute positional encodings can lack gen- eralizability and meaningfulness, particularly when sentences are short. Moreover, current methods like T5’s positional embedding face challenges with constructing a full attention matrix between positions. RoPE uses a rotation matrix to encode the absolute position of words and simultaneously in- cludes explicit relative position details in self-attention. RoPE brings useful features like flexibility with sentence lengths, a decrease in word dependency as relative distances increase, and the ability to improve linear self-attention with relative position encoding. GPT-NeoX-20B, PaLM, CODEGEN, and LLaMA are among models that take advantage of RoPE in their architectures.\n",
    "\n",
    "4) Relative Positional Bias: The concept behind this type of positional embedding is to facilitate extrapolation during inference for sequences longer than those encountered in train- ing. In [128] Press et al. proposed Attention with Linear Biases (ALiBi). Instead of simply adding positional embeddings to word embeddings, they introduced a bias to the attention scores of query-key pairs, imposing a penalty proportional to their distance. In the BLOOM model, ALiBi is leveraged."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bbdbb8-4cde-49fc-880c-15fd84680294",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">\n",
    "\n",
    "## **E. Model Pre-training** <a class=\"anchor\" id=\"LLMs_page_3.5\"></a>\n",
    "\n",
    "[Back to Top](#LLMs_toc)\n",
    "\n",
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6116c57f-0192-47ed-afe6-f2650e808d59",
   "metadata": {},
   "source": [
    "Pre-training is the very first step in large language model training pipeline, and it helps LLMs to acquire fundamental language understanding capabilities, which can be useful in a wide range of language related tasks. During pre-training, the LLM is trained on a massive amount of (usually) unlabeled texts, usually in a self-supervised manner. There are different approaches used for pre-training like next sentence prediction [24], two most common ones include, next token prediction (autoregressive language modeling), and masked language modeling.\n",
    "\n",
    "In Autoregressive Language Modeling framework, given a sequence of n tokens x1, ..., xn, the model tries to predict next token xn+1 (and sometimes next sequence of tokens) in an auto-regressive fashion. One popular loss function in this case is the log-likelihood of predicted tokens as shown in Eq 2\n",
    "\n",
    "N\n",
    "LALM (x) = X p(xi+n|xi, ..., xi+n−1) (1)\n",
    "i=1\n",
    "\n",
    "<figure>\n",
    "  <img src=\"../LLM-Images/eq1.png\" alt=\"Equation 1\" style=\"width:100%;\">\n",
    "  <figcaption style=\"text-align: center;\">Equation 1 <a href=\"\">[eq1]</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "Given the auto-regressive nature of this framework, the decoder-only models are naturally better suited to learn how to accomplish these task.\n",
    "In Masked Language Modeling, some words are masked in a sequence and the model is trained to predict the masked words based on the surrounding context. Sometimes people refer to this approach as denoising autoencoding, too. If we denote the masked/corrupted samples in the sequence x, as x ̃, then the training objective of this approach can be written as:\n",
    "\n",
    "LM LM (x) =\n",
    "XN\n",
    "i=1\n",
    "p(x ̃|x\\x ̃) (2)\n",
    "\n",
    "<figure>\n",
    "  <img src=\"../LLM-Images/eq1.png\" alt=\"Equation 2\" style=\"width:100%;\">\n",
    "  <figcaption style=\"text-align: center;\">Equation 2 <a href=\"\">[eq1]</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "And more recently, Mixture of Experts (MoE) [130], [131] have become very popular in LLM space too. MoEs enable models to be pre-trained with much less compute, which means one can dramatically scale up the model or dataset size with the same compute budget as a dense model. MoE consists of two main elements: Sparse MoE layers, which are used instead of dense feed-forward network (FFN) layers, and have a certain number of “experts” (e.g. 8), in which each expert is a neural network. In practice, the experts are FFNs, but they can also be more complex networks. A gate network or router, that determines which tokens are sent to which expert. It is worth noting that, one can send a token to more than one expert. How to route a token to an expert is one of the big decisions when working with MoEs - the router is composed of learned parameters and is pretrained at the same time as the rest of the network. Fig 29 provides an illustration of a Switch Transformer encoder block, which are used in MoE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4903e27f-2c5f-4924-8ecd-0ef2d4c2c9d3",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">\n",
    "\n",
    "## **F. Fine-tuning and Instruction Tuning** <a class=\"anchor\" id=\"LLMs_page_3.6\"></a>\n",
    "\n",
    "[Back to Top](#LLMs_toc)\n",
    "\n",
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d4fff8-cb10-4d24-a67b-dd2e74113cb1",
   "metadata": {},
   "source": [
    "Early language models such as BERT trained using self-supervision as explained in section III-E were not able to perform specific tasks. In order for the foundation model to be useful it needed to be fine-tuned to a specific task with labeled data (so-called supervised fine-tuning or SFT for short). For example, in the original BERT paper [24], the model was fine- tuned to 11 different tasks. While more recent LLMs no longer require fine-tuning to be used, they can still benefit from task or data-specific fine-tuning. For example, OpenAI reports that the much smaller GPT-3.5 Turbo model can outperform GPT-4 when fine-tuned with task specific data <sup>2</sup>.\n",
    "\n",
    "<sup>2</sup> https://platform.openai.com/docs/guides/fine-tuning\n",
    "\n",
    "\n",
    "\n",
    "<figure>\n",
    "  <img src=\"../LLM-Images/Fig28-Various-positional-encodings-are-employed-in-LLMs.png\" alt=\"Fig. 28: Various positional encodings are employed in LLMs.\" style=\"width:100%;\">\n",
    "  <figcaption style=\"text-align: center;\">Fig. 28: Various positional encodings are employed in LLMs <a href=\"\">[various]</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "<figure>\n",
    "  <img src=\"../LLM-Images/Fig17-High-level-model-architecture-of-ERNIE.png\" alt=\"Fig. 17: High-level model architecture of ERNIE 3.0. Courtesy of [81].\" style=\"width:100%;\">\n",
    "  <figcaption style=\"text-align: center;\">Fig. 17: High-level model architecture of ERNIE 3.0. Courtesy of <a href=\"\">[81]</a></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b007c2-1a17-481f-abd4-f5eca29efbc0",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">\n",
    "\n",
    "## **REFERENCES** <a class=\"anchor\" id=\"LLMs_page_8\"></a>\n",
    "\n",
    "[Back to Top](#LLMs_toc)\n",
    "\n",
    "<hr style=\"height:3px;border-width:0;color:Blue;background-color:Blue\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff19f400-8644-44f5-b202-b6cd9196ce67",
   "metadata": {},
   "source": [
    "- [1] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei, “Scaling laws for neural language models,” arXiv preprint arXiv:2001.08361, 2020.\n",
    "- [2] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. d. L. Casas, L. A. Hendricks, J. Welbl, A. Clark et al., “Training compute-optimal large language models,” arXiv preprint arXiv:2203.15556, 2022.\n",
    "- [3] C. E. Shannon, “Prediction and entropy of printed english,” Bell system technical journal, vol. 30, no. 1, pp. 50–64, 1951.\n",
    "- [4] F. Jelinek, Statistical methods for speech recognition. 1998. MIT press,1998.\n",
    "- [5] C. Manning and H. Schutze, Foundations of statistical natural language processing. MIT press, 1999.\n",
    "- [6] C. D. Manning, An introduction to information retrieval. Cambridge university press, 2009.\n",
    "- [7] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong et al., “A survey of large language models,” arXiv preprint arXiv:2303.18223, 2023.\n",
    "- [8] C. Zhou, Q. Li, C. Li, J. Yu, Y. Liu, G. Wang, K. Zhang, C. Ji, Q. Yan, L. He et al., “A comprehensive survey on pretrained foundation mod- els: A history from bert to chatgpt,” arXiv preprint arXiv:2302.09419, 2023.\n",
    "- [9] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig, “Pre- train, prompt, and predict: A systematic survey of prompting methods in natural language processing,” ACM Computing Surveys, vol. 55, no. 9, pp. 1–35, 2023.\n",
    "- [10] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun, J. Xu, and Z. Sui, “A survey for in-context learning,” arXiv preprint arXiv:2301.00234, 2022.\n",
    "- [11] J. Huang and K. C.-C. Chang, “Towards reasoning in large language models: A survey,” arXiv preprint arXiv:2212.10403, 2022.\n",
    "- [12] S. F. Chen and J. Goodman, “An empirical study of smoothing techniques for language modeling,” Computer Speech & Language, vol. 13, no. 4, pp. 359–394, 1999.\n",
    "- [13] Y. Bengio, R. Ducharme, and P. Vincent, “A neural probabilistic language model,” Advances in neural information processing systems, vol. 13, 2000.\n",
    "- [14] H.Schwenk,D.De ́chelotte,andJ.-L.Gauvain,“Continuousspace language models for statistical machine translation,” in Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, 2006, pp. 723–730.\n",
    "- [15] T. Mikolov, M. Karafia ́t, L. Burget, J. Cernocky , and S. Khudanpur, “Recurrent neural network based language model.” in Interspeech, vol. 2, no. 3. Makuhari, 2010, pp. 1045–1048.\n",
    "- [16] A. Graves, “Generating sequences with recurrent neural networks,” arXiv preprint arXiv:1308.0850, 2013.\n",
    "- [17] P.-S. Huang, X. He, J. Gao, L. Deng, A. Acero, and L. Heck, “Learning deep structured semantic models for web search using clickthrough data,” in Proceedings of the 22nd ACM international conference on Information & Knowledge Management, 2013, pp. 2333–2338.\n",
    "- [18] J. Gao, C. Xiong, P. Bennett, and N. Craswell, Neural Approaches to Conversational Information Retrieval. Springer Nature, 2023, vol. 44.\n",
    "- [19] I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learning with neural networks,” Advances in neural information processing systems, vol. 27, 2014.\n",
    "- [20] K. Cho, B. Van Merrie ̈nboer, D. Bahdanau, and Y. Bengio, “On the properties of neural machine translation: Encoder-decoder ap- proaches,” arXiv preprint arXiv:1409.1259, 2014.\n",
    "- [21] H. Fang, S. Gupta, F. Iandola, R. K. Srivastava, L. Deng, P. Dolla ́r, J. Gao, X. He, M. Mitchell, J. C. Platt et al., “From captions to visual concepts and back,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2015, pp. 1473–1482.\n",
    "- [22] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan, “Show and tell: A neural image caption generator,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2015, pp. 3156–3164.\n",
    "- [23] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer, “Deep contextualized word representations. corr abs/1802.05365 (2018),” arXiv preprint arXiv:1802.05365, 2018.\n",
    "- [24] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training of deep bidirectional transformers for language understanding,” arXiv preprint arXiv:1810.04805, 2018.\n",
    "- [25] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov, “Roberta: A robustly optimized bert pretraining approach,” arXiv preprint arXiv:1907.11692, 2019.\n",
    "- [26] P. He, X. Liu, J. Gao, and W. Chen, “Deberta: Decoding-enhanced bert with disentangled attention,” arXiv preprint arXiv:2006.03654, 2020.\n",
    "- [27] X. Han, Z. Zhang, N. Ding, Y. Gu, X. Liu, Y. Huo, J. Qiu, Y. Yao, A. Zhang, L. Zhang et al., “Pre-trained models: Past, present and future,” AI Open, vol. 2, pp. 225–250, 2021.\n",
    "- [28] X. Qiu, T. Sun, Y. Xu, Y. Shao, N. Dai, and X. Huang, “Pre-trained models for natural language processing: A survey,” Science China Technological Sciences, vol. 63, no. 10, pp. 1872–1897, 2020.\n",
    "- [29] A. Gu, K. Goel, and C. Re ́, “Efficiently modeling long sequences with structured state spaces,” 2022.\n",
    "- [30] A. Gu and T. Dao, “Mamba: Linear-time sequence modeling with selective state spaces,” arXiv preprint arXiv:2312.00752, 2023.\n",
    "- [31] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann et al., “Palm: Scaling language modeling with pathways,” arXiv preprint arXiv:2204.02311, 2022.\n",
    "- [32] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Roziere, N. Goyal, E. Hambro, F. Azhar et al., “Llama: Open and efficient foundation language models,” arXiv preprint arXiv:2302.13971, 2023.\n",
    "- [33] OpenAI, “GPT-4 Technical Report,” https://arxiv.org/pdf/2303.08774v3.pdf, 2023.\n",
    "- [34] J. Wei, X. Wang, D. Schuurmans, M. Bosma, b. ichter, F. Xia, E. Chi, Q. V. Le, and D. Zhou, “Chain-of-thought prompting elicits reasoning in large language models,” in Advances in Neural Information Processing Systems, S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, Eds., vol. 35. Curran Associates, Inc., 2022, pp. 24 824–24 837. [Online]. Available: https://proceedings.neurips.cc/paper files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf\n",
    "- [35] G. Mialon, R. Dessı, M. Lomeli, C. Nalmpantis, R. Pasunuru, R. Raileanu, B. Roziere, T. Schick, J. Dwivedi-Yu, A. Celikyil-maz et al., “Augmented language models: a survey,” arXiv preprint arXiv:2302.07842, 2023.\n",
    "- [36] B. Peng, M. Galley, P. He, H. Cheng, Y. Xie, Y. Hu, Q. Huang, L. Liden, Z. Yu, W. Chen, and J. Gao, “Check your facts and try again: Improving large language models with external knowledge and automated feedback,” arXiv preprint arXiv:2302.12813, 2023.\n",
    "- [37] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao, “React: Synergizing reasoning and acting in language models,” arXiv preprint arXiv:2210.03629, 2022.\n",
    "- [38] D. E. Rumelhart, G. E. Hinton, R. J. Williams et al., “Learning internal representations by error propagation,” 1985.\n",
    "- [39] J. L. Elman, “Finding structure in time,” Cognitive science, vol. 14, no. 2, pp. 179–211, 1990.\n",
    "- [40] M. V. Mahoney, “Fast text compression with neural networks.” in FLAIRS conference, 2000, pp. 230–234.\n",
    "- [41] T. Mikolov, A. Deoras, D. Povey, L. Burget, and J. Cˇernocky, “Strategies for training large scale neural network language models,” in 2011 IEEE Workshop on Automatic Speech Recognition & Understanding. IEEE, 2011, pp. 196–201.\n",
    "- [42] tmikolov. rnnlm. [Online]. Available: https://www.fit.vutbr.cz/∼imikolov/rnnlm/\n",
    "- [43] S. Minaee, N. Kalchbrenner, E. Cambria, N. Nikzad, M. Chenaghlu, and J. Gao, “Deep learning–based text classification: a comprehensive review,” ACM computing surveys (CSUR), vol. 54, no. 3, pp. 1–40, 2021.\n",
    "- [44] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in neural information processing systems, vol. 30, 2017.\n",
    "- [45] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut, “Albert: A lite bert for self-supervised learning of language represen- tations,” arXiv preprint arXiv:1909.11942, 2019.\n",
    "- [46] K. Clark, M.-T. Luong, Q. V. Le, and C. D. Manning, “Electra: Pre- training text encoders as discriminators rather than generators,” arXiv preprint arXiv:2003.10555, 2020.\n",
    "- [47] G. Lample and A. Conneau, “Cross-lingual language model pretrain- ing,” arXiv preprint arXiv:1901.07291, 2019.\n",
    "- [48] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov, and Q. V. Le, “Xlnet: Generalized autoregressive pretraining for language understanding,” Advances in neural information processing systems, vol. 32, 2019.\n",
    "- [49] L. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y. Wang, J. Gao, M. Zhou, and H.-W. Hon, “Unified language model pre-training for natural language understanding and generation,” Advances in neural information processing systems, vol. 32, 2019.\n",
    "- [50] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever et al., “Improv- ing language understanding by generative pre-training,” 2018.\n",
    "- [51] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al., “Language models are unsupervised multitask learners,” OpenAI blog, vol. 1, no. 8, p. 9, 2019.\n",
    "- [52] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu, “Exploring the limits of transfer learning with a unified text-to-text transformer,” The Journal of Machine Learning Research, vol. 21, no. 1, pp. 5485–5551, 2020.\n",
    "- [53] L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant, A. Barua, and C. Raffel, “mt5: A massively multilingual pre-trained text-to-text transformer,” arXiv preprint arXiv:2010.11934, 2020.\n",
    "- [54] K. Song, X. Tan, T. Qin, J. Lu, and T.-Y. Liu, “Mass: Masked sequence to sequence pre-training for language generation,” arXiv preprint arXiv:1905.02450, 2019.\n",
    "- [55] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, and L. Zettlemoyer, “Bart: Denoising sequence-to- sequence pre-training for natural language generation, translation, and comprehension,” arXiv preprint arXiv:1910.13461, 2019.\n",
    "- [56] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., “Language mod- els are few-shot learners,” Advances in neural information processing systems, vol. 33, pp. 1877–1901, 2020.\n",
    "- [57] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Ka- plan, H. Edwards, Y. Burda, N. Joseph, G. Brockman et al., “Evaluating large language models trained on code,” arXiv preprint arXiv:2107.03374, 2021.\n",
    "- [58] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, C. Hesse, S. Jain, V. Kosaraju, W. Saunders et al., “Webgpt: Browser- assisted question-answering with human feedback,” arXiv preprint arXiv:2112.09332, 2021.\n",
    "- [59] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray et al., “Training language models to follow instructions with human feedback,” Advances in Neural Information Processing Systems, vol. 35, pp. 27730–27744, 2022.\n",
    "- [60] OpenAI. (2022) Introducing chatgpt. [Online]. Available: https: //openai.com/blog/chatgpt\n",
    "- [61] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al., “Llama 2: Open foundation and fine-tuned chat models,” arXiv preprint arXiv:2307.09288, 2023.\n",
    "- [62] R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang, and T. B. Hashimoto, “Alpaca: A strong, replicable instruction- following model,” Stanford Center for Research on Foundation Mod- els. https://crfm. stanford. edu/2023/03/13/alpaca. html, vol. 3, no. 6, p. 7, 2023.\n",
    "- [63] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, “Qlora: Ef- ficient finetuning of quantized llms,” arXiv preprint arXiv:2305.14314, 2023.\n",
    "- [64] X. Geng, A. Gudibande, H. Liu, E. Wallace, P. Abbeel, S. Levine, and D. Song, “Koala: A dialogue model for academic research,” Blog post, April, vol. 1, 2023.\n",
    "- [65] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier et al., “Mistral 7b,” arXiv preprint arXiv:2310.06825, 2023.\n",
    "- [66] B. Roziere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, T. Remez, J. Rapin et al., “Code llama: Open foundation models for code,” arXiv preprint arXiv:2308.12950, 2023.\n",
    "- [67] S. G. Patil, T. Zhang, X. Wang, and J. E. Gonzalez, “Gorilla: Large language model connected with massive apis,” 2023.\n",
    "- [68] A. Pal, D. Karkhanis, M. Roberts, S. Dooley, A. Sundararajan, and S. Naidu, “Giraffe: Adventures in expanding context lengths in llms,” arXiv preprint arXiv:2308.10882, 2023.\n",
    "- [69] B. Huang, “Vigogne: French instruction-following and chat models,” https://github.com/bofenghuang/vigogne, 2023.\n",
    "- [70] Y. Wang, H. Ivison, P. Dasigi, J. Hessel, T. Khot, K. R. Chandu, D. Wadden, K. MacMillan, N. A. Smith, I. Beltagy et al., “How far can camels go? exploring the state of instruction tuning on open resources,” arXiv preprint arXiv:2306.04751, 2023.\n",
    "- [71] S. Tworkowski, K. Staniszewski, M. Pacek, Y. Wu, H. Michalewski, and P. Miłos ́, “Focused transformer: Contrastive training for context scaling,” arXiv preprint arXiv:2307.03170, 2023.\n",
    "- [72] D. Mahan, R. Carlow, L. Castricato, N. Cooper, and C. Laforte, “Stable beluga models.” [Online]. Available: [https://huggingface.co/stabilityai/StableBeluga2](https://huggingface.co/stabilityai/StableBeluga2)\n",
    "- [73] Y. Tay, J. Wei, H. W. Chung, V. Q. Tran, D. R. So, S. Shakeri, X. Gar- cia, H. S. Zheng, J. Rao, A. Chowdhery et al., “Transcending scaling laws with 0.1% extra compute,” arXiv preprint arXiv:2210.11399, 2022.\n",
    "- [74] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, Y. Li, X. Wang, M. Dehghani, S. Brahma et al., “Scaling instruction- finetuned language models,” arXiv preprint arXiv:2210.11416, 2022.\n",
    "- [75] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey, Z. Chen et al., “Palm 2 technical report,” arXiv preprint arXiv:2305.10403, 2023.\n",
    "- [76] K. Singhal, S. Azizi, T. Tu, S. S. Mahdavi, J. Wei, H. W. Chung, N. Scales, A. Tanwani, H. Cole-Lewis, S. Pfohl et al., “Large language models encode clinical knowledge,” arXiv preprint arXiv:2212.13138, 2022.\n",
    "- [77] K. Singhal, T. Tu, J. Gottweis, R. Sayres, E. Wulczyn, L. Hou, K. Clark, S. Pfohl, H. Cole-Lewis, D. Neal et al., “Towards expert- level medical question answering with large language models,” arXiv preprint arXiv:2305.09617, 2023.\n",
    "- [78] J. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le, “Finetuned language models are zero-shot learners,” arXiv preprint arXiv:2109.01652, 2021.\n",
    "- [79] J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song, J. Aslanides, S. Henderson, R. Ring, S. Young et al., “Scaling language models: Methods, analysis & insights from training gopher,” arXiv preprint arXiv:2112.11446, 2021.\n",
    "- [80] V. Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai, A. Chaffin, A. Stiegler, T. L. Scao, A. Raja et al., “Multi- task prompted training enables zero-shot task generalization,” arXiv preprint arXiv:2110.08207, 2021.\n",
    "- [81] Y. Sun, S. Wang, S. Feng, S. Ding, C. Pang, J. Shang, J. Liu, X. Chen, Y. Zhao, Y. Lu et al., “Ernie 3.0: Large-scale knowledge enhanced pre- training for language understanding and generation,” arXiv preprint arXiv:2107.02137, 2021.\n",
    "- [82] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Mil- lican, G. B. Van Den Driessche, J.-B. Lespiau, B. Damoc, A. Clark et al., “Improving language models by retrieving from trillions of tokens,” in International conference on machine learning. PMLR, 2022, pp. 2206–2240.\n",
    "- [83] O. Lieber, O. Sharir, B. Lenz, and Y. Shoham, “Jurassic-1: Technical details and evaluation,” White Paper. AI21 Labs, vol. 1, p. 9, 2021.\n",
    "- [84] N. Du, Y. Huang, A. M. Dai, S. Tong, D. Lepikhin, Y. Xu, M. Krikun, Y. Zhou, A. W. Yu, O. Firat et al., “Glam: Efficient scaling of language models with mixture-of-experts,” in International Conference on Machine Learning. PMLR, 2022, pp. 5547–5569.\n",
    "- [85] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.- T. Cheng, A. Jin, T. Bos, L. Baker, Y. Du et al., “Lamda: Language models for dialog applications,” arXiv preprint arXiv:2201.08239, 2022.\n",
    "- [86] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin et al., “Opt: Open pre-trained transformer language models,” arXiv preprint arXiv:2205.01068, 2022.\n",
    "- [87] R. Taylor, M. Kardas, G. Cucurull, T. Scialom, A. Hartshorn, E. Sar- avia, A. Poulton, V. Kerkez, and R. Stojnic, “Galactica: A large language model for science,” arXiv preprint arXiv:2211.09085, 2022.\n",
    "- [88] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese, and C. Xiong, “Codegen: An open large language model for code with multi-turn program synthesis,” arXiv preprint arXiv:2203.13474, 2022.\n",
    "- [89] S. Soltan, S. Ananthakrishnan, J. FitzGerald, R. Gupta, W. Hamza, H. Khan, C. Peris, S. Rawls, A. Rosenbaum, A. Rumshisky et al., “Alexatm 20b: Few-shot learning using a large-scale multilingual seq2seq model,” arXiv preprint arXiv:2208.01448, 2022.\n",
    "- [90] A. Glaese, N. McAleese, M. Trebacz, J. Aslanides, V. Firoiu, T. Ewalds, M. Rauh, L. Weidinger, M. Chadwick, P. Thacker et al., “Improving alignment of dialogue agents via targeted human judge- ments,” arXiv preprint arXiv:2209.14375, 2022.\n",
    "- [91] A. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski, V. Ramasesh, A. Slone, C. Anil, I. Schlag, T. Gutman-Solo et al.,“Solving quantitative reasoning problems with language models,” Advances in Neural Information Processing Systems, vol. 35, pp. 3843–3857, 2022.\n",
    "- [92] Y. Tay, M. Dehghani, V. Q. Tran, X. Garcia, D. Bahri, T. Schuster, H. S. Zheng, N. Houlsby, and D. Metzler, “Unifying language learning paradigms,” arXiv preprint arXiv:2205.05131, 2022.\n",
    "- [93] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilic ́, D. Hesslow, R. Castagne ́, A. S. Luccioni, F. Yvon, M. Galle ́ et al., “Bloom: A 176b- parameter open-access multilingual language model,” arXiv preprint arXiv:2211.05100, 2022.\n",
    "- [94] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y. Xu, W. Zheng, X. Xia et al., “Glm-130b: An open bilingual pre-trained model,” arXiv preprint arXiv:2210.02414, 2022.\n",
    "- [95] S. Biderman, H. Schoelkopf, Q. G. Anthony, H. Bradley, K. O’Brien, E. Hallahan, M. A. Khan, S. Purohit, U. S. Prashanth, E. Raff et al., “Pythia: A suite for analyzing large language models across train- ing and scaling,” in International Conference on Machine Learning. PMLR, 2023, pp. 2397–2430.\n",
    "- [96] S. Mukherjee, A. Mitra, G. Jawahar, S. Agarwal, H. Palangi, and A. Awadallah, “Orca: Progressive learning from complex explanation traces of gpt-4,” arXiv preprint arXiv:2306.02707, 2023.\n",
    "- [97] R. Li, L. B. Allal, Y. Zi, N. Muennighoff, D. Kocetkov, C. Mou, M. Marone, C. Akiki, J. Li, J. Chim et al., “Starcoder: may the source be with you!” arXiv preprint arXiv:2305.06161, 2023.\n",
    "- [98] S. Huang, L. Dong, W. Wang, Y. Hao, S. Singhal, S. Ma, T. Lv, L. Cui, O. K. Mohammed, Q. Liu et al., “Language is not all you need: Aligning perception with language models,” arXiv preprint arXiv:2302.14045, 2023.\n",
    "- [99] G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth et al., “Gemini: a family of highly capable multimodal models,” arXiv preprint arXiv:2312.11805, 2023.\n",
    "- [100] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch, Y. Chebotar et al., “Inner monologue: Embodied reasoning through planning with language models,” arXiv preprint arXiv:2207.05608, 2022.\n",
    "- [101] S. Smith, M. Patwary, B. Norick, P. LeGresley, S. Rajbhandari, J. Casper, Z. Liu, S. Prabhumoye, G. Zerveas, V. Korthikanti et al., “Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model,” arXiv preprint arXiv:2201.11990, 2022.\n",
    "- [102] I. Beltagy, M. E. Peters, and A. Cohan, “Longformer: The long- document transformer,” arXiv preprint arXiv:2004.05150, 2020.\n",
    "- [103] S. Iyer, X. V. Lin, R. Pasunuru, T. Mihaylov, D. Simig, P. Yu, K. Shus- ter, T. Wang, Q. Liu, P. S. Koura et al., “Opt-iml: Scaling language model instruction meta learning through the lens of generalization,” arXiv preprint arXiv:2212.12017, 2022.\n",
    "- [104] Y. Hao, H. Song, L. Dong, S. Huang, Z. Chi, W. Wang, S. Ma, and F. Wei, “Language models are general-purpose interfaces,” arXiv preprint arXiv:2206.06336, 2022.\n",
    "- [105] Z. Sun, Y. Shen, Q. Zhou, H. Zhang, Z. Chen, D. Cox, Y. Yang, and C. Gan, “Principle-driven self-alignment of language mod- els from scratch with minimal human supervision,” arXiv preprint arXiv:2305.03047, 2023.\n",
    "- [106] W. E. team, “Palmyra-base Parameter Autoregressive Language Model,” https://dev.writer.com, 2023.\n",
    "- [107] ——, “Camel-5b instructgpt,” https://dev.writer.com, 2023.\n",
    "- [108] Yandex. Yalm. [Online]. Available: https://github.com/yandex/ YaLM- 100B\n",
    "- [109] M. Team et al., “Introducing mpt-7b: a new standard for open-source, commercially usable llms,” 2023.\n",
    "- [110] A. Mitra, L. D. Corro, S. Mahajan, A. Codas, C. Simoes, S. Agarwal, X. Chen, A. Razdaibiedina, E. Jones, K. Aggarwal, H. Palangi, G. Zheng, C. Rosset, H. Khanpour, and A. Awadallah, “Orca 2: Teaching small language models how to reason,” 2023.\n",
    "- [111] L. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y. Yang, J. Callan, and G. Neubig, “Pal: Program-aided language models,” in International Conference on Machine Learning. PMLR, 2023, pp. 10 764–10 799.\n",
    "- [113] E. Nijkamp, H. Hayashi, C. Xiong, S. Savarese, and Y. Zhou, “Codegen2: Lessons for training llms on programming and natural languages,” arXiv preprint arXiv:2305.02309, 2023.\n",
    "- [114] L. Tunstall, E. Beeching, N. Lambert, N. Rajani, K. Rasul, Y. Belkada, S. Huang, L. von Werra, C. Fourrier, N. Habib et al., “Zephyr: Direct distillation of lm alignment,” arXiv preprint arXiv:2310.16944, 2023.\n",
    "- [115] X. team. Grok. [Online]. Available: https://grok.x.ai/\n",
    "- [116] J. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou, and J. Zhou, “Qwen-vl: A frontier large vision-language model with versatile abilities,” arXiv preprint arXiv:2308.12966, 2023.\n",
    "- [117] mixtral. mixtral. [Online]. Available: https://mistral.ai/news/ mixtral- of- experts/\n",
    "- [118] D. Wang, N. Raman, M. Sibue, Z. Ma, P. Babkin, S. Kaur, Y. Pei, A. Nourbakhsh, and X. Liu, “Docllm: A layout-aware generative language model for multimodal document understanding,” 2023.\n",
    "- [119] D. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen, X. Bi, Y. Wu, Y. K. Li, F. Luo, Y. Xiong, and W. Liang, “Deepseek-coder: When the large language model meets programming – the rise of code intelligence,” 2024.\n",
    "- [120] F. Wan, X. Huang, D. Cai, X. Quan, W. Bi, and S. Shi, “Knowledge fusion of large language models,” 2024.\n",
    "- [121] P. Zhang, G. Zeng, T. Wang, and W. Lu, “Tinyllama: An open-source small language model,” 2024.\n",
    "- [122] C. Wu, Y. Gan, Y. Ge, Z. Lu, J. Wang, Y. Feng, P. Luo, and Y. Shan, “Llama pro: Progressive llama with block expansion,” 2024.\n",
    "- [123] X. Amatriain, A. Sankar, J. Bing, P. K. Bodigutla, T. J. Hazen, and M. Kazi, “Transformer models: an introduction and catalog,” 2023.\n",
    "- [124] G. Penedo, Q. Malartic, D. Hesslow, R. Cojocaru, A. Cappelli, H. Alobeidli, B. Pannier, E. Almazrouei, and J. Launay, “The refined- web dataset for falcon llm: outperforming curated corpora with web data, and web data only,” arXiv preprint arXiv:2306.01116, 2023.\n",
    "- [125] D. Hernandez, T. Brown, T. Conerly, N. DasSarma, D. Drain, S. El- Showk, N. Elhage, Z. Hatfield-Dodds, T. Henighan, T. Hume et al., “Scaling laws and interpretability of learning from repeated data,” arXiv preprint arXiv:2205.10487, 2022.\n",
    "- [126] P. Shaw, J. Uszkoreit, and A. Vaswani, “Self-attention with relative position representations,” arXiv preprint arXiv:1803.02155, 2018.\n",
    "- [127] J. Su, Y. Lu, S. Pan, B. Wen, and Y. Liu, “Roformer: En- hanced transformer with rotary position embedding,” arXiv preprint arXiv:2104.09864, 2021.\n",
    "- [128] O. Press, N. A. Smith, and M. Lewis, “Train short, test long: Attention with linear biases enables input length extrapolation,” arXiv preprint arXiv:2108.12409, 2021.\n",
    "- [129] G. Ke, D. He, and T.-Y. Liu, “Rethinking positional encoding in language pre-training,” arXiv preprint arXiv:2006.15595, 2020.\n",
    "- [130] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean, “Outrageously large neural networks: The sparsely-gated mixture-of-experts layer,” arXiv preprint arXiv:1701.06538, 2017.\n",
    "- [131] W. Fedus, B. Zoph, and N. Shazeer, “Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity,” The Journal of Machine Learning Research, vol. 23, no. 1, pp. 5232–5270, 2022.\n",
    "- [132] R. K. Mahabadi, S. Ruder, M. Dehghani, and J. Henderson, “Parameter-efficient multi-task fine-tuning for transformers via shared hypernetworks,” 2021.\n",
    "- [133] S. Zhang, L. Dong, X. Li, S. Zhang, X. Sun, S. Wang, J. Li, R. Hu, T. Zhang, F. Wu, and G. Wang, “Instruction tuning for large language models: A survey,” 2023.\n",
    "- [134] S. Mishra, D. Khashabi, C. Baral, and H. Hajishirzi, “Cross-task generalization via natural language crowdsourcing instructions,” arXiv preprint arXiv:2104.08773, 2021.\n",
    "- [135] Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and H. Hajishirzi, “Self-instruct: Aligning language model with self generated instructions,” arXiv preprint arXiv:2212.10560, 2022.\n",
    "- [136] K. Ethayarajh, W. Xu, D. Jurafsky, and D. Kiela. Kto. [Online]. Available: https://github.com/ContextualAI/HALOs/blob/main/assets/ report.pdf\n",
    "- [137] P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei, “Deep reinforcement learning from human preferences,” Advances in neural information processing systems, vol. 30, 2017.\n",
    "- [138] H. Lee, S. Phatale, H. Mansoor, K. Lu, T. Mesnard, C. Bishop, V. Car- bune, and A. Rastogi, “Rlaif: Scaling reinforcement learning from human feedback with ai feedback,” arXiv preprint arXiv:2309.00267, 2023.\n",
    "- [139] R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and C. Finn, “Direct preference optimization: Your language model is secretly a reward model,” arXiv preprint arXiv:2305.18290, 2023.\n",
    "- [140] S. Rajbhandari, J. Rasley, O. Ruwase, and Y. He, “Zero: Memory optimizations toward training trillion parameter models,” in SC20: In- ternational Conference for High Performance Computing, Networking, Storage and Analysis. IEEE, 2020, pp. 1–16.\n",
    "- [141] B. Peng, E. Alcaide, Q. Anthony, A. Albalak, S. Arcadinho, H. Cao, X. Cheng, M. Chung, M. Grella, K. K. GV et al., “Rwkv: Reinventing rnns for the transformer era,” arXiv preprint arXiv:2305.13048, 2023.\n",
    "- [142] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, “Lora: Low-rank adaptation of large language models,” arXiv preprint arXiv:2106.09685, 2021.\n",
    "- [143] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural network,” arXiv preprint arXiv:1503.02531, 2015.\n",
    "- [144] J. Gou, B. Yu, S. J. Maybank, and D. Tao, “Knowledge distillation: A survey,” International Journal of Computer Vision, vol. 129, pp. 1789–1819, 2021.\n",
    "- [145] Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. J. Bang, A. Madotto, and P. Fung, “Survey of hallucination in natural language generation,” ACM Comput. Surv., vol. 55, no. 12, mar 2023. [Online]. Available: https://doi.org/10.1145/3571730\n",
    "- [146] N. McKenna, T. Li, L. Cheng, M. J. Hosseini, M. Johnson, and M. Steedman, “Sources of hallucination by large language models on inference tasks,” 2023.\n",
    "- [147] C.-Y. Lin, “ROUGE: A package for automatic evaluation of summaries,” in Text Summarization Branches Out. Barcelona, Spain: Association for Computational Linguistics, Jul. 2004, pp. 74–81. [Online]. Available: https://aclanthology.org/W04-1013\n",
    "- [148] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “Bleu: a method for automatic evaluation of machine translation,” in Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, P. Isabelle, E. Charniak, and D. Lin, Eds. Philadelphia, Pennsylvania, USA: Association for Computational Linguistics, Jul. 2002, pp. 311– 318. [Online]. Available: https://aclanthology.org/P02-1040\n",
    "- [149] B. Dhingra, M. Faruqui, A. Parikh, M.-W. Chang, D. Das, and W. Cohen, “Handling divergent reference texts when evaluating table-to-text generation,” in Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, A. Korhonen, D. Traum, and L. Marquez, Eds. Florence, Italy: Association for Computational Linguistics, Jul. 2019, pp. 4884–4895. [Online]. Available: https://aclanthology.org/P19-1483\n",
    "- [150] Z. Wang, X. Wang, B. An, D. Yu, and C. Chen, “Towards faithful neural table-to-text generation with content-matching constraints,” in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, D. Jurafsky, J. Chai, N. Schluter, and J. Tetreault, Eds. Online: Association for Computational Linguistics, Jul. 2020, pp. 1072–1086. [Online]. Available: https: //aclanthology.org/2020.acl-main.101\n",
    "- [151] H. Song, W.-N. Zhang, J. Hu, and T. Liu, “Generating persona consis- tent dialogues by exploiting natural language inference,” Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, no. 05, pp. 8878–8885, Apr. 2020.\n",
    "- [152] O. Honovich, L. Choshen, R. Aharoni, E. Neeman, I. Szpektor, and O. Abend, “q2: Evaluating factual consistency in knowledge- grounded dialogues via question generation and question answering,” in Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, M.-F. Moens, X. Huang, L. Specia, and S. W.-t. Yih, Eds. Online and Punta Cana, Dominican Republic: Association for Computational Linguistics, Nov. 2021, pp. 7856–7870. [Online]. Available: https://aclanthology.org/2021.emnlp-main.619\n",
    "- [153] N. Dziri, H. Rashkin, T. Linzen, and D. Reitter, “Evaluating attribution in dialogue systems: The BEGIN benchmark,” Transactions of the Association for Computational Linguistics, vol. 10, pp. 1066–1083, 2022. [Online]. Available: https://aclanthology.org/2022.tacl-1.62\n",
    "- [154] S. Santhanam, B. Hedayatnia, S. Gella, A. Padmakumar, S. Kim, Y. Liu, and D. Z. Hakkani-Tu ̈r, “Rome was built in 1776: A case study on factual correctness in knowledge-grounded response generation,” ArXiv, vol. abs/2110.05456, 2021.\n",
    "- [155] S. Min, K. Krishna, X. Lyu, M. Lewis, W. tau Yih, P. W. Koh, M. Iyyer, L. Zettlemoyer, and H. Hajishirzi, “Factscore: Fine-grained atomic evaluation of factual precision in long form text generation,” 2023.\n",
    "- [156] D. Sculley, G. Holt, D. Golovin, E. Davydov, T. Phillips, D. Ebner, V. Chaudhary, and M. Young, “Machine learning: The high interest credit card of technical debt,” in SE4ML: Software Engineering for Machine Learning (NIPS 2014 Workshop), 2014.\n",
    "- [157] Z. Zhang, A. Zhang, M. Li, and A. Smola, “Automatic chain of thought prompting in large language models,” 2022.\n",
    "- [158] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, and K. Narasimhan, “Tree of thoughts: Deliberate problem solving with large language models,” 2023.\n",
    "- [159] P. Manakul, A. Liusie, and M. J. F. Gales, “Selfcheckgpt: Zero- resource black-box hallucination detection for generative large lan- guage models,” 2023.\n",
    "- [160] N. Shinn, F. Cassano, E. Berman, A. Gopinath, K. Narasimhan, and S. Yao, “Reflexion: Language agents with verbal reinforcement learning,” 2023.\n",
    "- [161] S. J. Zhang, S. Florin, A. N. Lee, E. Niknafs, A. Marginean, A. Wang, K. Tyser, Z. Chin, Y. Hicke, N. Singh, M. Udell, Y. Kim, T. Buonassisi, A. Solar-Lezama, and I. Drori, “Exploring the mit mathematics and eecs curriculum using large language models,” 2023.\n",
    "- [162] T. Wu, E. Jiang, A. Donsbach, J. Gray, A. Molina, M. Terry, and C. J. Cai, “Promptchainer: Chaining large language model prompts through visual programming,” 2022.\n",
    "- [163] Y. Zhou, A. I. Muresanu, Z. Han, K. Paster, S. Pitis, H. Chan, and J. Ba, “Large language models are human-level prompt engineers,” 2023.\n",
    "- [164] P. S. H. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Ku ̈ttler, M. Lewis, W. Yih, T. Rockta ̈schel, S. Riedel, and D. Kiela, “Retrieval-augmented generation for knowledge-intensive NLP tasks,” CoRR, vol. abs/2005.11401, 2020. [Online]. Available: https://arxiv.org/abs/2005.11401\n",
    "- [165] Y. Gao, Y. Xiong, X. Gao, K. Jia, J. Pan, Y. Bi, Y. Dai, J. Sun, and H. Wang, “Retrieval-augmented generation for large language models: A survey,” arXiv preprint arXiv:2312.10997, 2023.\n",
    "- [166] A. W. Services. (Year of publication, e.g., 2023) Question answering using retrieval augmented generation with foundation models in amazon sagemaker jumpstart. Accessed: Date of access, e.g., December 5, 2023. [Online]. Available: https://shorturl.at/dSV47\n",
    "- [167] S. Pan, L. Luo, Y. Wang, C. Chen, J. Wang, and X. Wu, “Unifying large language models and knowledge graphs: A roadmap,” arXiv preprint arXiv:2306.08302, 2023.\n",
    "- [168] Z. Jiang, F. F. Xu, L. Gao, Z. Sun, Q. Liu, J. Dwivedi-Yu, Y. Yang, J. Callan, and G. Neubig, “Active retrieval augmented generation,” 2023.\n",
    "- [169] T. Schick, J. Dwivedi-Yu, R. Dess`ı, R. Raileanu, M. Lomeli, L. Zettle- moyer, N. Cancedda, and T. Scialom, “Toolformer: Language models can teach themselves to use tools,” 2023.\n",
    "- [170] B. Paranjape, S. Lundberg, S. Singh, H. Hajishirzi, L. Zettlemoyer, and M. T. Ribeiro, “Art: Automatic multi-step reasoning and tool-use for large language models,” 2023.\n",
    "- [171] Y. Shen, K. Song, X. Tan, D. Li, W. Lu, and Y. Zhuang, “Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface,” arXiv preprint arXiv:2303.17580, 2023.\n",
    "- [172] Z. Xi, W. Chen, X. Guo, W. He, Y. Ding, B. Hong, M. Zhang, J. Wang, S. Jin, E. Zhou et al., “The rise and potential of large language model based agents: A survey,” arXiv preprint arXiv:2309.07864, 2023.\n",
    "- [173] L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen, J. Tang, X. Chen, Y. Lin et al., “A survey on large language model based autonomous agents,” arXiv preprint arXiv:2308.11432, 2023.\n",
    "- [174] Z. Durante, Q. Huang, N. Wake, R. Gong, J. S. Park, B. Sarkar, R. Taori, Y. Noda, D. Terzopoulos, Y. Choi, K. Ikeuchi, H. Vo, L. Fei-Fei, and J. Gao, “Agent ai: Surveying the horizons of multimodal interaction,” arXiv preprint arXiv:2401.03568, 2024.\n",
    "- [175] B. Xu, Z. Peng, B. Lei, S. Mukherjee, Y. Liu, and D. Xu, “Rewoo: Decoupling reasoning from observations for efficient augmented lan- guage models,” 2023.\n",
    "- [176] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao, “React: Synergizing reasoning and acting in language models,” 2023.\n",
    "- [177] V. Nair, E. Schumacher, G. Tso, and A. Kannan, “Dera: Enhanc- ing large language model completions with dialog-enabled resolving agents,” 2023.\n",
    "- [178] Y. Chang, X. Wang, J. Wang, Y. Wu, L. Yang, K. Zhu, H. Chen, X. Yi, C. Wang, Y. Wang, W. Ye, Y. Zhang, Y. Chang, P. S. Yu, Q. Yang, and X. Xie, “A survey on evaluation of large language models,” 2023.\n",
    "- [179] T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh, C. Alberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee, K. Toutanova, L. Jones, M. Kelcey, M.-W. Chang, A. M. Dai, J. Uszkoreit, Q. Le, and S. Petrov, “Natural questions: A benchmark for question answering research,” Transactions of the Association for Computational Linguistics, vol. 7, pp. 452–466, 2019. [Online]. Available: https://aclanthology.org/Q19-1026\n",
    "- [180] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt, “Measuring massive multitask language understanding,” 2021.\n",
    "- [181] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry, Q. Le et al., “Program synthesis with large language models,” arXiv preprint arXiv:2108.07732, 2021.\n",
    "- [182] E. Choi, H. He, M. Iyyer, M. Yatskar, W.-t. Yih, Y. Choi, P. Liang, and L. Zettlemoyer, “QuAC: Question answering in context,” in Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, E. Riloff, D. Chiang, J. Hockenmaier, and J. Tsujii, Eds. Brussels, Belgium: Association for Computational Linguistics, Oct.-Nov. 2018, pp. 2174–2184. [Online]. Available: https://aclanthology.org/D18-1241\n",
    "- [183] D. Hendrycks, S. Basart, S. Kadavath, M. Mazeika, A. Arora, E. Guo, C. Burns, S. Puranik, H. He, D. Song, and J. Steinhardt, “Measuring coding challenge competence with apps,” NeurIPS, 2021.\n",
    "- [184] V. Zhong, C. Xiong, and R. Socher, “Seq2sql: Generating structured queries from natural language using reinforcement learning,” arXiv preprint arXiv:1709.00103, 2017.\n",
    "- [185] “TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension,” in Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), R. Barzilay and M.-Y. Kan, Eds. Vancouver, Canada: Association for Computational Linguistics, Jul. 2017, pp. 1601–1611. [Online]. Available: https://aclanthology.org/P17-1147\n",
    "- [186] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, \n",
    "- G. Lai, Q. Xie, H. Liu, Y. Yang, and E. Hovy, “RACE: Large-scale ReAding comprehension dataset from examinations,” in Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, M. Palmer, R. Hwa, and S. Riedel, Eds. Copenhagen, Denmark: Association for Computational Linguistics, Sep. 2017, pp. 785–794. [Online]. Available: https://aclanthology.org/D17-1082\n",
    "- [187] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, “SQuAD: 100,000+ questions for machine comprehension of text,” in Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, J. Su, K. Duh, and X. Carreras, Eds. Austin, Texas: Association for Computational Linguistics, Nov. 2016, pp. 2383–2392. [Online]. Available: https://aclanthology.org/D16-1264\n",
    "- [188] C. Clark, K. Lee, M. Chang, T. Kwiatkowski, M. Collins, and K. Toutanova, “Boolq: Exploring the surprising difficulty of natural yes/no questions,” CoRR, vol. abs/1905.10044, 2019. [Online]. Available: http://arxiv.org/abs/1905.10044\n",
    "- [189] D. Khashabi, S. Chaturvedi, M. Roth, S. Upadhyay, and D. Roth, “Looking beyond the surface:a challenge set for reading compre- hension over multiple sentences,” in Proceedings of North American Chapter of the Association for Computational Linguistics (NAACL), 2018.\n",
    "- [190] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and J. Schulman, “Training verifiers to solve math word problems,” CoRR, vol. abs/2110.14168, 2021. [Online]. Available: https: //arxiv.org/abs/2110.14168\n",
    "- [191] D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt, “Measuring mathematical problem solving with the MATH dataset,” CoRR, vol. abs/2103.03874, 2021. [Online]. Available: https://arxiv.org/abs/2103.03874\n",
    "- [192] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi, “Hellaswag: Can a machine really finish your sentence?” 2019.\n",
    "- [193] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord, “Think you have solved question answering? try arc, the AI2 reasoning challenge,” CoRR, vol. abs/1803.05457, 2018. [Online]. Available: http://arxiv.org/abs/1803.05457\n",
    "- [194] Y. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi, “PIQA: reasoning about physical commonsense in natural language,” CoRR, vol. abs/1911.11641, 2019. [Online]. Available: http://arxiv.org/abs/ 1911.11641\n",
    "- [195] M. Sap, H. Rashkin, D. Chen, R. L. Bras, and Y. Choi, “Socialiqa: Commonsense reasoning about social interactions,” CoRR, vol. abs/1904.09728, 2019. [Online]. Available: http://arxiv.org/abs/1904. 09728\n",
    "- [196] T. Mihaylov, P. Clark, T. Khot, and A. Sabharwal, “Can a suit of armor conduct electricity? A new dataset for open book question answering,” CoRR, vol. abs/1809.02789, 2018. [Online]. Available: http://arxiv.org/abs/1809.02789\n",
    "- [197] S. Lin, J. Hilton, and O. Evans, “Truthfulqa: Measuring how models mimic human falsehoods,” arXiv preprint arXiv:2109.07958, 2021.\n",
    "- [198] Z. Yang, P. Qi, S. Zhang, Y. Bengio, W. W. Cohen, R. Salakhutdinov, and C. D. Manning, “Hotpotqa: A dataset for diverse, explainable multi-hop question answering,” CoRR, vol. abs/1809.09600, 2018. [Online]. Available: http://arxiv.org/abs/1809.09600\n",
    "- [199] Y. Zhuang, Y. Yu, K. Wang, H. Sun, and C. Zhang, “Toolqa: A dataset for llm question answering with external tools,” arXiv preprint arXiv:2306.13304, 2023.\n",
    "- [200] D. Chen, J. Bolton, and C. D. Manning, “A thorough examination of the cnn/daily mail reading comprehension task,” in Association for Computational Linguistics (ACL), 2016.\n",
    "- [201] R. Nallapati, B. Zhou, C. Gulcehre, B. Xiang et al., “Abstractive text summarization using sequence-to-sequence rnns and beyond,” arXiv preprint arXiv:1602.06023, 2016.\n",
    "- [202] Y. Bai and D. Z. Wang, “More than reading comprehension: A survey on datasets and metrics of textual question answering,” arXiv preprint arXiv:2109.12264, 2021.\n",
    "- [203] H.-Y. Huang, E. Choi, and W.-t. Yih, “Flowqa: Grasping flow in history for conversational machine comprehension,” arXiv preprint arXiv:1810.06683, 2018.\n",
    "- [204] S. Lee, J. Lee, H. Moon, C. Park, J. Seo, S. Eo, S. Koo, and H. Lim, “A survey on evaluation metrics for machine translation,” Mathematics, vol. 11, no. 4, p. 1006, 2023.\n",
    "- [205] J. Li, X. Cheng, W. X. Zhao, J.-Y. Nie, and J.-R. Wen, “Halueval: A large-scale hallucination evaluation benchmark for large language models,” in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, 2023, pp. 6449–6464.\n",
    "- [206] Simon Mark Hughes, “Hughes hallucination evaluation model (hhem) leaderboard,” 2024, https://huggingface.co/spaces/vectara/ Hallucination-evaluation-leaderboard, Last accessed on 2024-01-21.\n",
    "- [207] S. Gunasekar, Y. Zhang, J. Aneja, C. C. T. Mendes, A. Del Giorno, S. Gopi, M. Javaheripi, P. Kauffmann, G. de Rosa, O. Saarikivi et al., “Textbooks are all you need,” arXiv preprint arXiv:2306.11644, 2023.\n",
    "- [208] Y. Li, S. Bubeck, R. Eldan, A. Del Giorno, S. Gunasekar, and Y. T. Lee, “Textbooks are all you need ii: phi-1.5 technical report,” arXiv preprint arXiv:2309.05463, 2023.\n",
    "- [209] M. Poli, S. Massaroli, E. Nguyen, D. Y. Fu, T. Dao, S. Baccus, Y. Bengio, S. Ermon, and C. Re ́, “Hyena hierarchy: Towards larger convolutional language models,” 2023.\n",
    "- [210] M. Poli, J. Wang, S. Massaroli, J. Quesnelle, E. Nguyen, and A. Thomas, “StripedHyena: Moving Beyond Transformers with Hybrid Signal Processing Models,” 12 2023. [Online]. Available: https://github.com/togethercomputer/stripedhyena\n",
    "- [211] D. Y. Fu, S. Arora, J. Grogan, I. Johnson, S. Eyuboglu, A. W. Thomas, B. Spector, M. Poli, A. Rudra, and C. Re ́, “Monarch mixer: A simple sub-quadratic gemm-based architecture,” 2023.\n",
    "- [212] G. J. McLachlan, S. X. Lee, and S. I. Rathnayake, “Finite mixture models,” Annual review of statistics and its application, vol. 6, pp. 355–378, 2019.\n",
    "- [213] H. Liu, C. Li, Q. Wu, and Y. J. Lee, “Visual instruction tuning,” arXiv preprint arXiv:2304.08485, 2023.\n",
    "- [214] S. Liu, H. Cheng, H. Liu, H. Zhang, F. Li, T. Ren, X. Zou, J. Yang, H. Su, J. Zhu, L. Zhang, J. Gao, and C. Li, “Llava-plus: Learning to use tools for creating multimodal agents,” arXiv preprint arXiv:2311.05437, 2023.\n",
    "- [215] S. Wu, H. Fei, L. Qu, W. Ji, and T.-S. Chua, “Next-gpt: Any-to-any multimodal llm,” arXiv preprint arXiv:2309.05519, 2023. \n",
    "- [216] N. N. Khasmakhi, M. Asgari-Chenaghlu, N. Asghar, P. Schaer, and D. Zu ̈hlke, “Convgenvismo: Evaluation of conversational generative vision models,” 2023.\n",
    "- [217] L. Sun, Y. Huang, H. Wang, S. Wu, Q. Zhang, C. Gao, Y. Huang, W. Lyu, Y. Zhang, X. Li et al., “Trustllm: Trustworthiness in large language models,” arXiv preprint arXiv:2401.05561, 2024.\n",
    "- [218] Microsoft. Deepspeed. [Online]. Available: https://github.com/microsoft/DeepSpeed\n",
    "- [219] HuggingFace. Transformers. [Online]. Available: https://github.com/huggingface/transformers\n",
    "- [220] Nvidia. Megatron. [Online]. Available: https://github.com/NVIDIA/Megatron-LM\n",
    "- [221] BMTrain. Bmtrain. [Online]. Available: https://github.com/OpenBMB/BMTrain\n",
    "- [222] EleutherAI. gpt-neox. [Online]. Available: https://github.com/EleutherAI/gpt-neox\n",
    "- [223] Microsoft. Lora. [Online]. Available: https://github.com/microsoft/LoRA\n",
    "- [224] ColossalAI. Colossalai. [Online]. Available: https://github.com/hpcaitech/ColossalAI\n",
    "- [225] FastChat. Fastchat. [Online]. Available: https://github.com/lm-sys/FastChat\n",
    "- [226] skypilot. skypilot. [Online]. Available: https://github.com/skypilot-org/skypilot\n",
    "- [227] vllm. vllm. [Online]. Available: https://github.com/vllm-project/vllm\n",
    "- [228] huggingface. text-generation-inference. [Online]. Available: https://github.com/huggingface/text-generation-inference\n",
    "- [229] langchain. langchain. [Online]. Available: https://github.com/langchain-ai/langchain\n",
    "- [230] bentoml. Openllm. [Online]. Available: https://github.com/bentoml/OpenLLM\n",
    "- [231] embedchain. embedchain. [Online]. Available: https://github.com/embedchain/embedchain\n",
    "- [232] microsoft. autogen. [Online]. Available: https://github.com/microsoft/autogen\n",
    "- [233] babyagi. babyagi. [Online]. Available: https://github.com/yoheinakajima/babyagi\n",
    "- [234] guidance. guidance. [Online]. Available: github.com/guidance-ai/guidance\n",
    "- [235] prompttools. prompttools. [Online]. Available: github.com/hegelai/prompttools\n",
    "- [236] promptfoo. promptfoo. [Online]. Available: github.com/promptfoo/promptfoo\n",
    "- [237] facebook. faiss. [Online]. Available: github.com/facebookresearch/faiss\n",
    "- [238] milvus. milvus. [Online]. Available: https://github.com/milvus-io/milvus\n",
    "- [239] qdrant. qdrant. [Online]. Available: https://github.com/qdrant/qdrant\n",
    "- [240] weaviate. weaviate. [Online]. Available: https://github.com/weaviate/weaviate\n",
    "- [241] llama index. llama-index. [Online]. Available: https://github.com/run-llama/llamaindex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84f2b59-b1b0-481e-b32c-c7963154ddca",
   "metadata": {},
   "source": [
    "[1] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei, “Scaling laws for neural language models,” arXiv preprint arXiv:2001.08361, 2020.\n",
    "[2] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. d. L. Casas, L. A. Hendricks, J. Welbl, A. Clark et al., “Training compute-optimal large language models,” arXiv preprint arXiv:2203.15556, 2022.\n",
    "[3] C. E. Shannon, “Prediction and entropy of printed english,” Bell system technical journal, vol. 30, no. 1, pp. 50–64, 1951.\n",
    "[4] F. Jelinek, Statistical methods for speech recognition. 1998. MIT press,1998.\n",
    "[5] C. Manning and H. Schutze, Foundations of statistical natural language processing. MIT press, 1999.\n",
    "[6] C. D. Manning, An introduction to information retrieval. Cambridge university press, 2009.\n",
    "[7] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong et al., “A survey of large language models,” arXiv preprint arXiv:2303.18223, 2023.\n",
    "[8] C. Zhou, Q. Li, C. Li, J. Yu, Y. Liu, G. Wang, K. Zhang, C. Ji, Q. Yan, L. He et al., “A comprehensive survey on pretrained foundation mod- els: A history from bert to chatgpt,” arXiv preprint arXiv:2302.09419, 2023.\n",
    "[9] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig, “Pre- train, prompt, and predict: A systematic survey of prompting methods in natural language processing,” ACM Computing Surveys, vol. 55, no. 9, pp. 1–35, 2023.\n",
    "[10] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun, J. Xu, and Z. Sui, “A survey for in-context learning,” arXiv preprint arXiv:2301.00234, 2022.\n",
    "[11] J. Huang and K. C.-C. Chang, “Towards reasoning in large language models: A survey,” arXiv preprint arXiv:2212.10403, 2022.\n",
    "[12] S. F. Chen and J. Goodman, “An empirical study of smoothing techniques for language modeling,” Computer Speech & Language, vol. 13, no. 4, pp. 359–394, 1999.\n",
    "[13] Y. Bengio, R. Ducharme, and P. Vincent, “A neural probabilistic language model,” Advances in neural information processing systems, vol. 13, 2000.\n",
    "[14] H.Schwenk,D.De ́chelotte,andJ.-L.Gauvain,“Continuousspace language models for statistical machine translation,” in Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, 2006, pp. 723–730.\n",
    "[15] T. Mikolov, M. Karafia ́t, L. Burget, J. Cernocky , and S. Khudanpur, “Recurrent neural network based language model.” in Interspeech, vol. 2, no. 3. Makuhari, 2010, pp. 1045–1048.\n",
    "[16] A. Graves, “Generating sequences with recurrent neural networks,” arXiv preprint arXiv:1308.0850, 2013.\n",
    "[17] P.-S. Huang, X. He, J. Gao, L. Deng, A. Acero, and L. Heck, “Learning deep structured semantic models for web search using clickthrough data,” in Proceedings of the 22nd ACM international conference on Information & Knowledge Management, 2013, pp. 2333–2338.\n",
    "[18] J. Gao, C. Xiong, P. Bennett, and N. Craswell, Neural Approaches to Conversational Information Retrieval. Springer Nature, 2023, vol. 44.\n",
    "[19] I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learning with neural networks,” Advances in neural information processing systems, vol. 27, 2014.\n",
    "[20] K. Cho, B. Van Merrie ̈nboer, D. Bahdanau, and Y. Bengio, “On the properties of neural machine translation: Encoder-decoder ap- proaches,” arXiv preprint arXiv:1409.1259, 2014.\n",
    "[21] H. Fang, S. Gupta, F. Iandola, R. K. Srivastava, L. Deng, P. Dolla ́r, J. Gao, X. He, M. Mitchell, J. C. Platt et al., “From captions to visual concepts and back,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2015, pp. 1473–1482.\n",
    "[22] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan, “Show and tell: A neural image caption generator,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2015, pp. 3156–3164.\n",
    "[23] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer, “Deep contextualized word representations. corr abs/1802.05365 (2018),” arXiv preprint arXiv:1802.05365, 2018.\n",
    "[24] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training of deep bidirectional transformers for language understanding,” arXiv preprint arXiv:1810.04805, 2018.\n",
    "[25] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov, “Roberta: A robustly optimized bert pretraining approach,” arXiv preprint arXiv:1907.11692, 2019.\n",
    "[26] P. He, X. Liu, J. Gao, and W. Chen, “Deberta: Decoding-enhanced bert with disentangled attention,” arXiv preprint arXiv:2006.03654, 2020.\n",
    "[27] X. Han, Z. Zhang, N. Ding, Y. Gu, X. Liu, Y. Huo, J. Qiu, Y. Yao, A. Zhang, L. Zhang et al., “Pre-trained models: Past, present and future,” AI Open, vol. 2, pp. 225–250, 2021.\n",
    "[28] X. Qiu, T. Sun, Y. Xu, Y. Shao, N. Dai, and X. Huang, “Pre-trained models for natural language processing: A survey,” Science China Technological Sciences, vol. 63, no. 10, pp. 1872–1897, 2020.\n",
    "[29] A. Gu, K. Goel, and C. Re ́, “Efficiently modeling long sequences with structured state spaces,” 2022.\n",
    "[30] A. Gu and T. Dao, “Mamba: Linear-time sequence modeling with selective state spaces,” arXiv preprint arXiv:2312.00752, 2023.\n",
    "[31] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann et al., “Palm: Scaling language modeling with pathways,” arXiv preprint arXiv:2204.02311, 2022.\n",
    "[32] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Roziere, N. Goyal, E. Hambro, F. Azhar et al., “Llama: Open and efficient foundation language models,” arXiv preprint arXiv:2302.13971, 2023.\n",
    "[33] OpenAI, “GPT-4 Technical Report,” https://arxiv.org/pdf/2303. 08774v3.pdf, 2023.\n",
    "[34] J. Wei, X. Wang, D. Schuurmans, M. Bosma, b. ichter, F. Xia, E. Chi, Q. V. Le, and D. Zhou, “Chain-of-thought prompting elicits reasoning in large language models,” in Advances in Neural Information Processing Systems, S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, Eds., vol. 35. Curran Associates, Inc., 2022, pp. 24 824–24 837. [Online]. Available: https://proceedings.neurips.cc/paper files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf\n",
    "[35] G. Mialon, R. Dessı, M. Lomeli, C. Nalmpantis, R. Pasunuru, R. Raileanu, B. Roziere, T. Schick, J. Dwivedi-Yu, A. Celikyil-maz et al., “Augmented language models: a survey,” arXiv preprint arXiv:2302.07842, 2023.\n",
    "[36] B. Peng, M. Galley, P. He, H. Cheng, Y. Xie, Y. Hu, Q. Huang, L. Liden, Z. Yu, W. Chen, and J. Gao, “Check your facts and try again: Improving large language models with external knowledge and automated feedback,” arXiv preprint arXiv:2302.12813, 2023.\n",
    "[37] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao, “React: Synergizing reasoning and acting in language models,” arXiv preprint arXiv:2210.03629, 2022.\n",
    "[38] D. E. Rumelhart, G. E. Hinton, R. J. Williams et al., “Learning internal representations by error propagation,” 1985.\n",
    "[39] J. L. Elman, “Finding structure in time,” Cognitive science, vol. 14, no. 2, pp. 179–211, 1990.\n",
    "[40] M. V. Mahoney, “Fast text compression with neural networks.” in FLAIRS conference, 2000, pp. 230–234.\n",
    "[41] T. Mikolov, A. Deoras, D. Povey, L. Burget, and J. Cˇernocky, “Strategies for training large scale neural network language models,” in 2011 IEEE Workshop on Automatic Speech Recognition & Understanding. IEEE, 2011, pp. 196–201.\n",
    "[42] tmikolov. rnnlm. [Online]. Available: https://www.fit.vutbr.cz/∼imikolov/rnnlm/\n",
    "[43] S. Minaee, N. Kalchbrenner, E. Cambria, N. Nikzad, M. Chenaghlu, and J. Gao, “Deep learning–based text classification: a comprehensive review,” ACM computing surveys (CSUR), vol. 54, no. 3, pp. 1–40, 2021.\n",
    "[44] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in neural information processing systems, vol. 30, 2017.\n",
    "[45] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut, “Albert: A lite bert for self-supervised learning of language represen- tations,” arXiv preprint arXiv:1909.11942, 2019.\n",
    "[46] K. Clark, M.-T. Luong, Q. V. Le, and C. D. Manning, “Electra: Pre- training text encoders as discriminators rather than generators,” arXiv preprint arXiv:2003.10555, 2020.\n",
    "[47] G. Lample and A. Conneau, “Cross-lingual language model pretrain- ing,” arXiv preprint arXiv:1901.07291, 2019.\n",
    "[48] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov, and Q. V. Le, “Xlnet: Generalized autoregressive pretraining for language understanding,” Advances in neural information processing systems, vol. 32, 2019.\n",
    "[49] L. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y. Wang, J. Gao, M. Zhou, and H.-W. Hon, “Unified language model pre-training for natural language understanding and generation,” Advances in neural information processing systems, vol. 32, 2019.\n",
    "[50] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever et al., “Improv- ing language understanding by generative pre-training,” 2018.\n",
    "[51] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al., “Language models are unsupervised multitask learners,” OpenAI blog, vol. 1, no. 8, p. 9, 2019.\n",
    "[52] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu, “Exploring the limits of transfer learning with a unified text-to-text transformer,” The Journal of Machine Learning Research, vol. 21, no. 1, pp. 5485–5551, 2020.\n",
    "[53] L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant, A. Barua, and C. Raffel, “mt5: A massively multilingual pre-trained text-to-text transformer,” arXiv preprint arXiv:2010.11934, 2020.\n",
    "[54] K. Song, X. Tan, T. Qin, J. Lu, and T.-Y. Liu, “Mass: Masked sequence to sequence pre-training for language generation,” arXiv preprint arXiv:1905.02450, 2019.\n",
    "[55] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, and L. Zettlemoyer, “Bart: Denoising sequence-to- sequence pre-training for natural language generation, translation, and comprehension,” arXiv preprint arXiv:1910.13461, 2019.\n",
    "[56] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., “Language mod- els are few-shot learners,” Advances in neural information processing systems, vol. 33, pp. 1877–1901, 2020.\n",
    "[57] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Ka- plan, H. Edwards, Y. Burda, N. Joseph, G. Brockman et al., “Evaluating large language models trained on code,” arXiv preprint arXiv:2107.03374, 2021.\n",
    "[58] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, C. Hesse, S. Jain, V. Kosaraju, W. Saunders et al., “Webgpt: Browser- assisted question-answering with human feedback,” arXiv preprint arXiv:2112.09332, 2021.\n",
    "[59] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray et al., “Training language models to follow instructions with human feedback,” Advances in Neural Information Processing Systems, vol. 35, pp. 27730–27744, 2022.\n",
    "[60] OpenAI. (2022) Introducing chatgpt. [Online]. Available: https: //openai.com/blog/chatgpt\n",
    "[61] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al., “Llama 2: Open foundation and fine-tuned chat models,” arXiv preprint arXiv:2307.09288, 2023.\n",
    "[62] R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang, and T. B. Hashimoto, “Alpaca: A strong, replicable instruction- following model,” Stanford Center for Research on Foundation Mod- els. https://crfm. stanford. edu/2023/03/13/alpaca. html, vol. 3, no. 6, p. 7, 2023.\n",
    "[63] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, “Qlora: Ef- ficient finetuning of quantized llms,” arXiv preprint arXiv:2305.14314, 2023.\n",
    "[64] X. Geng, A. Gudibande, H. Liu, E. Wallace, P. Abbeel, S. Levine, and D. Song, “Koala: A dialogue model for academic research,” Blog post, April, vol. 1, 2023.\n",
    "[65] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier et al., “Mistral 7b,” arXiv preprint arXiv:2310.06825, 2023.\n",
    "[66] B. Roziere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, T. Remez, J. Rapin et al., “Code llama: Open foundation models for code,” arXiv preprint arXiv:2308.12950, 2023.\n",
    "[67] S. G. Patil, T. Zhang, X. Wang, and J. E. Gonzalez, “Gorilla: Large language model connected with massive apis,” 2023.\n",
    "[68] A. Pal, D. Karkhanis, M. Roberts, S. Dooley, A. Sundararajan, and S. Naidu, “Giraffe: Adventures in expanding context lengths in llms,” arXiv preprint arXiv:2308.10882, 2023.\n",
    "[69] B. Huang, “Vigogne: French instruction-following and chat models,” https://github.com/bofenghuang/vigogne, 2023.\n",
    "[70] Y. Wang, H. Ivison, P. Dasigi, J. Hessel, T. Khot, K. R. Chandu, D. Wadden, K. MacMillan, N. A. Smith, I. Beltagy et al., “How far can camels go? exploring the state of instruction tuning on open resources,” arXiv preprint arXiv:2306.04751, 2023.\n",
    "[71] S. Tworkowski, K. Staniszewski, M. Pacek, Y. Wu, H. Michalewski, and P. Miłos ́, “Focused transformer: Contrastive training for context scaling,” arXiv preprint arXiv:2307.03170, 2023.\n",
    "[72] D. Mahan, R. Carlow, L. Castricato, N. Cooper, and C. Laforte, “Stable beluga models.” [Online]. Available: [https://huggingface.co/stabilityai/StableBeluga2](https://huggingface.co/stabilityai/StableBeluga2)\n",
    "[73] Y. Tay, J. Wei, H. W. Chung, V. Q. Tran, D. R. So, S. Shakeri, X. Gar- cia, H. S. Zheng, J. Rao, A. Chowdhery et al., “Transcending scaling laws with 0.1% extra compute,” arXiv preprint arXiv:2210.11399, 2022.\n",
    "[74] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, Y. Li, X. Wang, M. Dehghani, S. Brahma et al., “Scaling instruction- finetuned language models,” arXiv preprint arXiv:2210.11416, 2022.\n",
    "[75] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey, Z. Chen et al., “Palm 2 technical report,” arXiv preprint arXiv:2305.10403, 2023.\n",
    "[76] K. Singhal, S. Azizi, T. Tu, S. S. Mahdavi, J. Wei, H. W. Chung, N. Scales, A. Tanwani, H. Cole-Lewis, S. Pfohl et al., “Large language models encode clinical knowledge,” arXiv preprint arXiv:2212.13138, 2022.\n",
    "[77] K. Singhal, T. Tu, J. Gottweis, R. Sayres, E. Wulczyn, L. Hou, K. Clark, S. Pfohl, H. Cole-Lewis, D. Neal et al., “Towards expert- level medical question answering with large language models,” arXiv preprint arXiv:2305.09617, 2023.\n",
    "[78] J. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le, “Finetuned language models are zero-shot learners,” arXiv preprint arXiv:2109.01652, 2021.\n",
    "[79] J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song, J. Aslanides, S. Henderson, R. Ring, S. Young et al., “Scaling language models: Methods, analysis & insights from training gopher,” arXiv preprint arXiv:2112.11446, 2021.\n",
    "[80] V. Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai, A. Chaffin, A. Stiegler, T. L. Scao, A. Raja et al., “Multi- task prompted training enables zero-shot task generalization,” arXiv preprint arXiv:2110.08207, 2021.\n",
    "[81] Y. Sun, S. Wang, S. Feng, S. Ding, C. Pang, J. Shang, J. Liu, X. Chen, Y. Zhao, Y. Lu et al., “Ernie 3.0: Large-scale knowledge enhanced pre- training for language understanding and generation,” arXiv preprint arXiv:2107.02137, 2021.\n",
    "[82] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Mil- lican, G. B. Van Den Driessche, J.-B. Lespiau, B. Damoc, A. Clark et al., “Improving language models by retrieving from trillions of tokens,” in International conference on machine learning. PMLR, 2022, pp. 2206–2240.\n",
    "[83] O. Lieber, O. Sharir, B. Lenz, and Y. Shoham, “Jurassic-1: Technical details and evaluation,” White Paper. AI21 Labs, vol. 1, p. 9, 2021.\n",
    "[84] N. Du, Y. Huang, A. M. Dai, S. Tong, D. Lepikhin, Y. Xu, M. Krikun, Y. Zhou, A. W. Yu, O. Firat et al., “Glam: Efficient scaling of language models with mixture-of-experts,” in International Conference on Machine Learning. PMLR, 2022, pp. 5547–5569.\n",
    "[85] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.- T. Cheng, A. Jin, T. Bos, L. Baker, Y. Du et al., “Lamda: Language models for dialog applications,” arXiv preprint arXiv:2201.08239, 2022.\n",
    "[86] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin et al., “Opt: Open pre-trained transformer language models,” arXiv preprint arXiv:2205.01068, 2022.\n",
    "[87] R. Taylor, M. Kardas, G. Cucurull, T. Scialom, A. Hartshorn, E. Sar- avia, A. Poulton, V. Kerkez, and R. Stojnic, “Galactica: A large language model for science,” arXiv preprint arXiv:2211.09085, 2022.\n",
    "[88] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese, and C. Xiong, “Codegen: An open large language model for code with multi-turn program synthesis,” arXiv preprint arXiv:2203.13474, 2022.\n",
    "[89] S. Soltan, S. Ananthakrishnan, J. FitzGerald, R. Gupta, W. Hamza, H. Khan, C. Peris, S. Rawls, A. Rosenbaum, A. Rumshisky et al., “Alexatm 20b: Few-shot learning using a large-scale multilingual seq2seq model,” arXiv preprint arXiv:2208.01448, 2022.\n",
    "[90] A. Glaese, N. McAleese, M. Trebacz, J. Aslanides, V. Firoiu, T. Ewalds, M. Rauh, L. Weidinger, M. Chadwick, P. Thacker et al., “Improving alignment of dialogue agents via targeted human judge- ments,” arXiv preprint arXiv:2209.14375, 2022.\n",
    "[91] A. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski, V. Ramasesh, A. Slone, C. Anil, I. Schlag, T. Gutman-Solo et al.,“Solving quantitative reasoning problems with language models,” Advances in Neural Information Processing Systems, vol. 35, pp. 3843–3857, 2022.\n",
    "[92] Y. Tay, M. Dehghani, V. Q. Tran, X. Garcia, D. Bahri, T. Schuster, H. S. Zheng, N. Houlsby, and D. Metzler, “Unifying language learning paradigms,” arXiv preprint arXiv:2205.05131, 2022.\n",
    "[93] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilic ́, D. Hesslow, R. Castagne ́, A. S. Luccioni, F. Yvon, M. Galle ́ et al., “Bloom: A 176b- parameter open-access multilingual language model,” arXiv preprint arXiv:2211.05100, 2022.\n",
    "[94] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y. Xu, W. Zheng, X. Xia et al., “Glm-130b: An open bilingual pre-trained model,” arXiv preprint arXiv:2210.02414, 2022.\n",
    "[95] S. Biderman, H. Schoelkopf, Q. G. Anthony, H. Bradley, K. O’Brien, E. Hallahan, M. A. Khan, S. Purohit, U. S. Prashanth, E. Raff et al., “Pythia: A suite for analyzing large language models across train- ing and scaling,” in International Conference on Machine Learning. PMLR, 2023, pp. 2397–2430.\n",
    "[96] S. Mukherjee, A. Mitra, G. Jawahar, S. Agarwal, H. Palangi, and A. Awadallah, “Orca: Progressive learning from complex explanation traces of gpt-4,” arXiv preprint arXiv:2306.02707, 2023.\n",
    "[97] R. Li, L. B. Allal, Y. Zi, N. Muennighoff, D. Kocetkov, C. Mou, M. Marone, C. Akiki, J. Li, J. Chim et al., “Starcoder: may the source be with you!” arXiv preprint arXiv:2305.06161, 2023.\n",
    "[98] S. Huang, L. Dong, W. Wang, Y. Hao, S. Singhal, S. Ma, T. Lv, L. Cui, O. K. Mohammed, Q. Liu et al., “Language is not all you need: Aligning perception with language models,” arXiv preprint arXiv:2302.14045, 2023.\n",
    "[99] G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth et al., “Gemini: a family of highly capable multimodal models,” arXiv preprint arXiv:2312.11805, 2023.\n",
    "[100] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch, Y. Chebotar et al., “Inner monologue: Embodied reasoning through planning with language models,” arXiv preprint arXiv:2207.05608, 2022.\n",
    "[101] S. Smith, M. Patwary, B. Norick, P. LeGresley, S. Rajbhandari, J. Casper, Z. Liu, S. Prabhumoye, G. Zerveas, V. Korthikanti et al., “Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model,” arXiv preprint arXiv:2201.11990, 2022.\n",
    "[102] I. Beltagy, M. E. Peters, and A. Cohan, “Longformer: The long- document transformer,” arXiv preprint arXiv:2004.05150, 2020.\n",
    "[103] S. Iyer, X. V. Lin, R. Pasunuru, T. Mihaylov, D. Simig, P. Yu, K. Shus- ter, T. Wang, Q. Liu, P. S. Koura et al., “Opt-iml: Scaling language model instruction meta learning through the lens of generalization,” arXiv preprint arXiv:2212.12017, 2022.\n",
    "[104] Y. Hao, H. Song, L. Dong, S. Huang, Z. Chi, W. Wang, S. Ma, and F. Wei, “Language models are general-purpose interfaces,” arXiv preprint arXiv:2206.06336, 2022.\n",
    "[105] Z. Sun, Y. Shen, Q. Zhou, H. Zhang, Z. Chen, D. Cox, Y. Yang, and C. Gan, “Principle-driven self-alignment of language mod- els from scratch with minimal human supervision,” arXiv preprint arXiv:2305.03047, 2023.\n",
    "[106] W. E. team, “Palmyra-base Parameter Autoregressive Language Model,” https://dev.writer.com, 2023.\n",
    "[107] ——, “Camel-5b instructgpt,” https://dev.writer.com, 2023.\n",
    "[108] Yandex. Yalm. [Online]. Available: https://github.com/yandex/ YaLM- 100B\n",
    "[109] M. Team et al., “Introducing mpt-7b: a new standard for open-source, commercially usable llms,” 2023.\n",
    "[110] A. Mitra, L. D. Corro, S. Mahajan, A. Codas, C. Simoes, S. Agarwal, X. Chen, A. Razdaibiedina, E. Jones, K. Aggarwal, H. Palangi, G. Zheng, C. Rosset, H. Khanpour, and A. Awadallah, “Orca 2: Teaching small language models how to reason,” 2023.\n",
    "[111] L. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y. Yang, J. Callan, and G. Neubig, “Pal: Program-aided language models,” in International Conference on Machine Learning. PMLR, 2023, pp. 10 764–10 799.\n",
    "[113] E. Nijkamp, H. Hayashi, C. Xiong, S. Savarese, and Y. Zhou, “Codegen2: Lessons for training llms on programming and natural languages,” arXiv preprint arXiv:2305.02309, 2023.\n",
    "[114] L. Tunstall, E. Beeching, N. Lambert, N. Rajani, K. Rasul, Y. Belkada, S. Huang, L. von Werra, C. Fourrier, N. Habib et al., “Zephyr: Direct distillation of lm alignment,” arXiv preprint arXiv:2310.16944, 2023.\n",
    "[115] X. team. Grok. [Online]. Available: https://grok.x.ai/\n",
    "[116] J. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou, and J. Zhou, “Qwen-vl: A frontier large vision-language model with versatile abilities,” arXiv preprint arXiv:2308.12966, 2023.\n",
    "[117] mixtral. mixtral. [Online]. Available: https://mistral.ai/news/ mixtral- of- experts/\n",
    "[118] D. Wang, N. Raman, M. Sibue, Z. Ma, P. Babkin, S. Kaur, Y. Pei, A. Nourbakhsh, and X. Liu, “Docllm: A layout-aware generative language model for multimodal document understanding,” 2023.\n",
    "[119] D. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen, X. Bi, Y. Wu, Y. K. Li, F. Luo, Y. Xiong, and W. Liang, “Deepseek-coder: When the large language model meets programming – the rise of code intelligence,” 2024.\n",
    "[120] F. Wan, X. Huang, D. Cai, X. Quan, W. Bi, and S. Shi, “Knowledge fusion of large language models,” 2024.\n",
    "[121] P. Zhang, G. Zeng, T. Wang, and W. Lu, “Tinyllama: An open-source small language model,” 2024.\n",
    "[122] C. Wu, Y. Gan, Y. Ge, Z. Lu, J. Wang, Y. Feng, P. Luo, and Y. Shan, “Llama pro: Progressive llama with block expansion,” 2024.\n",
    "[123] X. Amatriain, A. Sankar, J. Bing, P. K. Bodigutla, T. J. Hazen, and M. Kazi, “Transformer models: an introduction and catalog,” 2023.\n",
    "[124] G. Penedo, Q. Malartic, D. Hesslow, R. Cojocaru, A. Cappelli, H. Alobeidli, B. Pannier, E. Almazrouei, and J. Launay, “The refined- web dataset for falcon llm: outperforming curated corpora with web data, and web data only,” arXiv preprint arXiv:2306.01116, 2023.\n",
    "[125] D. Hernandez, T. Brown, T. Conerly, N. DasSarma, D. Drain, S. El- Showk, N. Elhage, Z. Hatfield-Dodds, T. Henighan, T. Hume et al., “Scaling laws and interpretability of learning from repeated data,” arXiv preprint arXiv:2205.10487, 2022.\n",
    "[126] P. Shaw, J. Uszkoreit, and A. Vaswani, “Self-attention with relative position representations,” arXiv preprint arXiv:1803.02155, 2018.\n",
    "[127] J. Su, Y. Lu, S. Pan, B. Wen, and Y. Liu, “Roformer: En- hanced transformer with rotary position embedding,” arXiv preprint arXiv:2104.09864, 2021.\n",
    "[128] O. Press, N. A. Smith, and M. Lewis, “Train short, test long: Attention with linear biases enables input length extrapolation,” arXiv preprint arXiv:2108.12409, 2021.\n",
    "[129] G. Ke, D. He, and T.-Y. Liu, “Rethinking positional encoding in language pre-training,” arXiv preprint arXiv:2006.15595, 2020.\n",
    "[130] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean, “Outrageously large neural networks: The sparsely-gated mixture-of-experts layer,” arXiv preprint arXiv:1701.06538, 2017.\n",
    "[131] W. Fedus, B. Zoph, and N. Shazeer, “Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity,” The Journal of Machine Learning Research, vol. 23, no. 1, pp. 5232–5270, 2022.\n",
    "[132] R. K. Mahabadi, S. Ruder, M. Dehghani, and J. Henderson, “Parameter-efficient multi-task fine-tuning for transformers via shared hypernetworks,” 2021.\n",
    "[133] S. Zhang, L. Dong, X. Li, S. Zhang, X. Sun, S. Wang, J. Li, R. Hu, T. Zhang, F. Wu, and G. Wang, “Instruction tuning for large language models: A survey,” 2023.\n",
    "[134] S. Mishra, D. Khashabi, C. Baral, and H. Hajishirzi, “Cross-task generalization via natural language crowdsourcing instructions,” arXiv preprint arXiv:2104.08773, 2021.\n",
    "[135] Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and H. Hajishirzi, “Self-instruct: Aligning language model with self generated instructions,” arXiv preprint arXiv:2212.10560, 2022.\n",
    "[136] K. Ethayarajh, W. Xu, D. Jurafsky, and D. Kiela. Kto. [Online]. Available: https://github.com/ContextualAI/HALOs/blob/main/assets/ report.pdf\n",
    "[137] P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei, “Deep reinforcement learning from human preferences,” Advances in neural information processing systems, vol. 30, 2017.\n",
    "[138] H. Lee, S. Phatale, H. Mansoor, K. Lu, T. Mesnard, C. Bishop, V. Car- bune, and A. Rastogi, “Rlaif: Scaling reinforcement learning from human feedback with ai feedback,” arXiv preprint arXiv:2309.00267, 2023.\n",
    "[139] R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and C. Finn, “Direct preference optimization: Your language model is secretly a reward model,” arXiv preprint arXiv:2305.18290, 2023.\n",
    "[140] S. Rajbhandari, J. Rasley, O. Ruwase, and Y. He, “Zero: Memory optimizations toward training trillion parameter models,” in SC20: In- ternational Conference for High Performance Computing, Networking, Storage and Analysis. IEEE, 2020, pp. 1–16.\n",
    "[141] B. Peng, E. Alcaide, Q. Anthony, A. Albalak, S. Arcadinho, H. Cao, X. Cheng, M. Chung, M. Grella, K. K. GV et al., “Rwkv: Reinventing rnns for the transformer era,” arXiv preprint arXiv:2305.13048, 2023.\n",
    "[142] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, “Lora: Low-rank adaptation of large language models,” arXiv preprint arXiv:2106.09685, 2021.\n",
    "[143] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural network,” arXiv preprint arXiv:1503.02531, 2015.\n",
    "[144] J. Gou, B. Yu, S. J. Maybank, and D. Tao, “Knowledge distillation: A survey,” International Journal of Computer Vision, vol. 129, pp. 1789–1819, 2021.\n",
    "[145] Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. J. Bang, A. Madotto, and P. Fung, “Survey of hallucination in natural language generation,” ACM Comput. Surv., vol. 55, no. 12, mar 2023. [Online]. Available: https://doi.org/10.1145/3571730\n",
    "[146] N. McKenna, T. Li, L. Cheng, M. J. Hosseini, M. Johnson, and M. Steedman, “Sources of hallucination by large language models on inference tasks,” 2023.\n",
    "[147] C.-Y. Lin, “ROUGE: A package for automatic evaluation of summaries,” in Text Summarization Branches Out. Barcelona, Spain: Association for Computational Linguistics, Jul. 2004, pp. 74–81. [Online]. Available: https://aclanthology.org/W04-1013\n",
    "[148] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “Bleu: a method for automatic evaluation of machine translation,” in Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, P. Isabelle, E. Charniak, and D. Lin, Eds. Philadelphia, Pennsylvania, USA: Association for Computational Linguistics, Jul. 2002, pp. 311– 318. [Online]. Available: https://aclanthology.org/P02-1040\n",
    "[149] B. Dhingra, M. Faruqui, A. Parikh, M.-W. Chang, D. Das, and W. Cohen, “Handling divergent reference texts when evaluating table-to-text generation,” in Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, A. Korhonen, D. Traum, and L. Marquez, Eds. Florence, Italy: Association for Computational Linguistics, Jul. 2019, pp. 4884–4895. [Online]. Available: https://aclanthology.org/P19-1483\n",
    "[150] Z. Wang, X. Wang, B. An, D. Yu, and C. Chen, “Towards faithful neural table-to-text generation with content-matching constraints,” in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, D. Jurafsky, J. Chai, N. Schluter, and J. Tetreault, Eds. Online: Association for Computational Linguistics, Jul. 2020, pp. 1072–1086. [Online]. Available: https: //aclanthology.org/2020.acl-main.101\n",
    "[151] H. Song, W.-N. Zhang, J. Hu, and T. Liu, “Generating persona consis- tent dialogues by exploiting natural language inference,” Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, no. 05, pp. 8878–8885, Apr. 2020.\n",
    "[152] O. Honovich, L. Choshen, R. Aharoni, E. Neeman, I. Szpektor, and O. Abend, “q2: Evaluating factual consistency in knowledge- grounded dialogues via question generation and question answering,” in Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, M.-F. Moens, X. Huang, L. Specia, and S. W.-t. Yih, Eds. Online and Punta Cana, Dominican Republic: Association for Computational Linguistics, Nov. 2021, pp. 7856–7870. [Online]. Available: https://aclanthology.org/2021.emnlp-main.619\n",
    "[153] N. Dziri, H. Rashkin, T. Linzen, and D. Reitter, “Evaluating attribution in dialogue systems: The BEGIN benchmark,” Transactions of the Association for Computational Linguistics, vol. 10, pp. 1066–1083, 2022. [Online]. Available: https://aclanthology.org/2022.tacl-1.62\n",
    "[154] S. Santhanam, B. Hedayatnia, S. Gella, A. Padmakumar, S. Kim, Y. Liu, and D. Z. Hakkani-Tu ̈r, “Rome was built in 1776: A case study on factual correctness in knowledge-grounded response generation,” ArXiv, vol. abs/2110.05456, 2021.\n",
    "[155] S. Min, K. Krishna, X. Lyu, M. Lewis, W. tau Yih, P. W. Koh, M. Iyyer, L. Zettlemoyer, and H. Hajishirzi, “Factscore: Fine-grained atomic evaluation of factual precision in long form text generation,” 2023.\n",
    "[156] D. Sculley, G. Holt, D. Golovin, E. Davydov, T. Phillips, D. Ebner, V. Chaudhary, and M. Young, “Machine learning: The high interest credit card of technical debt,” in SE4ML: Software Engineering for Machine Learning (NIPS 2014 Workshop), 2014.\n",
    "[157] Z. Zhang, A. Zhang, M. Li, and A. Smola, “Automatic chain of thought prompting in large language models,” 2022.\n",
    "[158] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, and K. Narasimhan, “Tree of thoughts: Deliberate problem solving with large language models,” 2023.\n",
    "[159] P. Manakul, A. Liusie, and M. J. F. Gales, “Selfcheckgpt: Zero- resource black-box hallucination detection for generative large lan- guage models,” 2023.\n",
    "[160] N. Shinn, F. Cassano, E. Berman, A. Gopinath, K. Narasimhan, and S. Yao, “Reflexion: Language agents with verbal reinforcement learning,” 2023.\n",
    "[161] S. J. Zhang, S. Florin, A. N. Lee, E. Niknafs, A. Marginean, A. Wang, K. Tyser, Z. Chin, Y. Hicke, N. Singh, M. Udell, Y. Kim, T. Buonassisi, A. Solar-Lezama, and I. Drori, “Exploring the mit mathematics and eecs curriculum using large language models,” 2023.\n",
    "[162] T. Wu, E. Jiang, A. Donsbach, J. Gray, A. Molina, M. Terry, and C. J. Cai, “Promptchainer: Chaining large language model prompts through visual programming,” 2022.\n",
    "[163] Y. Zhou, A. I. Muresanu, Z. Han, K. Paster, S. Pitis, H. Chan, and J. Ba, “Large language models are human-level prompt engineers,” 2023.\n",
    "[164] P. S. H. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Ku ̈ttler, M. Lewis, W. Yih, T. Rockta ̈schel, S. Riedel, and D. Kiela, “Retrieval-augmented generation for knowledge-intensive NLP tasks,” CoRR, vol. abs/2005.11401, 2020. [Online]. Available: https://arxiv.org/abs/2005.11401\n",
    "[165] Y. Gao, Y. Xiong, X. Gao, K. Jia, J. Pan, Y. Bi, Y. Dai, J. Sun, and H. Wang, “Retrieval-augmented generation for large language models: A survey,” arXiv preprint arXiv:2312.10997, 2023.\n",
    "[166] A. W. Services. (Year of publication, e.g., 2023) Question answering using retrieval augmented generation with foundation models in amazon sagemaker jumpstart. Accessed: Date of access, e.g., December 5, 2023. [Online]. Available: https://shorturl.at/dSV47\n",
    "[167] S. Pan, L. Luo, Y. Wang, C. Chen, J. Wang, and X. Wu, “Unifying large language models and knowledge graphs: A roadmap,” arXiv preprint arXiv:2306.08302, 2023.\n",
    "[168] Z. Jiang, F. F. Xu, L. Gao, Z. Sun, Q. Liu, J. Dwivedi-Yu, Y. Yang, J. Callan, and G. Neubig, “Active retrieval augmented generation,” 2023.\n",
    "[169] T. Schick, J. Dwivedi-Yu, R. Dess`ı, R. Raileanu, M. Lomeli, L. Zettle- moyer, N. Cancedda, and T. Scialom, “Toolformer: Language models can teach themselves to use tools,” 2023.\n",
    "[170] B. Paranjape, S. Lundberg, S. Singh, H. Hajishirzi, L. Zettlemoyer, and M. T. Ribeiro, “Art: Automatic multi-step reasoning and tool-use for large language models,” 2023.\n",
    "[171] Y. Shen, K. Song, X. Tan, D. Li, W. Lu, and Y. Zhuang, “Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface,” arXiv preprint arXiv:2303.17580, 2023.\n",
    "[172] Z. Xi, W. Chen, X. Guo, W. He, Y. Ding, B. Hong, M. Zhang, J. Wang, S. Jin, E. Zhou et al., “The rise and potential of large language model based agents: A survey,” arXiv preprint arXiv:2309.07864, 2023.\n",
    "[173] L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen, J. Tang, X. Chen, Y. Lin et al., “A survey on large language model based autonomous agents,” arXiv preprint arXiv:2308.11432, 2023.\n",
    "[174] Z. Durante, Q. Huang, N. Wake, R. Gong, J. S. Park, B. Sarkar, R. Taori, Y. Noda, D. Terzopoulos, Y. Choi, K. Ikeuchi, H. Vo, L. Fei-Fei, and J. Gao, “Agent ai: Surveying the horizons of multimodal interaction,” arXiv preprint arXiv:2401.03568, 2024.\n",
    "[175] B. Xu, Z. Peng, B. Lei, S. Mukherjee, Y. Liu, and D. Xu, “Rewoo: Decoupling reasoning from observations for efficient augmented lan- guage models,” 2023.\n",
    "[176] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao, “React: Synergizing reasoning and acting in language models,” 2023.\n",
    "[177] V. Nair, E. Schumacher, G. Tso, and A. Kannan, “Dera: Enhanc- ing large language model completions with dialog-enabled resolving agents,” 2023.\n",
    "[178] Y. Chang, X. Wang, J. Wang, Y. Wu, L. Yang, K. Zhu, H. Chen, X. Yi, C. Wang, Y. Wang, W. Ye, Y. Zhang, Y. Chang, P. S. Yu, Q. Yang, and X. Xie, “A survey on evaluation of large language models,” 2023.\n",
    "[179] T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh, C. Alberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee, K. Toutanova, L. Jones, M. Kelcey, M.-W. Chang, A. M. Dai, J. Uszkoreit, Q. Le, and S. Petrov, “Natural questions: A benchmark for question answering research,” Transactions of the Association for Computational Linguistics, vol. 7, pp. 452–466, 2019. [Online]. Available: https://aclanthology.org/Q19-1026\n",
    "[180] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt, “Measuring massive multitask language understanding,” 2021.\n",
    "[181] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry, Q. Le et al., “Program synthesis with large language models,” arXiv preprint arXiv:2108.07732, 2021.\n",
    "[182] E. Choi, H. He, M. Iyyer, M. Yatskar, W.-t. Yih, Y. Choi, P. Liang, and L. Zettlemoyer, “QuAC: Question answering in context,” in Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, E. Riloff, D. Chiang, J. Hockenmaier, and J. Tsujii, Eds. Brussels, Belgium: Association for Computational Linguistics, Oct.-Nov. 2018, pp. 2174–2184. [Online]. Available: https://aclanthology.org/D18-1241\n",
    "[183] D. Hendrycks, S. Basart, S. Kadavath, M. Mazeika, A. Arora, E. Guo, C. Burns, S. Puranik, H. He, D. Song, and J. Steinhardt, “Measuring coding challenge competence with apps,” NeurIPS, 2021.\n",
    "[184] V. Zhong, C. Xiong, and R. Socher, “Seq2sql: Generating structured queries from natural language using reinforcement learning,” arXiv preprint arXiv:1709.00103, 2017.\n",
    "[185] “TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension,” in Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), R. Barzilay and M.-Y. Kan, Eds. Vancouver, Canada: Association for Computational Linguistics, Jul. 2017, pp. 1601–1611. [Online]. Available: https://aclanthology.org/P17-1147\n",
    "[186] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, \n",
    "G. Lai, Q. Xie, H. Liu, Y. Yang, and E. Hovy, “RACE: Large-scale ReAding comprehension dataset from examinations,” in Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, M. Palmer, R. Hwa, and S. Riedel, Eds. Copenhagen, Denmark: Association for Computational Linguistics, Sep. 2017, pp. 785–794. [Online]. Available: https://aclanthology.org/D17-1082\n",
    "[187] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, “SQuAD: 100,000+ questions for machine comprehension of text,” in Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, J. Su, K. Duh, and X. Carreras, Eds. Austin, Texas: Association for Computational Linguistics, Nov. 2016, pp. 2383–2392. [Online]. Available: https://aclanthology.org/D16-1264\n",
    "[188] C. Clark, K. Lee, M. Chang, T. Kwiatkowski, M. Collins, and K. Toutanova, “Boolq: Exploring the surprising difficulty of natural yes/no questions,” CoRR, vol. abs/1905.10044, 2019. [Online]. Available: http://arxiv.org/abs/1905.10044\n",
    "[189] D. Khashabi, S. Chaturvedi, M. Roth, S. Upadhyay, and D. Roth, “Looking beyond the surface:a challenge set for reading compre- hension over multiple sentences,” in Proceedings of North American Chapter of the Association for Computational Linguistics (NAACL), 2018.\n",
    "[190] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and J. Schulman, “Training verifiers to solve math word problems,” CoRR, vol. abs/2110.14168, 2021. [Online]. Available: https: //arxiv.org/abs/2110.14168\n",
    "[191] D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt, “Measuring mathematical problem solving with the MATH dataset,” CoRR, vol. abs/2103.03874, 2021. [Online]. Available: https://arxiv.org/abs/2103.03874\n",
    "[192] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi, “Hellaswag: Can a machine really finish your sentence?” 2019.\n",
    "[193] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord, “Think you have solved question answering? try arc, the AI2 reasoning challenge,” CoRR, vol. abs/1803.05457, 2018. [Online]. Available: http://arxiv.org/abs/1803.05457\n",
    "[194] Y. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi, “PIQA: reasoning about physical commonsense in natural language,” CoRR, vol. abs/1911.11641, 2019. [Online]. Available: http://arxiv.org/abs/ 1911.11641\n",
    "[195] M. Sap, H. Rashkin, D. Chen, R. L. Bras, and Y. Choi, “Socialiqa: Commonsense reasoning about social interactions,” CoRR, vol. abs/1904.09728, 2019. [Online]. Available: http://arxiv.org/abs/1904. 09728\n",
    "[196] T. Mihaylov, P. Clark, T. Khot, and A. Sabharwal, “Can a suit of armor conduct electricity? A new dataset for open book question answering,” CoRR, vol. abs/1809.02789, 2018. [Online]. Available: http://arxiv.org/abs/1809.02789\n",
    "[197] S. Lin, J. Hilton, and O. Evans, “Truthfulqa: Measuring how models mimic human falsehoods,” arXiv preprint arXiv:2109.07958, 2021.\n",
    "[198] Z. Yang, P. Qi, S. Zhang, Y. Bengio, W. W. Cohen, R. Salakhutdinov, and C. D. Manning, “Hotpotqa: A dataset for diverse, explainable multi-hop question answering,” CoRR, vol. abs/1809.09600, 2018. [Online]. Available: http://arxiv.org/abs/1809.09600\n",
    "[199] Y. Zhuang, Y. Yu, K. Wang, H. Sun, and C. Zhang, “Toolqa: A dataset for llm question answering with external tools,” arXiv preprint arXiv:2306.13304, 2023.\n",
    "[200] D. Chen, J. Bolton, and C. D. Manning, “A thorough examination of the cnn/daily mail reading comprehension task,” in Association for Computational Linguistics (ACL), 2016.\n",
    "[201] R. Nallapati, B. Zhou, C. Gulcehre, B. Xiang et al., “Abstractive text summarization using sequence-to-sequence rnns and beyond,” arXiv preprint arXiv:1602.06023, 2016.\n",
    "[202] Y. Bai and D. Z. Wang, “More than reading comprehension: A survey on datasets and metrics of textual question answering,” arXiv preprint arXiv:2109.12264, 2021.\n",
    "[203] H.-Y. Huang, E. Choi, and W.-t. Yih, “Flowqa: Grasping flow in history for conversational machine comprehension,” arXiv preprint arXiv:1810.06683, 2018.\n",
    "[204] S. Lee, J. Lee, H. Moon, C. Park, J. Seo, S. Eo, S. Koo, and H. Lim, “A survey on evaluation metrics for machine translation,” Mathematics, vol. 11, no. 4, p. 1006, 2023.\n",
    "[205] J. Li, X. Cheng, W. X. Zhao, J.-Y. Nie, and J.-R. Wen, “Halueval: A large-scale hallucination evaluation benchmark for large language models,” in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, 2023, pp. 6449–6464.\n",
    "[206] Simon Mark Hughes, “Hughes hallucination evaluation model (hhem) leaderboard,” 2024, https://huggingface.co/spaces/vectara/ Hallucination-evaluation-leaderboard, Last accessed on 2024-01-21.\n",
    "[207] S. Gunasekar, Y. Zhang, J. Aneja, C. C. T. Mendes, A. Del Giorno, S. Gopi, M. Javaheripi, P. Kauffmann, G. de Rosa, O. Saarikivi et al., “Textbooks are all you need,” arXiv preprint arXiv:2306.11644, 2023.\n",
    "[208] Y. Li, S. Bubeck, R. Eldan, A. Del Giorno, S. Gunasekar, and Y. T. Lee, “Textbooks are all you need ii: phi-1.5 technical report,” arXiv preprint arXiv:2309.05463, 2023.\n",
    "[209] M. Poli, S. Massaroli, E. Nguyen, D. Y. Fu, T. Dao, S. Baccus, Y. Bengio, S. Ermon, and C. Re ́, “Hyena hierarchy: Towards larger convolutional language models,” 2023.\n",
    "[210] M. Poli, J. Wang, S. Massaroli, J. Quesnelle, E. Nguyen, and A. Thomas, “StripedHyena: Moving Beyond Transformers with Hybrid Signal Processing Models,” 12 2023. [Online]. Available: https://github.com/togethercomputer/stripedhyena\n",
    "[211] D. Y. Fu, S. Arora, J. Grogan, I. Johnson, S. Eyuboglu, A. W. Thomas, B. Spector, M. Poli, A. Rudra, and C. Re ́, “Monarch mixer: A simple sub-quadratic gemm-based architecture,” 2023.\n",
    "[212] G. J. McLachlan, S. X. Lee, and S. I. Rathnayake, “Finite mixture models,” Annual review of statistics and its application, vol. 6, pp. 355–378, 2019.\n",
    "[213] H. Liu, C. Li, Q. Wu, and Y. J. Lee, “Visual instruction tuning,” arXiv preprint arXiv:2304.08485, 2023.\n",
    "[214] S. Liu, H. Cheng, H. Liu, H. Zhang, F. Li, T. Ren, X. Zou, J. Yang, H. Su, J. Zhu, L. Zhang, J. Gao, and C. Li, “Llava-plus: Learning to use tools for creating multimodal agents,” arXiv preprint arXiv:2311.05437, 2023.\n",
    "[215] S. Wu, H. Fei, L. Qu, W. Ji, and T.-S. Chua, “Next-gpt: Any-to-any multimodal llm,” arXiv preprint arXiv:2309.05519, 2023. \n",
    "[216] N. N. Khasmakhi, M. Asgari-Chenaghlu, N. Asghar, P. Schaer, and D. Zu ̈hlke, “Convgenvismo: Evaluation of conversational generative vision models,” 2023.\n",
    "[217] L. Sun, Y. Huang, H. Wang, S. Wu, Q. Zhang, C. Gao, Y. Huang, W. Lyu, Y. Zhang, X. Li et al., “Trustllm: Trustworthiness in large language models,” arXiv preprint arXiv:2401.05561, 2024.\n",
    "[218] Microsoft. Deepspeed. [Online]. Available: https://github.com/microsoft/DeepSpeed\n",
    "[219] HuggingFace. Transformers. [Online]. Available: https://github.com/huggingface/transformers\n",
    "[220] Nvidia. Megatron. [Online]. Available: https://github.com/NVIDIA/Megatron-LM\n",
    "[221] BMTrain. Bmtrain. [Online]. Available: https://github.com/OpenBMB/BMTrain\n",
    "[222] EleutherAI. gpt-neox. [Online]. Available: https://github.com/EleutherAI/gpt-neox\n",
    "[223] Microsoft. Lora. [Online]. Available: https://github.com/microsoft/LoRA\n",
    "[224] ColossalAI. Colossalai. [Online]. Available: https://github.com/hpcaitech/ColossalAI\n",
    "[225] FastChat. Fastchat. [Online]. Available: https://github.com/lm-sys/FastChat\n",
    "[226] skypilot. skypilot. [Online]. Available: https://github.com/skypilot-org/skypilot\n",
    "[227] vllm. vllm. [Online]. Available: https://github.com/vllm-project/vllm\n",
    "[228] huggingface. text-generation-inference. [Online]. Available: https://github.com/huggingface/text-generation-inference\n",
    "[229] langchain. langchain. [Online]. Available: https://github.com/langchain-ai/langchain\n",
    "[230] bentoml. Openllm. [Online]. Available: https://github.com/bentoml/OpenLLM\n",
    "[231] embedchain. embedchain. [Online]. Available: https://github.com/embedchain/embedchain\n",
    "[232] microsoft. autogen. [Online]. Available: https://github.com/microsoft/autogen\n",
    "[233] babyagi. babyagi. [Online]. Available: https://github.com/yoheinakajima/babyagi\n",
    "[234] guidance. guidance. [Online]. Available: github.com/guidance-ai/guidance\n",
    "[235] prompttools. prompttools. [Online]. Available: github.com/hegelai/prompttools\n",
    "[236] promptfoo. promptfoo. [Online]. Available: github.com/promptfoo/promptfoo\n",
    "[237] facebook. faiss. [Online]. Available: github.com/facebookresearch/faiss\n",
    "[238] milvus. milvus. [Online]. Available: https://github.com/milvus-io/milvus\n",
    "[239] qdrant. qdrant. [Online]. Available: https://github.com/qdrant/qdrant\n",
    "[240] weaviate. weaviate. [Online]. Available: https://github.com/weaviate/weaviate\n",
    "[241] llama index. llama-index. [Online]. Available: https://github.com/run-llama/llamaindex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095bb322-91af-49de-9f60-5d0fce617257",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
