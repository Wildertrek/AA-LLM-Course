{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "badcc535-ed8b-4049-85ad-5b48c08675f0",
   "metadata": {},
   "source": [
    "<a href=\"https://cloud.google.com/ai/llms\" target=\"_blank\">\n",
    "    <img src=\"../../LLM-Images/Google-AI.png\" alt=\"Google AI Logo\" width=\"300\" height=\"100\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "</a>\n",
    "\n",
    "\n",
    "- [Google Research](https://research.google/)\n",
    "- [Google Research Areas](https://research.google/research-areas/)\n",
    "- [Gemini Login](https://gemini.google.com/app)\n",
    "- [Google AI](https://ai.google/)\n",
    "- [Google AI Leading Models](https://ai.google/discover/our-models/)\n",
    "- Build AI agents and generative AI applications with our [fully managed AI platform, Vertex AI—enhanced by Gemini](https://cloud.google.com/vertex-ai), over 150 foundation models, and [ecosystem of AI partners](https://cloud.google.com/partners/ai?hl=en)\n",
    "- Improve customer service with Contact Center AI’s [virtual agents](https://cloud.google.com/solutions/contact-center) and conversational AI products like [Speech-to-Text](https://cloud.google.com/speech-to-text)\n",
    "- Get [AI-powered code generation, recommendations, and completion](https://cloud.google.com/products/gemini/code-assist) from Gemini Code Assist\n",
    "- Explore our ecosystem of [Gemini products for businesses and developers](https://cloud.google.com/ai/gemini) to help you get the most out of Google AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cbcde6-088d-4bfa-ae0b-5f0e0ae837a9",
   "metadata": {},
   "source": [
    "##  Google Large Language Models (LLMs):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08acdbf3-a04a-41d6-aa0b-a8450746e8b0",
   "metadata": {},
   "source": [
    "| Model | Series | Parameters | Release | Open Source | \\# Tokens | Training Dataset | Context Window Size | Capabilities | Best At | Architecture | Availability | Input Modalities | Output Modalities |\n",
    "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
    "| [BERT](https://arxiv.org/pdf/1810.04805) | [BERT](https://www.google.com/url?sa=E&source=gmail&q=https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html) | 110M, 340M | 2018 | ✓ | 3.3B | BooksCorpus, English Wikipedia | 512 tokens | Language understanding | Sentence-level NLP tasks | Transformer (Encoder only) | Open source | Text | Text |\n",
    "| [T5](https://arxiv.org/pdf/1910.10683) | [T5](https://www.google.com/url?sa=E&source=gmail&q=https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html) | 220M - 11B | 2019 | ✓ | 745GB | C4 (Colossal Clean Crawled Corpus) | 512 tokens | Text generation and understanding | Flexible NLP tasks | Transformer (Encoder-Decoder) | Open source | Text | Text |\n",
    "| [LaMDA](https://arxiv.org/pdf/2201.08239) | [LaMDA](https://www.google.com/url?sa=E&source=gmail&q=https://blog.google/technology/ai/lamda/)  | 137B | 2021 | ✗ | 1.56T | Diverse internet sources, dialogue-focused | 4096 tokens | Conversational AI, dialogue | Open-ended dialogue | Transformer (Decoder only) | Google products (e.g., Bard) | Text | Text |\n",
    "| [MUM](https://blog.google/products/search/introducing-mum/) | [MUM](https://blog.google/products/search/introducing-mum/) | 1T+ | 2021 | ✗ | N/A | Multilingual web content | 2048 tokens | Multimodal, multilingual understanding | Complex search and information tasks |  Transformer (Encoder-Decoder) | Google Search | Text, Images | Text |\n",
    "| [PaLM](https://arxiv.org/pdf/2204.02311) | [PaLM](https://www.google.com/url?sa=E&source=gmail&q=https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html) | 540B | 2022 | ✗ | 780B | High-quality web and multilingual datasets | 4096 tokens | General NLP, reasoning | Complex language generation | Transformer (Decoder only) | Google AI Platform | Text | Text |\n",
    "| [Med-PaLM](https://arxiv.org/pdf/2212.13138) | [PaLM](https://www.google.com/url?sa=E&source=gmail&q=https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html) | 540B | 2022 | ✗ | 780B | Medical-specific texts and data | 4096 tokens | Medical knowledge and Q\\&A | Healthcare, clinical queries | Transformer (Decoder only) |  Google AI Platform | Text | Text |\n",
    "| [Gemini 1](https://ai.googleblog.com/2023/10/introducing-gemini-generative-ai-with.html) | [Gemini](https://ai.googleblog.com/2023/10/introducing-gemini-generative-ai-with.html) | 2B, 8B, 70B | 2023 | ✗ | N/A | Curated text, multimodal, multilingual | 8192 tokens | Text and image understanding | Multimodal and multilingual tasks |  Mixture of Experts | Google AI Platform | Text, Images | Text, Images |\n",
    "| [Gemini 1.5](https://ai.googleblog.com/2024/02/announcing-gemini-15-expanding-generative.html) | [Gemini](https://ai.googleblog.com/2023/10/introducing-gemini-generative-ai-with.html) | 100B, 300B | 2024 | ✗ | N/A | Refined multimodal, high-resolution datasets | 8192 tokens | High-resolution image, complex NLP | Advanced image and text synthesis | Mixture of Experts | Google AI Platform | Text, Images, Code | Text, Images, Code |\n",
    "| [FLAN-T5](https://arxiv.org/pdf/2210.11416) | [T5](https://www.google.com/url?sa=E&source=gmail&q=https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html) | 220M - 11B | 2023 | ✓ | 745GB | C4 dataset, task-focused fine-tuning | 2048 tokens | Instruction-following, task specialization | Flexible task-based NLP | Transformer (Encoder-Decoder) | Open source | Text | Text |\n",
    "| [Med-PaLM 2](https://arxiv.org/pdf/2305.01610) | [PaLM](https://www.google.com/url?sa=E&source=gmail&q=https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html) | 340B | 2023 | ✗ | N/A | Medical journals, healthcare records | 4096 tokens | Clinical and healthcare-focused Q\\&A | Medical NLP tasks | Transformer (Decoder only) | Google AI Platform | Text | Text |\n",
    "| [Gemini Nano](https://ai.googleblog.com/2023/11/introducing-gemini-nano-for-on-device.html) | [Gemini](https://ai.googleblog.com/2023/10/introducing-gemini-generative-ai-with.html) | 1B - 3B | 2024 | ✗ | Lightweight | Curated text and multimodal | 2048 tokens | On-device NLP, edge deployment | Low-power, on-device NLP |  Mixture of Experts |  Android devices | Text, Images | Text |\n",
    "| [Imagen](https://www.google.com/url?sa=E&source=gmail&q=https://imagen.research.google/) | [Imagen](https://www.google.com/url?sa=E&source=gmail&q=https://imagen.research.google/) | N/A | 2022 | ✗ | N/A |  Large image and text datasets | N/A |  Text-to-image generation | High-quality image generation from text descriptions | Diffusion model | Google AI Platform | Text | Images |\n",
    "| [Parti](https://www.google.com/url?sa=E&source=gmail&q=https://parti.research.google/) | [Parti](https://www.google.com/url?sa=E&source=gmail&q=https://parti.research.google/) | N/A | 2022 | ✗ | N/A | Web-scale image and text data | N/A | Text-to-image generation | Photorealistic and creative image generation |  Autoregressive model | Google AI Platform | Text | Images |\n",
    "| [Codey]() | [Codey]() | N/A | 2023 | ✗ | N/A | Code and text from GitHub, Google internal data | N/A  | Code generation, code completion, code translation |  Improving developer productivity | Transformer-based | Google Colab, Google Cloud | Code, Text | Code |\n",
    "| [Chirp]() | [Chirp]() | N/A | 2022 | ✗ | N/A |  Large speech datasets, including under-resourced languages | N/A | Speech recognition, speech-to-text |  Accurate speech-to-text, especially for low-resource languages |  Transformer-based | Google AI Platform | Speech | Text |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92555fbe-c5bc-4f1b-9ef4-c36c91852203",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
