{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a51c070-afbf-4ab0-b24a-456ea3183a0a",
   "metadata": {},
   "source": [
    "# 1972 - term frequency-inverse document frequency (TF-IDF)\n",
    "\n",
    "[1972 TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n",
    " TF-IDF expanded on BOW by giving more weight to rare words and less to common terms, improving the model’s ability to detect document relevancy. Nonetheless, it made no mention of word context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a7f664-85c8-4956-a6b4-04e72fccf8fc",
   "metadata": {},
   "source": [
    "This Jupyter notebook for 1972 TF-IDF, we will follow a similar approach as the Bag of Words (BoW) model, but this time we will include the term frequency-inverse document frequency (TF-IDF) to give more weight to rarer words. This helps in improving the model's ability to identify document relevancy by reducing the impact of common words.\n",
    "\n",
    "Here’s the step-by-step process for our notebook:\n",
    "\n",
    "### Step-by-Step Explanation:\n",
    "\n",
    "1. **Collect Words and Calculate Frequencies**: As with the BoW model, we start by creating a list of unique words from all the sentences.\n",
    "2. **Calculate Term Frequency (TF)**: For each word in a sentence, we calculate the term frequency, which is the number of times a word appears in a sentence divided by the total number of words in that sentence.\n",
    "3. **Calculate Inverse Document Frequency (IDF)**: We then calculate the inverse document frequency for each word, which is the logarithm of the total number of documents divided by the number of documents that contain the word. This helps in giving less importance to common words across all documents.\n",
    "4. **Calculate TF-IDF**: Finally, we multiply the TF and IDF values to get the TF-IDF score for each word in each document.\n",
    "5. **Create a TF-IDF Table**: Similar to the BoW model, we put this information into a table where each row represents a sentence and each column represents a word from our vocabulary.\n",
    "\n",
    "### How It Works:\n",
    "- The `TfidfVectorizer` in scikit-learn is used to automatically compute the TF-IDF scores for each word in each document.\n",
    "- The `fit_transform` method learns the vocabulary from the documents and calculates the TF-IDF scores.\n",
    "- The resulting matrix gives a numeric representation of the importance of each word in each sentence, helping in text analysis tasks.\n",
    "\n",
    "### Explanation of the Output:\n",
    "- **Vocabulary**: Lists all the unique words found in the documents.\n",
    "- **TF-IDF Matrix**: Provides the TF-IDF score for each word in each document, showing the importance of each word within the context of the document.\n",
    "\n",
    "This code snippet provides a straightforward way to demonstrate the use of TF-IDF in text analysis, building on the foundation of the Bag of Words model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d0dc2c2-eec9-4050-ab8b-79ae94284013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      " ['and' 'brown' 'dog' 'fast' 'fox' 'is' 'jump' 'jumps' 'lazy' 'never'\n",
      " 'over' 'quick' 'quickly' 'the']\n",
      "\n",
      "TF-IDF Matrix:\n",
      " [[0.         0.31401745 0.31401745 0.         0.31401745 0.\n",
      "  0.         0.41289521 0.31401745 0.         0.31401745 0.31401745\n",
      "  0.         0.48772512]\n",
      " [0.         0.         0.33729513 0.         0.         0.\n",
      "  0.44350256 0.         0.33729513 0.44350256 0.33729513 0.\n",
      "  0.44350256 0.26193976]\n",
      " [0.38294157 0.29123694 0.         0.38294157 0.29123694 0.38294157\n",
      "  0.         0.         0.         0.         0.         0.58247387\n",
      "  0.         0.22617146]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Never jump over the lazy dog quickly.\",\n",
    "    \"The quick brown fox is quick and fast.\"\n",
    "]\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "# TfidfVectorizer will tokenize the documents, calculate the TF-IDF score for each word\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the documents\n",
    "# This step learns the vocabulary from the documents and transforms them into a TF-IDF matrix\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get the vocabulary (unique words in the corpus)\n",
    "# get_feature_names_out() provides the list of all unique words in the documents\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the TF-IDF matrix to an array\n",
    "# tfidf_matrix.toarray() converts the sparse matrix to a dense numpy array for easier viewing\n",
    "tfidf_array = tfidf_matrix.toarray()\n",
    "\n",
    "# Display the results\n",
    "# Print the vocabulary which lists all unique words in the documents\n",
    "print(\"Vocabulary:\\n\", vocab)\n",
    "# Print the TF-IDF matrix which shows the TF-IDF score of each word in each document\n",
    "print(\"\\nTF-IDF Matrix:\\n\", tfidf_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8eaf30-6c06-4409-a093-ab13065e426d",
   "metadata": {},
   "source": [
    "### TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "\n",
    "TF-IDF is a statistical measure used to evaluate the importance of a word in a document relative to a collection of documents (corpus). It is the product of two values: Term Frequency (TF) and Inverse Document Frequency (IDF).\n",
    "\n",
    "#### 1. Term Frequency (TF)\n",
    "The term frequency $ \\text{TF}(w_i, D_j) $ of a word $ w_i $ in a document $ D_j $ is defined as:\n",
    "\n",
    "$$\n",
    "\\text{TF}(w_i, D_j) = \\frac{f(w_i, D_j)}{\\sum_{k} f(w_k, D_j)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ f(w_i, D_j) $ is the frequency of word $ w_i $ in document $ D_j $.\n",
    "- The denominator is the total number of words in document $ D_j $.\n",
    "\n",
    "#### 2. Inverse Document Frequency (IDF)\n",
    "The inverse document frequency $ \\text{IDF}(w_i, \\mathcal{D}) $ for a word $ w_i $ in a corpus $ \\mathcal{D} $ is given by:\n",
    "\n",
    "$$\n",
    "\\text{IDF}(w_i, \\mathcal{D}) = \\log \\left( \\frac{N}{|\\{ D_j \\in \\mathcal{D} : w_i \\in D_j \\}|} \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ N $ is the total number of documents in the corpus $ \\mathcal{D} $.\n",
    "- $ |\\{ D_j \\in \\mathcal{D} : w_i \\in D_j \\}| $ is the number of documents that contain the word $ w_i $.\n",
    "\n",
    "#### 3. TF-IDF Score\n",
    "The TF-IDF score for a word $ w_i $ in a document $ D_j $ is then computed as:\n",
    "\n",
    "$$\n",
    "\\text{TF-IDF}(w_i, D_j, \\mathcal{D}) = \\text{TF}(w_i, D_j) \\times \\text{IDF}(w_i, \\mathcal{D})\n",
    "$$\n",
    "\n",
    "#### 4. TF-IDF Matrix for the Corpus\n",
    "Given a corpus $ \\mathcal{D} $ with documents $ D_1, D_2, \\ldots, D_m $ and a vocabulary of words $ w_1, w_2, \\ldots, w_n $, the TF-IDF matrix $ T $ for the corpus is an $ m \\times n $ matrix where each element is the TF-IDF score for the corresponding word in the document:\n",
    "\n",
    "$$\n",
    "T = \n",
    "\\begin{bmatrix}\n",
    "\\text{TF-IDF}(w_1, D_1, \\mathcal{D}) & \\text{TF-IDF}(w_2, D_1, \\mathcal{D}) & \\cdots & \\text{TF-IDF}(w_n, D_1, \\mathcal{D}) \\\\\n",
    "\\text{TF-IDF}(w_1, D_2, \\mathcal{D}) & \\text{TF-IDF}(w_2, D_2, \\mathcal{D}) & \\cdots & \\text{TF-IDF}(w_n, D_2, \\mathcal{D}) \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\text{TF-IDF}(w_1, D_m, \\mathcal{D}) & \\text{TF-IDF}(w_2, D_m, \\mathcal{D}) & \\cdots & \\text{TF-IDF}(w_n, D_m, \\mathcal{D}) \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Explanation in the Context of the Code\n",
    "- **Vocabulary**: The set of unique words $ w_1, w_2, \\ldots, w_n $ is extracted from the documents.\n",
    "- **TF-IDF Matrix**: Each document $ D_j $ is transformed into a vector $ \\text{TF-IDF}(D_j) $ using the TF-IDF scores for each word in the vocabulary, forming an $ m \\times n $ matrix $ T $.\n",
    "- The TF-IDF vectorizer in the code computes this matrix, where $ m $ is the number of documents, and $ n $ is the number of unique words in the vocabulary. Each cell in the matrix represents the TF-IDF score for a specific word in a specific document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fe5622-1d73-405c-be1e-1fbae3324d3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
