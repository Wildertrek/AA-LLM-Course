{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55a36e07-287a-4ea8-b359-9d9b72583346",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "\n",
    "Embedding models are essential components in both Retrieval-Augmented Generation (RAG) pipelines and Large Language Models (LLMs). They transform textual data into numerical vectors, capturing semantic meanings that facilitate efficient information retrieval and generation.\n",
    "\n",
    "**Embedding Models in RAG Pipelines**\n",
    "\n",
    "In RAG systems, embedding models play a pivotal role in retrieving relevant information to enhance the generation process. The typical workflow involves:\n",
    "\n",
    "1. **Data Ingestion and Processing**: Collecting and preparing external knowledge sources, such as documents or databases.\n",
    "\n",
    "2. **Chunking**: Dividing large texts into manageable pieces to improve retrieval accuracy.\n",
    "\n",
    "3. **Embedding**: Converting text chunks into vector representations using embedding models.\n",
    "\n",
    "4. **Storage in Vector Databases**: Storing these vectors in databases optimized for similarity searches.\n",
    "\n",
    "5. **Query Embedding and Retrieval**: Transforming user queries into vectors to retrieve semantically similar information from the database.\n",
    "\n",
    "6. **Generation**: Using the retrieved information to generate accurate and contextually relevant responses.\n",
    "\n",
    "This process ensures that the LLM can access up-to-date and pertinent information, enhancing its responses. ([amazee.io](https://www.amazee.io/blog/post/data-pipelines-for-rag))\n",
    "\n",
    "**Types of Embedding Models**\n",
    "\n",
    "Embedding models can be categorized based on their architecture:\n",
    "\n",
    "- **Bi-Encoders**: Process input texts independently to produce vector embeddings. They are efficient for large-scale retrieval tasks since embeddings can be precomputed and stored. However, they might miss intricate relationships between text pairs.\n",
    "\n",
    "- **Cross-Encoders**: Process text pairs jointly, allowing the model to consider the interaction between them. While they often achieve higher accuracy in tasks like reranking, they are computationally intensive and less suitable for real-time retrieval.\n",
    "\n",
    "In practice, a combination of both is often employed: using Bi-Encoders for initial retrieval and Cross-Encoders for reranking the results to improve precision. ([Unstructured Data ETL](https://unstructured.io/blog/understanding-embedding-models-make-an-informed-choice-for-your-rag))\n",
    "\n",
    "**Popular Embedding Models**\n",
    "\n",
    "Several embedding models are widely used in RAG pipelines and LLMs:\n",
    "\n",
    "- **BERT (Bidirectional Encoder Representations from Transformers)**: Captures context from both directions, making it effective for understanding the meaning of words in context.\n",
    "\n",
    "- **SBERT (Sentence-BERT)**: An extension of BERT that uses a Siamese network structure to derive semantically meaningful sentence embeddings, improving performance in tasks like semantic textual similarity and information retrieval.\n",
    "\n",
    "- **OpenAI's text-embedding-ada-002**: Known for its strong performance in various embedding tasks, offering a balance between efficiency and accuracy.\n",
    "\n",
    "The choice of an embedding model depends on factors such as the specific application, computational resources, and the nature of the data. It's often beneficial to experiment with multiple models to determine the best fit for a particular use case. ([HackerNoon](https://hackernoon.com/embeddings-for-rag-a-complete-overview))\n",
    "\n",
    "For a more in-depth understanding, you might find this video helpful:\n",
    "\n",
    "[Choosing Embedding Models for RAG Applications](https://www.youtube.com/watch?v=dN0lsF2cvm4)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce35bb3-8630-42d1-920e-2b224b7dcccc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
