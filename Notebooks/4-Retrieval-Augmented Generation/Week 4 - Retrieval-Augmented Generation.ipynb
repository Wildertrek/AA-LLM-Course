{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f5dbf34-2146-4c57-aca2-b20c8b460d70",
   "metadata": {},
   "source": [
    "# **Week 4: Retrieval-Augmented Generation (RAG)**\n",
    "\n",
    "- **Topics:** RAG architecture and concepts, vector search concepts, similarity and distance metrics, indexing strategies, using vectors in document retrieval and LLMs.\n",
    "- **Hands-on:** Building a basic RAG pipeline with pre-trained models, implementing a vector search mechanism with a document corpus.\n",
    "\n",
    "notes: create simple embedding concept in the foundations section. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ba2da8-4bd9-4453-8fa0-71cd40d19e9d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Designing the RAG Pipeline for the Medical AI Assistant**\n",
    "\n",
    "### **Overview**\n",
    "\n",
    "We have **two primary ways** to implement this pipeline:\n",
    "\n",
    "1. **Azure Public Sector Information Assistant** (Production-Ready)  \n",
    "   - Leverages **Azure OpenAI** for GPT-4o, which is approved for government usage.  \n",
    "   - Uses **`text-embedding-ada-002`** due to current availability on Azure Government.\n",
    "   - Integrates seamlessly with **Azure AI Search** (or other Azure services) for vector storage and retrieval.\n",
    "\n",
    "2. **Open-Source + OpenAI** (Learning & Experimentation)  \n",
    "   - Uses **OpenAI’s latest `text-embedding-3-small`** (a newer embedding model).  \n",
    "   - Employs **FAISS** or other open-source vector databases.  \n",
    "   - Provides a **lightweight** local environment suitable for hands-on projects and prototyping.\n",
    "\n",
    "---\n",
    "\n",
    "### **Objective**\n",
    "\n",
    "1. **Retrieve** relevant clinical/medical knowledge (e.g., TIU notes, ICD codes, guidelines).  \n",
    "2. **Generate** accurate, contextually relevant, and actionable responses for clinicians.  \n",
    "3. **Support** both:\n",
    "   - **Azure**-based production deployments (where compliance and scaling are paramount).  \n",
    "   - **Open-source** setups for students, researchers, or smaller organizations without Azure.\n",
    "\n",
    "---\n",
    "\n",
    "### **Core Components of the Pipeline**\n",
    "\n",
    "1. **Document Corpus**  \n",
    "   - **Clinical Data**: TIU notes, radiology reports, structured data like ICD codes.  \n",
    "   - **Guidelines/Encyclopedias**: VA/DoD guidelines, MedlinePlus, other medical references.  \n",
    "\n",
    "2. **Embedding Model**  \n",
    "   - **Azure Path**: `text-embedding-ada-002` (current Azure Government option).  \n",
    "   - **Open-Source Path**: `text-embedding-3-small` (newer, improved model from OpenAI).  \n",
    "\n",
    "3. **Vector Store**  \n",
    "   - **Azure Path**: Azure AI Search with vector search enabled.  \n",
    "   - **Open-Source Path**: FAISS, Milvus, or Weaviate for storing embeddings locally.  \n",
    "\n",
    "4. **Retrieval & Ranking**  \n",
    "   - Pull **top-k** documents or chunks based on embedding similarity.  \n",
    "   - Optionally filter by metadata (e.g., document type, specialty).  \n",
    "\n",
    "5. **Context-Aware Generation**  \n",
    "   - Combine user’s query with retrieved chunks.  \n",
    "   - Pass to a Large Language Model (LLM) such as GPT-4 (Azure or non-Azure) for the final response.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Design Framework**\n",
    "\n",
    "1. **Data Ingestion & Preparation**  \n",
    "   - **Collect** all relevant medical data sources.  \n",
    "   - **Preprocess** (clean, chunk, metadata).  \n",
    "\n",
    "2. **Embedding Generation**  \n",
    "   - **Azure**: Use Azure OpenAI with `text-embedding-ada-002`.  \n",
    "   - **Open-Source**: Use OpenAI’s `text-embedding-3-small` .  \n",
    "\n",
    "3. **Vector Indexing**  \n",
    "   - **Azure**: Store embeddings and metadata in Azure AI Search.  \n",
    "   - **Open-Source**: Use FAISS or another local vector DB.  \n",
    "\n",
    "4. **Query & Retrieval**  \n",
    "   - Embed the user query and perform a **similarity search** for top matches.  \n",
    "   - Filter or rank results as needed (e.g., by specialty, recent date).  \n",
    "\n",
    "5. **Augmented Prompt & LLM Generation**  \n",
    "   - Append retrieved context to the user query.  \n",
    "   - Send to GPT-4o (Azure GPT-4o) or open-source LLM for a contextualized response.  \n",
    "\n",
    "6. **Testing & Iteration**  \n",
    "   - **Validate** retrieval accuracy and response quality.  \n",
    "   - **Incorporate feedback** from clinicians or test cases.  \n",
    "   - **Refine** embeddings, prompts, or indexing strategies as data evolves.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Key Considerations**\n",
    "\n",
    "1. **Security & Compliance**  \n",
    "   - **Azure**: FedRAMP and HIPAA-compliant environment for PHI and sensitive data.  \n",
    "   - **Open-Source**: Ensure anonymized or synthetic data if hosting locally.  \n",
    "\n",
    "2. **Scalability**  \n",
    "   - Azure AI Search can handle production-scale loads.  \n",
    "   - FAISS is fine for prototypes or moderate datasets; may need advanced indexing for very large corpora.  \n",
    "\n",
    "3. **Future-Readiness**  \n",
    "   - Azure Government may eventually adopt `text-embedding-3-small`; code can switch easily when it’s available.  \n",
    "   - The open-source path can integrate new embeddings or LLMs as they are released.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Next Steps**\n",
    "\n",
    "1. **Choose Your Path**  \n",
    "   - If you have Azure Government access: Use [PubSec-Info-Assistant](https://github.com/microsoft/PubSec-Info-Assistant) with GPT-4o and `text-embedding-ada-002`.  \n",
    "   - Otherwise: Build a local pipeline with the open-source approach using `text-embedding-3-small` and FAISS.  \n",
    "\n",
    "2. **Implement End-to-End**  \n",
    "   - **Data ingestion** → **Chunking** → **Embedding** → **Indexing** → **Retrieval** → **LLM Generation**.  \n",
    "   - Validate with test queries (e.g., “What are the causes of chest pain?”).  \n",
    "\n",
    "3. **Evaluate**  \n",
    "   - Compare response quality and retrieval performance under both approaches.  \n",
    "   - Gather user feedback for iterative improvements.  RLHF? \n",
    "\n",
    "By following this **dual-track setup**, you can **rapidly prototype** in open-source while maintaining a **production-ready** path via Azure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f51bd74-590d-4709-86a4-25c407993e3a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Prioritizing Data Sources for the Vector Store**\n",
    "\n",
    "As part of our **Medical AI Assistant RAG pipeline**, we need to carefully select and organize data sources for maximum clinical relevance and efficient retrieval. Below is a **prioritized list** of data sources, grouped by tiers according to their **clinical impact**, **retrieval value**, and **ease of integration**.\n",
    "\n",
    "> **Note**:  \n",
    "> - **Azure Path**: You’ll embed these data sources using `text-embedding-ada-002` (currently available in Azure Government) and store them in **Azure AI Search**.  \n",
    "> - **Open-Source Path**: You can embed the same sources using `text-embedding-3-small` (or another open model) and store them in **FAISS** (or Milvus/Weaviate).\n",
    "\n",
    "---\n",
    "\n",
    "### **Tier 1: High-Priority Data Sources**\n",
    "\n",
    "Focus on these first to quickly boost the Assistant’s clinical utility.\n",
    "\n",
    "1. **TIU Notes (Text Integration Utility Notes)**  \n",
    "   - **Relevance**: Direct clinical context, critical for real-world patient scenarios (after thorough **de-identification**).  \n",
    "   - **Examples**: SOAP notes, admission/discharge summaries, progress notes.  \n",
    "   - **Integration**:\n",
    "     - **Azure**: Use Azure OpenAI embeddings → store in Azure AI Search index with metadata like patient condition or specialty.  \n",
    "     - **Open-Source**: Embed locally with `text-embedding-3-small` → store vectors in FAISS alongside chunk metadata.  \n",
    "   - **Challenge**: Anonymizing patient data while retaining key clinical details.\n",
    "\n",
    "2. **ICD Codes (International Classification of Diseases)**  \n",
    "   - **Relevance**: Structured mapping of diagnoses, essential for linking symptoms to conditions.  \n",
    "   - **Examples**: ICD-10 codes like “I20” (Angina Pectoris) or “K21” (GERD).  \n",
    "   - **Integration**:\n",
    "     - **Azure**: Enable Azure AI Search filters (e.g., code ranges).  \n",
    "     - **Open-Source**: Store codes + descriptions in a separate FAISS partition or index for structured lookups.  \n",
    "   - **Challenge**: Keeping up with ICD revisions (ICD-11, etc.).\n",
    "\n",
    "3. **VA/DoD Clinical Guidelines**  \n",
    "   - **Relevance**: Anchors responses in **VA-specific, evidence-based** practices.  \n",
    "   - **Examples**: PTSD guidelines, diabetes management, hypertension treatment.  \n",
    "   - **Integration**:\n",
    "     - **Azure**: Store guideline documents in a dedicated Azure AI Search index.  \n",
    "     - **Open-Source**: Chunk lengthy guidelines, embed, and store in FAISS.  \n",
    "   - **Challenge**: Document formatting inconsistencies.\n",
    "\n",
    "---\n",
    "\n",
    "### **Tier 2: Medium-Priority Data Sources**\n",
    "\n",
    "Enhance the Assistant’s breadth of knowledge after Tier 1 is integrated.\n",
    "\n",
    "4. **MedlinePlus Consumer Health Information**  \n",
    "   - **Relevance**: Plain-language medical overviews, good for patient-facing explanations.  \n",
    "   - **Examples**: Summaries on GERD, hypertension treatments.  \n",
    "   - **Integration**:\n",
    "     - **Azure**: Create a “consumer-info” index for user-friendly content.  \n",
    "     - **Open-Source**: Embeddings in FAISS with metadata like “reading level.”  \n",
    "   - **Challenge**: Merging layperson info with professional context.\n",
    "\n",
    "5. **SNOMED CT (Systematized Nomenclature of Medicine)**  \n",
    "   - **Relevance**: Adds a rich hierarchy of medical terms and relationships.  \n",
    "   - **Examples**: “Chest Pain” → “Cardiac Chest Pain” → “Angina.”  \n",
    "   - **Integration**:\n",
    "     - **Azure**: Use AI Search scoring profiles for hierarchical concept matching.  \n",
    "     - **Open-Source**: Store as structured text + embeddings for concept-based queries.  \n",
    "   - **Challenge**: Large vocabulary can impact indexing time and retrieval speed.\n",
    "\n",
    "6. **Drug Databases**  \n",
    "   - **Relevance**: Critical for addressing medication queries (dosages, interactions).  \n",
    "   - **Examples**: DailyMed (FDA labels), Lexicomp data.  \n",
    "   - **Integration**:\n",
    "     - **Azure**: Use role-based access for any proprietary data.  \n",
    "     - **Open-Source**: Possibly store publicly available FDA labels.  \n",
    "   - **Challenge**: Licensing constraints for certain commercial databases.\n",
    "\n",
    "---\n",
    "\n",
    "### **Tier 3: Supplemental Data Sources**\n",
    "\n",
    "Use these for **specialized** or advanced capabilities beyond the core clinical scenario.\n",
    "\n",
    "7. **DSM-5 Criteria (Mental Health)**  \n",
    "   - **Relevance**: Addresses mental health queries (e.g., PTSD, anxiety) for the VA population.  \n",
    "   - **Examples**: PTSD diagnostic criteria, depression screening.  \n",
    "   - **Integration**:\n",
    "     - **Azure**: Tag relevant DSM-5 sections, embed in a dedicated index.  \n",
    "     - **Open-Source**: Chunk by disorder or symptom set.  \n",
    "   - **Challenge**: Overly granular data might overwhelm the retrieval system.\n",
    "\n",
    "8. **PubMed Abstracts (Biomedical Literature)**  \n",
    "   - **Relevance**: Access to latest research—particularly for clinician-facing answers.  \n",
    "   - **Examples**: New treatment studies for atrial fibrillation.  \n",
    "   - **Integration**:\n",
    "     - **Azure**: Use semantic ranking in AI Search for more complex queries.  \n",
    "     - **Open-Source**: Implement an “abstracts” index in FAISS or Milvus.  \n",
    "   - **Challenge**: Need filtering by date, journal, or topic to ensure relevance.\n",
    "\n",
    "9. **Radiology and Lab Reports**  \n",
    "   - **Relevance**: Diagnostic context for imaging or lab-based queries (e.g., elevated troponin = possible MI).  \n",
    "   - **Examples**: Terms like “ground-glass opacity” or “microcytic anemia.”  \n",
    "   - **Integration**:\n",
    "     - **Azure**: Possibly store extracted text from structured DICOM or HL7.  \n",
    "     - **Open-Source**: Preprocess to highlight key terms (lab values, imaging findings).  \n",
    "   - **Challenge**: Requires specialized parsers to convert raw data into text.\n",
    "\n",
    "---\n",
    "\n",
    "### **Proposed Integration Timeline**\n",
    "\n",
    "1. **Phase 1 (Essential)**  \n",
    "   - **TIU Notes** (anonymized)  \n",
    "   - **ICD Codes**  \n",
    "   - **VA/DoD Guidelines**\n",
    "\n",
    "2. **Phase 2 (Enhanced)**  \n",
    "   - **MedlinePlus**  \n",
    "   - **SNOMED CT**  \n",
    "   - **Drug Databases**\n",
    "\n",
    "3. **Phase 3 (Advanced)**  \n",
    "   - **DSM-5**  \n",
    "   - **PubMed Abstracts**  \n",
    "   - **Radiology/Lab Reports**\n",
    "\n",
    "---\n",
    "\n",
    "### **Design Considerations**\n",
    "\n",
    "#### **1. Metadata Schema**\n",
    "- **Possible Fields**: \n",
    "  - `source_type` (TIU note, ICD code, guideline)  \n",
    "  - `specialty` (cardiology, gastroenterology)  \n",
    "  - `date` (e.g., 2023-10-01)  \n",
    "  - `source` (EHR, MedlinePlus, PubMed)  \n",
    "- **Azure**: Use custom analyzers or fields in Azure AI Search.  \n",
    "- **Open-Source**: Store metadata in a Python dictionary or separate DB, linking to vector IDs.\n",
    "\n",
    "#### **2. Storage and Partitioning**\n",
    "- **Indexes**:\n",
    "  - **Azure**: Create multiple indexes for structured vs. unstructured data.  \n",
    "  - **Open-Source**: Maintain separate FAISS indexes or partitions (e.g., “clinical notes,” “terminologies”).  \n",
    "- **Scalability**:\n",
    "  - Ensure incremental updates as new TIU notes or ICD revisions arrive.\n",
    "\n",
    "#### **3. Embedding Models**\n",
    "- **Azure**: `text-embedding-ada-002` for consistent integration.  \n",
    "- **Open-Source**: `text-embedding-3-small` (newer, improved) or domain-specific models (e.g., ClinicalBERT).\n",
    "\n",
    "#### **4. Retrieval Performance**\n",
    "- **Precision vs. Recall**: Tweak top-k settings, embeddings, or metadata filters.  \n",
    "- **Latency Requirements**: Consider caching for high-traffic scenarios.\n",
    "\n",
    "---\n",
    "\n",
    "### **Next Steps**\n",
    "\n",
    "1. **Implement Tier 1**  \n",
    "   - Ingest anonymized TIU notes, index ICD codes, and store VA/DoD guidelines.  \n",
    "2. **Set Up Metadata**  \n",
    "   - Finalize fields and indexing strategies for your chosen environment (Azure AI Search or FAISS).  \n",
    "3. **Test & Iterate**  \n",
    "   - Verify retrieval relevance with real medical queries.  \n",
    "   - Expand to Tier 2 sources once Tier 1 is stable.  \n",
    "\n",
    "By focusing on these data sources in **phases**, you can **incrementally** enhance the Medical AI Assistant’s knowledge, ensuring both **immediate clinical value** and a **clear roadmap** for future growth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b169150-cda1-47fd-8dfa-10f77bc58e42",
   "metadata": {},
   "source": [
    "### **Openly Available Public Datasets for Medical AI Assistant**\n",
    "\n",
    "Here’s a curated list of publicly available datasets suitable for building a Medical AI Assistant, along with their descriptions, focus areas, and links for access.\n",
    "\n",
    "| **Dataset Name**               | **Description**                                                                                             | **Focus Area**                                  | **Link**                                                                 |\n",
    "|--------------------------------|-------------------------------------------------------------------------------------------------------------|------------------------------------------------|-------------------------------------------------------------------------|\n",
    "| **MIMIC-III**                  | A large database containing de-identified health data from critical care patients.                          | Clinical text, ICU data                        | [MIMIC-III Dataset](https://physionet.org/content/mimiciii/1.4/)        |\n",
    "| **MIMIC-IV**                   | The successor to MIMIC-III with more recent data, enhanced granularity, and more structured formats.        | Clinical text, ICU data                        | [MIMIC-IV Dataset](https://physionet.org/content/mimiciv/2.2/)          |\n",
    "| **eICU Collaborative Research**| A multi-center critical care database with de-identified patient data from ICUs across the United States.  | ICU data, clinical outcomes                   | [eICU Dataset](https://physionet.org/content/eicu-crd/2.0/)             |\n",
    "| **BioASQ Dataset**             | Biomedical semantic indexing and question answering dataset for natural language processing.               | Biomedical text, QA systems                   | [BioASQ Dataset](http://bioasq.org/)                                    |\n",
    "| **PubMed Central Open Access** | A large repository of free full-text biomedical and life sciences journal articles.                        | Biomedical research, text embeddings           | [PubMed Central](https://www.ncbi.nlm.nih.gov/pmc/tools/openftlist/)    |\n",
    "| **MedlinePlus**                | Consumer-friendly health information about diseases, conditions, and wellness topics.                      | Patient education, health information          | [MedlinePlus](https://medlineplus.gov/)                                 |\n",
    "| **SNOMED CT**                  | A comprehensive, multilingual clinical healthcare terminology standard.                                     | Clinical terminology, medical codes           | [SNOMED CT](https://www.nlm.nih.gov/healthit/snomedct/index.html)       |\n",
    "| **ICD-10 Dataset**             | International Classification of Diseases, Tenth Revision, used for coding diagnoses and procedures.        | Clinical terminology, disease classification   | [ICD-10](https://www.who.int/standards/classifications/classification-of-diseases) |\n",
    "| **COVID-19 Open Research Dataset (CORD-19)** | A dataset of scholarly articles about COVID-19 for text mining and natural language processing research. | Pandemic-related biomedical research           | [CORD-19 Dataset](https://www.semanticscholar.org/cord19)               |\n",
    "| **Unified Medical Language System (UMLS)** | A collection of biomedical vocabularies integrated into a single framework for interoperability.      | Medical vocabularies, terminology             | [UMLS](https://www.nlm.nih.gov/research/umls/index.html)                |\n",
    "| **Disease Ontology**           | A standardized ontology for human disease terms, their definitions, and their relationships.               | Disease classification, ontologies            | [Disease Ontology](http://www.disease-ontology.org/)                    |\n",
    "| **Open-i Medical Image Dataset** | A collection of de-identified medical images with corresponding metadata and reports.                     | Medical imaging, radiology reports             | [Open-i Dataset](https://openi.nlm.nih.gov/)                            |\n",
    "| **CheXpert**                   | A large dataset of chest X-rays labeled for various pathologies.                                           | Radiology, chest diseases                      | [CheXpert Dataset](https://stanfordmlgroup.github.io/competitions/chexpert/) |\n",
    "| **PhysioNet**                  | Open access to complex physiological signals such as ECGs and EEGs.                                         | Physiological data, signal analysis           | [PhysioNet](https://physionet.org/)                                     |\n",
    "| **OHDSI OMOP**                 | Observational health data from a global collaboration focusing on standardized medical research databases.  | Clinical observations, research data           | [OHDSI Dataset](https://www.ohdsi.org/data-standardization/the-common-data-model/) |\n",
    "| **RxNorm**                     | A normalized naming system for generic and branded drugs and their relationships.                          | Drug information, pharmacology                 | [RxNorm](https://www.nlm.nih.gov/research/umls/rxnorm/index.html)       |\n",
    "| **DrugBank Open Data**         | A comprehensive database of drug and drug target information.                                               | Pharmacology, drug-target interactions         | [DrugBank Open Data](https://www.drugbank.com/releases/latest)          |\n",
    "| **ClinicalTrials.gov**         | A database of privately and publicly funded clinical studies conducted around the world.                   | Clinical trials, research studies              | [ClinicalTrials.gov](https://clinicaltrials.gov/)                       |\n",
    "\n",
    "---\n",
    "\n",
    "### **How This Table Can Be Used**\n",
    "1. **Building the Vector Store**:\n",
    "   - Choose datasets relevant to your use case (e.g., TIU notes from MIMIC-IV, disease definitions from Disease Ontology).\n",
    "   - Use embeddings to represent the dataset text and store them for retrieval.\n",
    "\n",
    "2. **Expanding the Knowledge Base**:\n",
    "   - Incorporate data from medical vocabularies (e.g., UMLS, SNOMED CT) to enhance terminology understanding.\n",
    "   - Include patient-facing resources (e.g., MedlinePlus) for consumer-level education.\n",
    "\n",
    "3. **Supporting Specific Applications**:\n",
    "   - Use imaging datasets (e.g., CheXpert) to complement textual data.\n",
    "   - Leverage pharmacological datasets (e.g., DrugBank, RxNorm) for drug-related queries.\n",
    "4. Sign **Data Use Agreements**: for those shareable datasets.\n",
    "\n",
    "5. Use **advanced webcrawlers** such as [Crawl4ai](https://crawl4ai.com/mkdocs/) to retrieve information from websites such as [MedlinePlus Medical Encyclopedia](https://medlineplus.gov/encyclopedia.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf10baa-f60c-491c-b18d-6e43ab84192d",
   "metadata": {},
   "source": [
    "## **Focus: Metadata Schema Design**\n",
    "\n",
    "Metadata is the backbone of effective document retrieval in a vector store. A well-designed schema ensures that documents are organized, searchable, and retrievable based on precise filters. For the Medical AI Assistant, we need a robust schema that supports diverse data types like TIU notes, ICD codes, and clinical guidelines.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Metadata Schema Design**\n",
    "\n",
    "#### **Core Metadata Fields**\n",
    "| **Field Name**      | **Description**                                                                 | **Example Values**                                  |\n",
    "|----------------------|---------------------------------------------------------------------------------|----------------------------------------------------|\n",
    "| **Document Type**    | The category of the document.                                                   | \"TIU Note\", \"ICD Code\", \"Guideline\"               |\n",
    "| **Specialty**        | The medical specialty relevant to the document.                                | \"Cardiology\", \"Gastroenterology\", \"Mental Health\" |\n",
    "| **Source**           | The origin of the document or data.                                             | \"EHR\", \"MedlinePlus\", \"VA Guidelines\"             |\n",
    "| **Title**            | A brief title or summary for the document.                                     | \"Managing GERD in Veterans\"                       |\n",
    "| **Keywords**         | Key terms associated with the document.                                        | \"GERD\", \"angina\", \"chest pain\"                    |\n",
    "| **Date**             | The date the document was created or last updated.                             | \"2024-11-15\"                                       |\n",
    "| **Relevance Score**  | A weight or rank indicating the document's importance for retrieval.            | \"0.85\", \"0.92\"                                    |\n",
    "| **Context Summary**  | A short summary or abstract of the document's content.                         | \"Chest pain caused by GERD, angina, and anxiety.\" |\n",
    "| **Clinical Tags**    | Specific tags for clinical features, diagnoses, or symptoms.                   | \"Chest Pain\", \"MI\", \"Pulmonary Embolism\"          |\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Metadata Examples for Tier 1 Sources**\n",
    "\n",
    "- **TIU Note Example**:\n",
    "  ```json\n",
    "  {\n",
    "      \"Document Type\": \"TIU Note\",\n",
    "      \"Specialty\": \"Cardiology\",\n",
    "      \"Source\": \"EHR\",\n",
    "      \"Title\": \"Progress Note for Chest Pain\",\n",
    "      \"Keywords\": [\"chest pain\", \"angina\", \"ECG\"],\n",
    "      \"Date\": \"2024-10-10\",\n",
    "      \"Relevance Score\": \"0.88\",\n",
    "      \"Context Summary\": \"Patient presented with chest pain; ruled out myocardial infarction.\",\n",
    "      \"Clinical Tags\": [\"Chest Pain\", \"Angina\", \"ECG\"]\n",
    "  }\n",
    "  ```\n",
    "\n",
    "- **ICD Code Example**:\n",
    "  ```json\n",
    "  {\n",
    "      \"Document Type\": \"ICD Code\",\n",
    "      \"Specialty\": \"General Medicine\",\n",
    "      \"Source\": \"ICD-10\",\n",
    "      \"Title\": \"I20 - Angina Pectoris\",\n",
    "      \"Keywords\": [\"angina\", \"ischemic heart disease\"],\n",
    "      \"Date\": \"2024-01-01\",\n",
    "      \"Relevance Score\": \"0.95\",\n",
    "      \"Context Summary\": \"ICD-10 code for angina, includes classifications like unstable angina.\",\n",
    "      \"Clinical Tags\": [\"Ischemic Heart Disease\", \"Angina\"]\n",
    "  }\n",
    "  ```\n",
    "\n",
    "- **VA Guideline Example**:\n",
    "  ```json\n",
    "  {\n",
    "      \"Document Type\": \"Guideline\",\n",
    "      \"Specialty\": \"Mental Health\",\n",
    "      \"Source\": \"VA/DoD\",\n",
    "      \"Title\": \"PTSD Treatment Guidelines\",\n",
    "      \"Keywords\": [\"PTSD\", \"mental health\", \"CBT\"],\n",
    "      \"Date\": \"2023-06-15\",\n",
    "      \"Relevance Score\": \"0.90\",\n",
    "      \"Context Summary\": \"Best practices for diagnosing and managing PTSD in veterans.\",\n",
    "      \"Clinical Tags\": [\"PTSD\", \"Mental Health\", \"CBT\"]\n",
    "  }\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Implementation Plan**\n",
    "\n",
    "#### **Step 1: Define Schema**\n",
    "- Finalize metadata fields and default values for missing information.\n",
    "\n",
    "#### **Step 2: Map Schema to Data Sources**\n",
    "- TIU Notes:\n",
    "  - Extract fields like **Date**, **Keywords**, and **Clinical Tags** from structured sections.\n",
    "  - Use NLP to summarize and tag.\n",
    "- ICD Codes:\n",
    "  - Predefined schema with codes, descriptions, and associated conditions.\n",
    "- Guidelines:\n",
    "  - Summarize key sections and tag with **Keywords** and **Clinical Tags**.\n",
    "\n",
    "#### **Step 3: Store Metadata**\n",
    "- **Azure Option**:\n",
    "  - Use **Azure Cosmos DB** or **Azure Blob Storage** with metadata as JSON objects.\n",
    "- **Open-Source Option**:\n",
    "  - Store metadata in MongoDB or a relational database (e.g., PostgreSQL).\n",
    "\n",
    "#### **Step 4: Integrate Metadata with Vector Store**\n",
    "- Ensure metadata is associated with embeddings in the vector store.\n",
    "- Enable filtering and sorting based on metadata fields during retrieval.\n",
    "\n",
    "---\n",
    "\n",
    "### **Next Steps**\n",
    "\n",
    "1. Expand this with **embedding strategies** for each data type.\n",
    "2. Move on to **vector store indexing and retrieval design**.\n",
    "3. Discuss **scalability and updating the vector store**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe8b104-4290-40c1-9db1-67cad48419f5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **Standardizing on a Single Embedding Model vs. Using Multiple Models**\n",
    "\n",
    "Choosing whether to use **one embedding model** across all data or **multiple specialized models** depends on your **use case**, **data diversity**, and **performance requirements**. Below is an analysis of each approach’s **pros and cons**—including insights for both **Azure Government** (where only `text-embedding-ada-002` may be available) and **open-source** usage (which can leverage `text-embedding-3-small`, ClinicalBERT, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "### **Option 1: Standardizing on a Single Embedding Model**\n",
    "\n",
    "#### **Description**  \n",
    "Adopt a single model for all data types. For instance:\n",
    "\n",
    "- **Azure Government**: `text-embedding-ada-002` (currently available).  \n",
    "- **Open-Source**: `text-embedding-3-small` (newer and improved) or a single domain-specific model (e.g., ClinicalBERT).\n",
    "\n",
    "#### **Pros**\n",
    "\n",
    "1. **Simplicity**  \n",
    "   - Easier to manage one embedding pipeline (no juggling model versions).  \n",
    "   - Consistent embedding dimensionality simplifies indexing and similarity search.\n",
    "\n",
    "2. **Consistency**  \n",
    "   - Similarity scores are more meaningful because all vectors come from the same model.  \n",
    "   - Reduces confusion over which model to use for each data type.\n",
    "\n",
    "3. **Efficiency & Scalability**  \n",
    "   - Minimizes overhead by avoiding multiple embeddings for the same data.  \n",
    "   - In Azure, you can scale a single model endpoint more easily.\n",
    "\n",
    "4. **Cost-Effectiveness**  \n",
    "   - Maintaining or calling one model (API or local) is often cheaper than multiple specialized setups.\n",
    "\n",
    "#### **Cons**\n",
    "\n",
    "1. **Broad vs. Deep**  \n",
    "   - A single, general-purpose model may not capture nuances in specialized medical or clinical data.  \n",
    "   - Could underperform on highly domain-specific text (e.g., radiology reports, PubMed abstracts).\n",
    "\n",
    "2. **Limited Fine-Tuning**  \n",
    "   - Fine-tuning one model for many data types can be challenging.  \n",
    "   - Overfitting risk: the model may become biased toward the dominant data type.\n",
    "\n",
    "3. **Potential Accuracy Trade-Off**  \n",
    "   - Might not achieve state-of-the-art accuracy on niche data (e.g., ICD codes with specialized taxonomies).\n",
    "\n",
    "---\n",
    "\n",
    "### **Option 2: Using Multiple Specialized Models**\n",
    "\n",
    "#### **Description**  \n",
    "Leverage multiple embeddings, each tailored to a different domain or data type. Examples include:\n",
    "\n",
    "- **ClinicalBERT** for TIU notes.  \n",
    "- **SciBERT** for scientific literature (PubMed abstracts).  \n",
    "- **Base** GPT-like embedding model for consumer-friendly sources (e.g., MedlinePlus).\n",
    "\n",
    "#### **Pros**\n",
    "\n",
    "1. **Domain-Specific Precision**  \n",
    "   - Each model is optimized for its particular data type (clinical text, biomedical research, etc.).  \n",
    "   - Potentially higher recall/precision for niche queries.\n",
    "\n",
    "2. **Task Optimization**  \n",
    "   - Different embeddings can better capture unique vocabulary and context (e.g., ICD codes vs. unstructured notes).  \n",
    "   - Flexibility to swap models if a better specialized one becomes available.\n",
    "\n",
    "3. **Potential for State-of-the-Art Performance**  \n",
    "   - By choosing the best-of-breed model for each domain, you can stay at the cutting edge of retrieval quality.\n",
    "\n",
    "#### **Cons**\n",
    "\n",
    "1. **Operational Complexity**  \n",
    "   - Maintaining multiple models means separate pipelines, dimensionalities, and indexing logic.  \n",
    "   - More difficult to ensure a unified similarity scoring approach.\n",
    "\n",
    "2. **Inconsistent Feature Spaces**  \n",
    "   - Vectors from different models can’t be directly compared; each model’s embedding space differs.  \n",
    "   - Typically requires separate vector stores or at least partitioned indexes.\n",
    "\n",
    "3. **Higher Costs**  \n",
    "   - Running, fine-tuning, or calling multiple models can significantly increase compute and storage expenses.\n",
    "\n",
    "4. **Integration Challenges**  \n",
    "   - Determining which model to use for a given query adds logic overhead.  \n",
    "   - Combining results from different models can be non-trivial.\n",
    "\n",
    "---\n",
    "\n",
    "### **Comparison Table**\n",
    "\n",
    "| **Aspect**               | **Single Model**                              | **Multiple Models**                             |\n",
    "|--------------------------|-----------------------------------------------|-------------------------------------------------|\n",
    "| **Ease of Maintenance**  | **High** (one model)                          | **Low** (multiple pipelines)                    |\n",
    "| **Consistency**          | **High** (one feature space)                  | **Medium** (varies by model)                    |\n",
    "| **Embedding Quality**    | **Generalized**                               | **Specialized**                                 |\n",
    "| **Scalability**          | **Easier** (single pipeline to scale)         | **More Complex** (varied resource requirements) |\n",
    "| **Cost**                 | **Lower**                                     | **Potentially Higher**                          |\n",
    "| **Performance**          | **Broad**                                     | **Optimized per domain**                        |\n",
    "| **Flexibility**          | **Lower**                                     | **High**                                        |\n",
    "\n",
    "---\n",
    "\n",
    "### **Recommendation for the Medical AI Assistant**\n",
    "\n",
    "Given your situation—**Azure Government** plus a **diverse medical dataset**—here’s a practical strategy:\n",
    "\n",
    "1. **Start with a Single Model**  \n",
    "   - **Azure Government**: `text-embedding-ada-002` (since `text-embedding-3-small` isn’t yet available).  \n",
    "   - **Open-Source** (students, prototypes): Use `text-embedding-3-small` to stay current.  \n",
    "   - **Benefits**: Simplifies your initial pipeline; ensures consistent embeddings across data types.\n",
    "\n",
    "2. **Evaluate Performance**  \n",
    "   - Test how well the single model handles critical datasets (e.g., TIU notes vs. ICD codes).  \n",
    "   - Track query accuracy and user feedback to identify weaknesses.\n",
    "\n",
    "3. **Add Specialized Models (If Needed)**  \n",
    "   - For highly specialized data (e.g., detailed imaging reports, scientific abstracts), consider adding ClinicalBERT, SciBERT, etc.  \n",
    "   - Keep these to key areas where the single model clearly underperforms.\n",
    "\n",
    "4. **Scale & Optimize**  \n",
    "   - Use **Azure AI Search** or FAISS with clear indexing strategies (one big index vs. multiple).  \n",
    "   - Incrementally introduce more advanced models only if gains in accuracy outweigh the increased complexity.\n",
    "\n",
    "---\n",
    "\n",
    "### **Next Steps**\n",
    "\n",
    "1. **Implement a Single Embedding Model Pipeline**  \n",
    "   - Validate feasibility with your core Tier 1 data sources (TIU notes, ICD codes, VA/DoD guidelines).\n",
    "\n",
    "2. **Conduct a Pilot Evaluation**  \n",
    "   - Measure retrieval and response quality.  \n",
    "   - Identify data domains where a single model may not suffice.\n",
    "\n",
    "3. **Prototype a Multi-Model Approach**  \n",
    "   - If performance gaps appear, test specialized embeddings on those high-priority tasks.  \n",
    "   - Compare search accuracy and complexity with your single-model pipeline.\n",
    "\n",
    "By following this **phased** method—starting with a **single model** and iterating only if needed—you can keep the pipeline **simple**, **cost-effective**, and **stable**, while leaving the door open for specialized solutions as required by the evolving needs of the VA’s Medical AI Assistant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a61861a-701c-449d-9ba9-c52554bda249",
   "metadata": {},
   "source": [
    "## **Designing a Single-Model Embedding Pipeline**\n",
    "\n",
    "The single-model embedding pipeline focuses on using a **standardized embedding model** to process all types of data in a Retrieval-Augmented Generation (RAG) pipeline. This simplifies operations while ensuring compatibility and scalability.\n",
    "\n",
    "---\n",
    "\n",
    "### **Pipeline Components**\n",
    "\n",
    "#### **1. Data Ingestion**\n",
    "- **Purpose**: Collect and preprocess data from various sources (e.g., TIU notes, ICD codes, MedlinePlus).\n",
    "- **Key Steps**:\n",
    "  - Connect to data sources (e.g., EHR systems, APIs, local files).\n",
    "  - Normalize the data (e.g., remove duplicates, format for readability).\n",
    "  - Ensure sensitive data is de-identified (e.g., for TIU notes).\n",
    "\n",
    "#### **2. Preprocessing**\n",
    "- **Purpose**: Prepare data for embedding by cleaning, tokenizing, and splitting text.\n",
    "- **Key Steps**:\n",
    "  - **Text Cleaning**: Remove unnecessary punctuation, standardize casing, and handle special characters.\n",
    "  - **Chunking**:\n",
    "    - Divide long documents (e.g., TIU notes) into smaller, manageable chunks.\n",
    "    - Overlap chunks slightly to retain context across boundaries.\n",
    "  - **Metadata Enrichment**:\n",
    "    - Add fields like `source`, `specialty`, `tags`, and `date` to enrich retrieval capabilities.\n",
    "\n",
    "#### **3. Embedding Generation**\n",
    "- **Purpose**: Convert text into dense numerical vectors using a single embedding model.\n",
    "- **Model Choice**:\n",
    "  - **Azure Option**: `text-embedding-ada-002` (recommended for uniformity and scalability).\n",
    "  - **Implementation**:\n",
    "    ```python\n",
    "    import openai\n",
    "\n",
    "    def generate_embedding(text):\n",
    "        response = openai.Embedding.create(\n",
    "            input=text,\n",
    "            engine=\"text-embedding-ada-002\"\n",
    "        )\n",
    "        return response[\"data\"][0][\"embedding\"]\n",
    "    ```\n",
    "- **Batch Processing**:\n",
    "  - Embed documents in batches to improve throughput.\n",
    "  - Store embeddings alongside metadata in a scalable storage solution.\n",
    "\n",
    "#### **4. Vector Store**\n",
    "- **Purpose**: Store embeddings and metadata for efficient similarity search.\n",
    "- **Options**:\n",
    "  - **Azure AI Search**: Fully managed search service.\n",
    "  - **Open-Source**: FAISS, Milvus, or Pinecone for local or hybrid setups.\n",
    "\n",
    "#### **5. Retrieval**\n",
    "- **Purpose**: Fetch the most relevant documents based on user queries.\n",
    "- **Process**:\n",
    "  - Convert the user query into an embedding.\n",
    "  - Perform similarity search in the vector store to retrieve the top results.\n",
    "  - Return results with associated metadata for contextual understanding.\n",
    "\n",
    "#### **6. Augmentation**\n",
    "- **Purpose**: Combine retrieved data with the user query for enhanced model responses.\n",
    "- **Process**:\n",
    "  - Concatenate retrieved documents with the query.\n",
    "  - Pass the combined text to the LLM for a final answer.\n",
    "\n",
    "---\n",
    "\n",
    "### **Pipeline Flow**\n",
    "\n",
    "1. **Input**:\n",
    "   - Data sources: TIU notes, ICD codes, guidelines.\n",
    "   - Query: \"What are the causes of chest pain?\"\n",
    "\n",
    "2. **Output**:\n",
    "   - Top relevant documents (e.g., \"TIU note mentioning angina and GERD\").\n",
    "   - LLM-enhanced response: \"Common causes of chest pain include angina, GERD, and anxiety. Here are recommended diagnostic steps...\"\n",
    "\n",
    "---\n",
    "\n",
    "### **Infrastructure and Containers**\n",
    "\n",
    "**Containers** like **Docker** or **Kubernetes** can help build a robust infrastructure for this pipeline. Here's how:\n",
    "\n",
    "#### **Advantages of Containers**:\n",
    "1. **Consistency**:\n",
    "   - Containers ensure that the pipeline runs consistently across different environments (e.g., development, staging, production).\n",
    "\n",
    "2. **Scalability**:\n",
    "   - Container orchestration (e.g., Kubernetes) allows scaling individual pipeline components based on demand (e.g., increase embedding throughput during peak ingestion periods).\n",
    "\n",
    "3. **Isolation**:\n",
    "   - Each component (e.g., preprocessing, embedding, vector search) can run in isolated containers to prevent dependency conflicts.\n",
    "\n",
    "4. **Portability**:\n",
    "   - Containers make it easy to deploy the pipeline across cloud providers (e.g., Azure, AWS) or hybrid setups.\n",
    "\n",
    "5. **Integration with CI/CD**:\n",
    "   - Automated deployment pipelines can be established to update and maintain the pipeline seamlessly.\n",
    "\n",
    "---\n",
    "\n",
    "### **Pipeline Design with Containers**\n",
    "\n",
    "#### **Key Components and Their Containers**:\n",
    "| **Component**           | **Description**                                       | **Container Functionality**                          |\n",
    "|--------------------------|-------------------------------------------------------|-----------------------------------------------------|\n",
    "| **Preprocessing**        | Cleans, chunks, and enriches metadata.                | Runs NLP preprocessing libraries (e.g., spaCy).     |\n",
    "| **Embedding Generation** | Converts text to embeddings using a single model.     | Hosts the embedding model (e.g., via Azure OpenAI). |\n",
    "| **Vector Store**         | Stores embeddings and metadata for retrieval.         | Hosts FAISS, Milvus, or integrates with Azure AI Search.      |\n",
    "| **Query Handler**        | Handles user queries and performs similarity search.  | Performs search and formats results.               |\n",
    "| **LLM Interface**        | Combines retrieved documents with user queries.       | Sends augmented input to the LLM API.              |\n",
    "\n",
    "#### **Example Workflow**:\n",
    "1. **Preprocessing Container**:\n",
    "   - Reads TIU notes from EHR, cleans text, and extracts metadata.\n",
    "2. **Embedding Container**:\n",
    "   - Generates embeddings for processed text.\n",
    "3. **Vector Store Container**:\n",
    "   - Stores embeddings and metadata for later retrieval.\n",
    "4. **Query Handler Container**:\n",
    "   - Accepts user queries, retrieves relevant documents, and passes results to the LLM.\n",
    "5. **LLM Container**:\n",
    "   - Uses Azure OpenAI API to generate the final response.\n",
    "\n",
    "---\n",
    "\n",
    "### **Next Steps**\n",
    "1. **Define Preprocessing Workflow**:\n",
    "   - Standardize text cleaning and chunking logic.\n",
    "2. **Detail Embedding Storage**:\n",
    "   - Decide on storage backends (e.g., Azure AI Search vs. FAISS).\n",
    "3. **Plan Containerization**:\n",
    "   - Identify container dependencies and orchestrator requirements.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9132dcf4-8dfb-4604-9198-37cee7555450",
   "metadata": {},
   "source": [
    "Below is the **rewritten version** of your **“Simplified RAG Pipeline Using OpenAI Embeddings and FAISS”** section. It highlights both **Azure** and **Open-Source** embedding model paths, and shows how to incorporate **GPT-4o or GPT-4o-mini** for final generation.\n",
    "\n",
    "---\n",
    "\n",
    "## **Simplified RAG Pipeline Using OpenAI Embeddings and FAISS**\n",
    "\n",
    "For **small-scale** implementations and prototyping, you can leverage:\n",
    "\n",
    "- **OpenAI Embedding Model**  \n",
    "  - **Azure Path**: `text-embedding-ada-002` (currently supported on Azure Government).  \n",
    "  - **Open-Source Path**: `text-embedding-3-small` (a newer, improved model accessible via the OpenAI API).\n",
    "\n",
    "- **FAISS**  \n",
    "  - An open-source vector database for fast similarity search and efficient retrieval.\n",
    "\n",
    "This setup is **lightweight**, **cost-effective**, and ideal for quick deployments or **classroom demos**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Pipeline Overview**\n",
    "\n",
    "1. **Data Ingestion**  \n",
    "   - **Input**: TIU notes, ICD codes, MedlinePlus articles, and other relevant medical content.  \n",
    "   - **Output**: Cleaned, chunked text ready for embedding.\n",
    "\n",
    "2. **Preprocessing**  \n",
    "   - Clean, tokenize, and split text into segments (e.g., 512 tokens).  \n",
    "   - Add metadata fields (e.g., `source`, `category`, `tags`) for filtering or reference.\n",
    "\n",
    "3. **Embedding Generation**  \n",
    "   - Use **OpenAI’s Embedding API** to transform each chunk into a dense vector.  \n",
    "   - **Azure**: `text-embedding-ada-002`  \n",
    "   - **Open-Source**: `text-embedding-3-small`\n",
    "\n",
    "4. **Vector Store (FAISS)**  \n",
    "   - Store the resulting embeddings + metadata in a **FAISS** index for similarity-based lookups.\n",
    "\n",
    "5. **Retrieval**  \n",
    "   - Convert user queries to embeddings.  \n",
    "   - Query FAISS to fetch the **top-k** relevant chunks.\n",
    "\n",
    "6. **Augmentation**  \n",
    "   - Merge the retrieved data with the user’s question.  \n",
    "   - Send this augmented text to GPT-4o or GPT-4o-mini for a final response.\n",
    "\n",
    "---\n",
    "\n",
    "### **Advantages of OpenAI + FAISS Setup**\n",
    "\n",
    "1. **Ease of Use**  \n",
    "   - **OpenAI’s Embedding API** is straightforward and versatile.  \n",
    "   - **FAISS** delivers rapid, scalable vector retrieval.\n",
    "\n",
    "2. **Cost Efficiency**  \n",
    "   - No specialized hardware or managed service required for moderate data volumes.\n",
    "\n",
    "3. **Customizability**  \n",
    "   - **On-premises** deployment with FAISS offers full control.  \n",
    "   - Advanced index configurations (e.g., IVF, PQ) for larger datasets.\n",
    "\n",
    "4. **Scalability**  \n",
    "   - FAISS can handle millions of vectors.  \n",
    "   - You can seamlessly transition to other vector DBs (Pinecone, Weaviate, Milvus) if needed.\n",
    "\n",
    "---\n",
    "\n",
    "### **Pipeline Design Steps**\n",
    "\n",
    "#### **Step 1: Preprocessing**\n",
    "- **Objective**: Standardize and prepare text for embedding.  \n",
    "- **Workflow**:\n",
    "  1. Clean and normalize text (remove special characters, unify casing).  \n",
    "  2. Split lengthy content into chunks (e.g., 512 tokens each).  \n",
    "  3. Attach relevant metadata (source, category, creation date).\n",
    "\n",
    "```python\n",
    "def preprocess_text(text, max_length=512, overlap=50):\n",
    "    \"\"\"\n",
    "    Clean and chunk text into manageable segments.\n",
    "    \"\"\"\n",
    "    text = text.lower().replace(\"\\n\", \" \").strip()\n",
    "    words = text.split()\n",
    "\n",
    "    for i in range(0, len(words), max_length - overlap):\n",
    "        yield \" \".join(words[i:i + max_length])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 2: Embedding and Indexing**\n",
    "- **Objective**: Generate embeddings and store them in FAISS.\n",
    "- **Workflow**:  \n",
    "  1. **Embed** each chunk using OpenAI’s API.  \n",
    "  2. **Store** embeddings + chunk metadata in a FAISS index.\n",
    "\n",
    "```python\n",
    "import openai\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Dimensionality depends on your embedding model\n",
    "dimension = 1536  # Works for text-embedding-ada-002 or text-embedding-3-small\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "def generate_and_store_embeddings(text_chunks, model=\"text-embedding-3-small\"):\n",
    "    embeddings = []\n",
    "    metadata = []\n",
    "\n",
    "    for chunk in text_chunks:\n",
    "        response = openai.Embedding.create(input=chunk, model=model)\n",
    "        embedding = response['data'][0]['embedding']\n",
    "        embeddings.append(embedding)\n",
    "        metadata.append(chunk)\n",
    "\n",
    "    embeddings_np = np.array(embeddings, dtype=\"float32\")\n",
    "    index.add(embeddings_np)\n",
    "    return metadata\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 3: Query and Retrieval**\n",
    "- **Objective**: Fetch the most relevant chunks for a user query.\n",
    "- **Workflow**:  \n",
    "  1. **Embed** the query with the same model.  \n",
    "  2. **Search** FAISS for top matches.  \n",
    "  3. Return the matched chunks and optional distance scores.\n",
    "\n",
    "```python\n",
    "def retrieve_similar_chunks(query, top_k=5, model=\"text-embedding-3-small\"):\n",
    "    query_embedding = openai.Embedding.create(input=query, model=model)['data'][0]['embedding']\n",
    "    query_np = np.array([query_embedding], dtype=\"float32\")\n",
    "    distances, indices = index.search(query_np, top_k)\n",
    "\n",
    "    results = [(metadata[i], distances[0][idx]) for idx, i in enumerate(indices[0])]\n",
    "    return results\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 4: Augmentation and Response Generation**\n",
    "- **Objective**: Use retrieved chunks to enrich the user’s query prior to LLM response.\n",
    "- **Workflow**:  \n",
    "  - Merge the retrieved text with the user’s query.  \n",
    "  - Invoke GPT-4o or GPT-4o-mini to produce a final, context-aware answer.\n",
    "\n",
    "```python\n",
    "def generate_response(query, top_k=5, model=\"text-embedding-3-small\"):\n",
    "    \"\"\"\n",
    "    Generate a response by augmenting the user query with retrieved chunks,\n",
    "    then calling GPT-4o (or GPT-4o-mini) for the final answer.\n",
    "    \"\"\"\n",
    "    similar_chunks = retrieve_similar_chunks(query, top_k=top_k, model=model)\n",
    "    augmented_query = query + \"\\n\\n\" + \"\\n\".join([chunk[0] for chunk in similar_chunks])\n",
    "\n",
    "    from langchain_core.messages import HumanMessage\n",
    "    user_message = HumanMessage(content=augmented_query)\n",
    "\n",
    "    # Invoke GPT-4o or GPT-4o-mini (e.g., gpt4o_chat from your code)\n",
    "    response = gpt4o_chat.invoke([user_message])  # or gpt4o_mini_chat.invoke([user_message])\n",
    "    return response.content\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Next Steps**\n",
    "\n",
    "1. **Prepare Sample Data**  \n",
    "   - Gather a small set of TIU notes, ICD codes, and MedlinePlus articles to test the end-to-end process.\n",
    "\n",
    "2. **Run the Pipeline**  \n",
    "   - **Ingest** your data → **Preprocess** → **Embed & Index** → **Retrieve** → **Augment & Generate**.\n",
    "\n",
    "3. **Containerize for Production**  \n",
    "   - Wrap each step (preprocessing, embedding, retrieval) in Docker containers or orchestrate with Kubernetes.\n",
    "\n",
    "4. **Evaluate & Iterate**  \n",
    "   - Monitor retrieval accuracy and answer clarity using real or synthetic queries.  \n",
    "   - If necessary, try advanced FAISS indexing (IVF, PQ) or switch to a hosted vector DB for scaling.\n",
    "\n",
    "By **combining** OpenAI embeddings (Azure or open-source) with **FAISS**, you can rapidly prototype and refine a **Medical AI Assistant** that provides **context-rich** responses, all while retaining full control over data storage and retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f4afa5-d656-4e6f-9ad0-0baba8115729",
   "metadata": {},
   "source": [
    "## Creating and Displaying Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64007f44-d18a-49a2-b1b8-103fc8d32bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from GPT-4o: Hello! How can I assist you today?\n",
      "Response from GPT-4o-mini: Hello, Joseph! How can I assist you today?\n",
      "Embedding length: 1536\n",
      "Embedding snippet (first 10): [-0.00205534347333014, -0.029356569051742554, 0.023372739553451538, 0.02247772179543972, -0.05318960174918175, -0.007319963537156582, -0.0008051160839386284, 0.026518085971474648, -0.01116853766143322, 0.0046125357039272785]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ======================================\n",
    "# 1. Load Environment Variables\n",
    "# ======================================\n",
    "load_dotenv()\n",
    "\n",
    "def get_env_var(var: str):\n",
    "    \"\"\"\n",
    "    Utility to fetch an environment variable or raise an error if missing.\n",
    "    \"\"\"\n",
    "    value = os.getenv(var)\n",
    "    if value is None:\n",
    "        raise ValueError(f\"{var} not found in environment variables. Make sure it is set in your .env file.\")\n",
    "    return value\n",
    "\n",
    "# Retrieve keys from the environment\n",
    "langchain_api_key = get_env_var(\"LANGCHAIN_API_KEY\")  # LangChain usage (if applicable)\n",
    "langchain_tracing_v2 = get_env_var(\"LANGCHAIN_TRACING_V2\")  # Optional for LangChain\n",
    "openai_api_key = get_env_var(\"OPENAI_API_COURSE_KEY\")  # OpenAI API key\n",
    "tavily_api_key = get_env_var(\"TAVILY_API_KEY\")  # Another API key if needed\n",
    "\n",
    "# Set the OpenAI API key for direct usage\n",
    "openai.api_key = openai_api_key\n",
    "\n",
    "\n",
    "# ======================================\n",
    "# 2. Import and Configure LangChain ChatOpenAI\n",
    "# ======================================\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Initialize ChatOpenAI with GPT-4o and GPT-4o-mini\n",
    "gpt4o_chat = ChatOpenAI(model=\"gpt-4o\", temperature=0, openai_api_key=openai_api_key)\n",
    "gpt4o_mini_chat = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "# Create a message\n",
    "msg = HumanMessage(content=\"Hello world\", name=\"Joseph\")\n",
    "messages = [msg]\n",
    "\n",
    "# Invoke GPT-4o\n",
    "response_gpt4o = gpt4o_chat.invoke(messages)\n",
    "print(\"Response from GPT-4o:\", response_gpt4o.content)\n",
    "\n",
    "# Invoke GPT-4o-mini\n",
    "response_gpt4o_mini = gpt4o_mini_chat.invoke(messages)\n",
    "print(\"Response from GPT-4o-mini:\", response_gpt4o_mini.content)\n",
    "\n",
    "\n",
    "# ======================================\n",
    "# 3. Test Embedding Function\n",
    "# ======================================\n",
    "def test_openai_embedding_api():\n",
    "    \"\"\"\n",
    "    Tests the OpenAI Embeddings API with a medical AI assistant text.\n",
    "    Uses openai.embeddings.create() -- requires openai<1.0.0\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Example text to embed\n",
    "        medical_text = (\n",
    "            \"I am a Medical AI Assistant trained to provide information on various medical topics. \"\n",
    "            \"Feel free to ask about diagnostics, treatments, or preventive measures.\"\n",
    "        )\n",
    "\n",
    "        # Request to generate embeddings (openai<1.0.0)\n",
    "        # Change 'model' to whatever embedding model you'd like to use (e.g., \"text-embedding-3-small\" or \"text-embedding-ada-002\")\n",
    "        response = openai.embeddings.create(\n",
    "            input=[medical_text],\n",
    "            model=\"text-embedding-3-small\"\n",
    "        )\n",
    "\n",
    "        # Extract the embedding from the first item\n",
    "        embedding = response.data[0].embedding\n",
    "\n",
    "        # Print some basic info\n",
    "        print(f\"Embedding length: {len(embedding)}\")\n",
    "        print(f\"Embedding snippet (first 10): {embedding[:10]}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Call the test function\n",
    "test_openai_embedding_api()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31171b7-0395-4776-ac56-9e552f09c7f2",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "\n",
    "OpenAI has developed several text embedding models, each tailored to different performance and efficiency needs:\n",
    "\n",
    "\n",
    "**1. text-embedding-ada-002**\n",
    "\n",
    "- **Dimensions**: 1536\n",
    "- **Performance**: Served as a robust model prior to the introduction of the text-embedding-3 series, with average scores of 31.4% on [MIRACL benchmark](https://project-miracl.github.io/) and 61.0% on [MTEB benchmark](https://github.com/embeddings-benchmark/mteb).\n",
    "- **Cost**: Priced at $0.0001 per 1,000 tokens.\n",
    "- **Use Cases**: Previously used for general-purpose applications; however, the text-embedding-3 models now offer improved performance and cost efficiency.\n",
    "\n",
    "**2. text-embedding-3-small**\n",
    "\n",
    "- **Dimensions**: 1536\n",
    "- **Performance**: Improved over its predecessor, text-embedding-ada-002, with an average score increase from 31.4% to 44.0% on the [MIRACL benchmark](https://project-miracl.github.io/) and from 61.0% to 62.3% on the [MTEB benchmark](https://github.com/embeddings-benchmark/mteb).\n",
    "- **Cost**: Priced at $0.00002 per 1,000 tokens, making it a cost-effective choice.\n",
    "- **Use Cases**: Suitable for applications requiring efficient embeddings with moderate performance needs.\n",
    "\n",
    "**3. text-embedding-3-large**\n",
    "\n",
    "- **Dimensions**: 3072\n",
    "- **Performance**: Offers superior performance, with scores increasing to 54.9% on [MIRACL benchmark](https://project-miracl.github.io/)  and 64.6% on [MTEB benchmark](https://github.com/embeddings-benchmark/mteb).\n",
    "- **Cost**: Priced at $0.00013 per 1,000 tokens, reflecting its advanced capabilities.\n",
    "- **Use Cases**: Ideal for tasks demanding high accuracy and the ability to capture complex semantic relationships.\n",
    "\n",
    "\n",
    "**Key Differences**:\n",
    "\n",
    "- **Dimensionality**: text-embedding-3-large has a higher dimensionality (3072) compared to text-embedding-3-small and text-embedding-ada-002 (both 1536), allowing it to capture more nuanced information.\n",
    "- **Performance**: The 3-large model outperforms the other models on both multilingual and English-specific benchmarks, making it suitable for more complex tasks.\n",
    "- **Cost Efficiency**: text-embedding-3-small offers the most cost-effective solution, especially for applications where budget constraints are a consideration.\n",
    "\n",
    "When selecting a model, consider the specific requirements of your application, including the need for accuracy, computational resources, and budget constraints. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130a075c-2147-412f-a69c-383431fc3191",
   "metadata": {},
   "source": [
    "# Next Steps in the RAG Pipeline\n",
    "\n",
    "Below is a **high-level outline** of what remains to build out your **Medical AI Assistant** using a **RAG (Retrieval-Augmented Generation) workflow**. You have already:\n",
    "\n",
    "1. **Identified Data Repositories** (e.g., TIU notes, ICD codes, MedlinePlus, etc.).  \n",
    "2. **Configured OpenAI** (for prompts and embeddings).  \n",
    "3. **Tested Prompting** with GPT-4o and GPT-4o-mini.  \n",
    "4. **Tested Embedding Generation** using the OpenAI Embedding API.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Data Ingestion & Preprocessing\n",
    "\n",
    "**Goal**: Convert raw medical content into a clean, chunked format.\n",
    "\n",
    "- **Ingest**: Pull or scrape data (articles, PDFs, structured data).  \n",
    "- **Chunk**: Break large docs into smaller pieces (1–2 paragraphs), which improves retrieval specificity.  \n",
    "- **Clean & Normalize**: Remove HTML tags, unify text formatting (UTF-8, etc.).\n",
    "\n",
    "> **Tip**: Anonymize any PHI in TIU notes or similar clinical data.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Create a Vector Store\n",
    "\n",
    "**Goal**: Store embeddings + metadata for fast, relevant retrieval.\n",
    "\n",
    "- **Vector DB**: Choose a local solution (FAISS, Chroma) or a managed service (Pinecone, Weaviate, Milvus).  \n",
    "- **Embeddings**: Use OpenAI’s embedding model (`text-embedding-3-small` or `text-embedding-ada-002`).  \n",
    "- **Metadata**: Store doc attributes (title, source, date, etc.) to support filtering or ranking.\n",
    "\n",
    "> **Indexing**: Some databases auto-index embeddings on ingestion; for FAISS, explicitly call `.add()` or similar.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Retrieval Strategy\n",
    "\n",
    "**Goal**: Transform user queries into embeddings, then find the top-k chunks.\n",
    "\n",
    "1. **Query Embedding**: Convert user query text into a vector.  \n",
    "2. **Similarity Search**: Return the closest vectors from your store.  \n",
    "3. **Context Assembly**: Merge the top results into one string or JSON for LLM usage.\n",
    "\n",
    "> **Tip**: Track source info (e.g., “Source: MedlinePlus 2023”) for potential citations in the answer.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Prompt Construction (RAG Step)\n",
    "\n",
    "**Goal**: Supply the user’s question and the retrieved context to GPT-4o or GPT-4o-mini\n",
    "\n",
    "- **Context Injection**: “Below are the most relevant documents from our medical database…”  \n",
    "- **LLM Call**: Provide instructions like “If the context is insufficient, say you don’t know.”  \n",
    "- **Response**: The model outputs a final consolidated answer.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Post-Processing & Output\n",
    "\n",
    "**Goal**: Optionally refine or format the LLM output.\n",
    "\n",
    "- **Summaries**: For longer documents or multi-turn dialogues, you might want to do a final summarization.  \n",
    "- **References**: Add citations (chunk metadata) if required for audit or compliance.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Iteration & Maintenance\n",
    "\n",
    "**Goal**: Continuously improve relevance and user experience.\n",
    "\n",
    "- **User Feedback**: Gather data on how often the user modifies or complains about answers.  \n",
    "- **Update Embeddings**: Re-embed if your corpus changes significantly or if a better embedding model becomes available.  \n",
    "- **Prompt Tweaks**: Adjust instructions or system messages based on user queries.\n",
    "\n",
    "---\n",
    "\n",
    "## Putting It All Together\n",
    "\n",
    "1. **Ingest & Preprocess** your VA or medical dataset.  \n",
    "2. **Embed & Index** those chunks in our chosen vector store.  \n",
    "3. **Embed Queries → Retrieve** top matches → **Augment Prompt**.  \n",
    "4. **Generate** final answers using GPT-4o or GPT-4o-mini with the retrieved context.  \n",
    "\n",
    "Following these steps, we’ll have a **complete RAG pipeline** for our **Medical AI Assistant**—capable of providing **context-rich**, **evidence-based** answers by combining LLM power with targeted retrieval from your medical corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979993db-7fc8-4fa2-99e0-6596490a2c7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
