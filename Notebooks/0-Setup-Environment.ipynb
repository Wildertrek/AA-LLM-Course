{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b669b815-cf4b-4061-a20e-13a64110ba13",
   "metadata": {},
   "source": [
    "# Set up your environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3deedde1-147e-41de-a6b1-33ba720e7774",
   "metadata": {},
   "source": [
    "Let's use a `.env` file and the `dotenv` library. This allows you to keep your API key secure and separated from your code.\n",
    "\n",
    "Hereâ€™s how you can setup to load the OpenAI API key using a `.env` file:\n",
    "\n",
    "### Step 1: Install Dependencies\n",
    "Make sure you have `python-dotenv` installed if you're using `.env` files:\n",
    "```bash\n",
    "pip install python-dotenv\n",
    "pip install -U langsmith\n",
    "pip install openai\n",
    "```\n",
    "\n",
    "### Step 2: Create a `.env` File\n",
    "In your project directory, create a `.env` file with the following content:\n",
    "```\n",
    "OPENAI_API_KEY=your-api-key-here\n",
    "```\n",
    "\n",
    "### Step 3: Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94a134a4-878c-4270-a8b1-ca214909e237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Function to load environment variables or raise an error if not found\n",
    "def get_env_var(var: str):\n",
    "    value = os.getenv(var)\n",
    "    if value is None:\n",
    "        raise ValueError(f\"{var} not found in environment variables. Make sure it is set in your .env file.\")\n",
    "    return value\n",
    "\n",
    "# Load API keys from the environment\n",
    "langchain_api_key = get_env_var(\"LANGCHAIN_API_KEY\")\n",
    "langchain_tracing_v2 = get_env_var(\"LANGCHAIN_TRACING_V2\")\n",
    "openai_api_key = get_env_var(\"OPENAI_API_KEY\")\n",
    "tavily_api_key = get_env_var(\"TAVILY_API_KEY\")\n",
    "\n",
    "# Now, you can use the keys in our setup as needed\n",
    "from langchain_openai import ChatOpenAI\n",
    "gpt4o_chat = ChatOpenAI(model=\"gpt-4o\", temperature=0, openai_api_key=openai_api_key)\n",
    "gpt35_chat = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0, openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8aaa1a2c-ea45-4552-9075-198123903ce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 11, 'total_tokens': 20, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_e375328146', 'finish_reason': 'stop', 'logprobs': None}, id='run-16722a96-9796-468a-8b1e-8e388edba9b5-0', usage_metadata={'input_tokens': 11, 'output_tokens': 9, 'total_tokens': 20})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Create a message\n",
    "msg = HumanMessage(content=\"Hello world\", name=\"Lance\")\n",
    "\n",
    "# Message list\n",
    "messages = [msg]\n",
    "\n",
    "# Invoke the model with a list of messages \n",
    "gpt4o_chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1acfc41-e0cc-404a-8c4e-c107b18884bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 9, 'total_tokens': 18, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_3537616b13', 'finish_reason': 'stop', 'logprobs': None}, id='run-a7c19b00-e391-46d7-a459-07b9ff02812d-0', usage_metadata={'input_tokens': 9, 'output_tokens': 9, 'total_tokens': 18})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt4o_chat.invoke(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93bcdd11-9a59-4e39-8daf-cdff98ec116c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 9, 'total_tokens': 18, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-90a6694a-76b6-4781-b6f7-36c0e786d2d1-0', usage_metadata={'input_tokens': 9, 'output_tokens': 9, 'total_tokens': 18})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt35_chat.invoke(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf97e30f-a1ab-41c1-80f2-c5fd729a495d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Tavily using the API key from the environment\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "tavily_search = TavilySearchResults(max_results=3, api_key=tavily_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a31011e-cffb-4c8d-8482-b0b70d94771f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://www.langchain.com/langgraph',\n",
       "  'content': 'LangGraph is a stateful, orchestration framework that brings added control to agent workflows. LangGraph Cloud is a service for deploying and scaling LangGraph applications, with a built-in Studio for prototyping, debugging, and sharing LangGraph applications.'},\n",
       " {'url': 'https://www.datacamp.com/tutorial/langgraph-tutorial',\n",
       "  'content': 'LangGraph is a library within the LangChain ecosystem designed to tackle these challenges head-on. LangGraph provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured manner. It simplifies the development process by enabling the creation of cyclical graphs, which are essential for developing ...'},\n",
       " {'url': 'https://towardsdatascience.com/from-basics-to-advanced-exploring-langgraph-e8c1cf4db787',\n",
       "  'content': \"LangGraph is a low-level framework that offers extensive customisation options, allowing you to build precisely what you need. Since LangGraph is built on top of LangChain, it's seamlessly integrated into its ecosystem, making it easy to leverage existing tools and components. However, there are areas where LangGrpah could be improved:\"}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform a search with Tavily\n",
    "search_docs = tavily_search.invoke(\"What is LangGraph?\")\n",
    "search_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9677a7af-8d33-4754-97ef-3bcc4c4a3f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain tracing started successfully.\n",
      "Agent's Response:\n",
      "content='Transformers have revolutionized natural language processing (NLP) by significantly improving the performance of various NLP tasks. Some of the key benefits of using transformers in NLP include:\\n\\n1. Enhanced performance: Transformers have shown superior performance compared to traditional NLP models, especially in tasks like language translation, text generation, and sentiment analysis.\\n\\n2. Attention mechanism: Transformers use an attention mechanism that allows the model to focus on different parts of the input sequence, enabling better understanding and capturing of long-range dependencies in the text.\\n\\n3. Scalability: Transformers are highly scalable and can be easily adapted to different languages and tasks without the need for extensive retraining.\\n\\n4. Transfer learning: Transformers support transfer learning, allowing pre-trained models to be fine-tuned on specific tasks with minimal data, leading to faster and more efficient model training.\\n\\n5. Interpretability: Transformers provide better interpretability compared to traditional NLP models, making it easier to understand how the model makes predictions and identify potential biases.\\n\\nOverall, transformers have become a cornerstone in modern NLP research and applications, offering significant advantages in terms of performance, scalability, and interpretability.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 227, 'prompt_tokens': 33, 'total_tokens': 260, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-57785612-8438-4a15-9ec7-3a78ac3f4cbd-0' usage_metadata={'input_tokens': 33, 'output_tokens': 227, 'total_tokens': 260}\n",
      "Follow-up Response:\n",
      "content='Certainly! GPT (Generative Pre-trained Transformer) and BERT (Bidirectional Encoder Representations from Transformers) are both popular models in the field of natural language processing (NLP). \\n\\nThe main difference between GPT and BERT lies in their architecture and training objectives. GPT is a generative model that predicts the next word in a sequence of text, while BERT is a discriminative model that is trained to understand the context of a word in a sentence by considering both the words before and after it.\\n\\nIn simpler terms, GPT is better suited for tasks that involve generating text, such as language modeling and text completion, while BERT is more suitable for tasks that require understanding the context of a word or sentence, such as question answering and sentiment analysis.\\n\\nOverall, both GPT and BERT have their own strengths and weaknesses, and the choice between them depends on the specific NLP task at hand.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 186, 'prompt_tokens': 33, 'total_tokens': 219, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-732828b7-1dd2-4336-a8ea-05bd37b67c60-0' usage_metadata={'input_tokens': 33, 'output_tokens': 186, 'total_tokens': 219}\n",
      "\n",
      "Total Tokens Used: 479\n",
      "Total Cost (USD): $0.0006525000000000001\n",
      "LangChain tracing ended successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Function to load environment variables or raise an error if not found\n",
    "def get_env_var(var: str):\n",
    "    value = os.getenv(var)\n",
    "    if value is None:\n",
    "        raise ValueError(f\"{var} not found in environment variables. Make sure it is set in your .env file.\")\n",
    "    return value\n",
    "\n",
    "# Load API keys from the environment\n",
    "openai_api_key = get_env_var(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Initialize the OpenAI LLM via LangChain\n",
    "gpt_chat = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "# Define a simple prompt template for conversation\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"user_input\"],\n",
    "    template=\"You are a helpful assistant. User asks: {user_input}. How would you respond?\"\n",
    ")\n",
    "\n",
    "# Instead of using RunnableSequence, directly chain the prompt and model using the | operator\n",
    "conversation_chain = prompt_template | gpt_chat\n",
    "\n",
    "try:\n",
    "    # Start callback for tracing OpenAI usage\n",
    "    with get_openai_callback() as cb:\n",
    "        print(\"LangChain tracing started successfully.\")\n",
    "\n",
    "        # Example conversation - LangChain will trace this operation\n",
    "        user_input = \"What are the benefits of using transformers in natural language processing?\"\n",
    "        response = conversation_chain.invoke({\"user_input\": user_input})\n",
    "\n",
    "        print(\"Agent's Response:\")\n",
    "        print(response)\n",
    "\n",
    "        # You can extend this conversation further\n",
    "        follow_up = \"Can you explain the difference between GPT and BERT?\"\n",
    "        follow_up_response = conversation_chain.invoke({\"user_input\": follow_up})\n",
    "\n",
    "        print(\"Follow-up Response:\")\n",
    "        print(follow_up_response)\n",
    "\n",
    "        # Output callback information\n",
    "        print(f\"\\nTotal Tokens Used: {cb.total_tokens}\")\n",
    "        print(f\"Total Cost (USD): ${cb.total_cost}\")\n",
    "\n",
    "    print(\"LangChain tracing ended successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error with LangChain tracing: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7eb07e-c9f5-4945-9163-405db7cdaa39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
