{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca957d72-3f50-4493-b504-28d53a041556",
   "metadata": {},
   "source": [
    "# Building a Streamlit application to act as our AI Assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5708dfd4-c8b4-4db1-bf62-545b8e9133b5",
   "metadata": {},
   "source": [
    "You need to run this **Streamlit app separately** from your Jupyter notebook. **Streamlit** runs as a web app and doesn't natively integrate within a Jupyter notebook like standard Python scripts. Here's how you can run it:\n",
    "\n",
    "### **Steps to Run the Streamlit App**\n",
    "1. **Save the Code**  \n",
    "   - Save the provided code in a Python script, e.g., `app.py`.\n",
    "  \n",
    "```python\n",
    "import streamlit as st\n",
    "import os\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "from openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from anthropic import Anthropic\n",
    "from langchain_core.messages import HumanMessage  \n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "def get_env_var(var: str):\n",
    "    value = os.getenv(var)\n",
    "    if value is None:\n",
    "        raise ValueError(f\"{var} not found in environment variables. Make sure it is set in your .env file.\")\n",
    "    return value\n",
    "\n",
    "# Load API keys\n",
    "openai_api_key = get_env_var(\"OPENAI_COURSE_KEY\")  \n",
    "gemini_api_key = get_env_var(\"GEMINI_API_KEY\")  \n",
    "anthropic_api_key = get_env_var(\"ANTHROPIC_API_KEY\")  \n",
    "xai_api_key = get_env_var(\"XAI_API_KEY\")\n",
    "\n",
    "# Initialize LLMs\n",
    "gpt4o_chat = ChatOpenAI(model=\"gpt-4o\", temperature=0, openai_api_key=openai_api_key)\n",
    "gpt4o_mini_chat = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, openai_api_key=openai_api_key)\n",
    "claude_chat = ChatAnthropic(model=\"claude-3-5-sonnet-20241022\", temperature=0, anthropic_api_key=anthropic_api_key)\n",
    "genai.configure(api_key=gemini_api_key)  \n",
    "gemini_model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "grok_client = OpenAI(api_key=xai_api_key, base_url=\"https://api.x.ai/v1\")\n",
    "\n",
    "def query_grok(prompt: str):\n",
    "    try:\n",
    "        completion = grok_client.chat.completions.create(\n",
    "            model=\"grok-2-latest\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are Grok, a chatbot inspired by the Hitchhiker's Guide to the Galaxy.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "        )\n",
    "        return completion.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error querying Grok: {e}\"\n",
    "\n",
    "# Streamlit App Title\n",
    "st.title(\"LLM & Agent Interaction App\")\n",
    "\n",
    "# Sidebar Configuration\n",
    "st.sidebar.header(\"Configuration\")\n",
    "llm_provider = st.sidebar.selectbox(\"Choose LLM Provider\", [\"GPT-4o\", \"GPT-4o-mini\", \"Claude-3.5-Sonnet\", \"Gemini-2.0-Flash\", \"Grok-2-Latest\"])\n",
    "user_input = st.text_area(\"Enter your prompt:\")\n",
    "\n",
    "if st.button(\"Submit Query\"):\n",
    "    if not user_input:\n",
    "        st.warning(\"Please enter a prompt before submitting.\")\n",
    "    else:\n",
    "        try:\n",
    "            if llm_provider == \"GPT-4o\":\n",
    "                response = gpt4o_chat.invoke([HumanMessage(content=user_input)]).content\n",
    "            elif llm_provider == \"GPT-4o-mini\":\n",
    "                response = gpt4o_mini_chat.invoke([HumanMessage(content=user_input)]).content\n",
    "            elif llm_provider == \"Claude-3.5-Sonnet\":\n",
    "                response = claude_chat.invoke(user_input).content\n",
    "            elif llm_provider == \"Gemini-2.0-Flash\":\n",
    "                response = gemini_model.generate_content(user_input).text\n",
    "            elif llm_provider == \"Grok-2-Latest\":\n",
    "                response = query_grok(user_input)\n",
    "            else:\n",
    "                response = \"Invalid model selection.\"\n",
    "            st.success(response)\n",
    "        except Exception as e:\n",
    "            st.error(f\"Error: {e}\")\n",
    "\n",
    "```\n",
    "\n",
    "2. **Install Streamlit et al (if not installed)**\n",
    "   ```bash\n",
    "   pip install streamlit langchain openai google-generativeai langchain-anthropic\n",
    "\n",
    "\n",
    "   ```\n",
    "\n",
    "3. **Run the App**\n",
    "   Navigate to the directory where your `app.py` file is located and run:\n",
    "   ```bash\n",
    "   streamlit run app.py\n",
    "   ```\n",
    "   This will launch a web interface in your browser.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d19a28a-7298-4577-9800-d9fd5d46f888",
   "metadata": {},
   "source": [
    "## Adding more advanced features to our AI Assistant\n",
    "\n",
    "```python\n",
    "import streamlit as st\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "from openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from anthropic import Anthropic\n",
    "from langchain_core.messages import HumanMessage  \n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "def get_env_var(var: str):\n",
    "    value = os.getenv(var)\n",
    "    if value is None:\n",
    "        raise ValueError(f\"{var} not found in environment variables. Make sure it is set in your .env file.\")\n",
    "    return value\n",
    "\n",
    "# Load API keys\n",
    "openai_api_key = get_env_var(\"OPENAI_API_COURSE_KEY\")  \n",
    "gemini_api_key = get_env_var(\"GEMINI_API_KEY\")  \n",
    "anthropic_api_key = get_env_var(\"ANTHROPIC_API_KEY\")  \n",
    "xai_api_key = get_env_var(\"XAI_API_KEY\")\n",
    "tavily_api_key = get_env_var(\"TAVILY_API_KEY\")\n",
    "\n",
    "# Initialize LLMs\n",
    "gpt4o_chat = ChatOpenAI(model=\"gpt-4o\", temperature=0, openai_api_key=openai_api_key)\n",
    "gpt4o_mini_chat = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, openai_api_key=openai_api_key)\n",
    "claude_chat = ChatAnthropic(model=\"claude-3-5-sonnet-20241022\", temperature=0, anthropic_api_key=anthropic_api_key)\n",
    "genai.configure(api_key=gemini_api_key)  \n",
    "gemini_model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "grok_client = OpenAI(api_key=xai_api_key, base_url=\"https://api.x.ai/v1\")\n",
    "\n",
    "def query_grok(prompt: str):\n",
    "    try:\n",
    "        completion = grok_client.chat.completions.create(\n",
    "            model=\"grok-2-latest\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "        )\n",
    "        return completion.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error querying Grok: {e}\"\n",
    "\n",
    "def query_tavily(search_query: str):\n",
    "    \"\"\"Perform a web search using Tavily API.\"\"\"\n",
    "    try:\n",
    "        url = \"https://api.tavily.com/search\"\n",
    "        headers = {\"Authorization\": f\"Bearer {tavily_api_key}\", \"Content-Type\": \"application/json\"}\n",
    "        payload = {\"query\": search_query}\n",
    "        response = requests.post(url, json=payload, headers=headers)\n",
    "        data = response.json()\n",
    "        return data.get(\"results\", [])\n",
    "    except Exception as e:\n",
    "        return [f\"Error querying Tavily: {e}\"]\n",
    "\n",
    "def generate_follow_up_queries(prompt: str, response: str):\n",
    "    \"\"\"Generate LLM-based follow-up queries based on the original query and response.\"\"\"\n",
    "    follow_up_prompt = f\"Based on the following user query and response, suggest three relevant follow-up questions:\\n\\nUser Query: {prompt}\\n\\nResponse: {response}\\n\\nFollow-up Questions:\"\n",
    "    try:\n",
    "        follow_up_text = gpt4o_chat.invoke([HumanMessage(content=follow_up_prompt)]).content\n",
    "        questions = [q.strip() for q in follow_up_text.split(\"\\n\") if q.strip()]\n",
    "        return questions\n",
    "    except Exception as e:\n",
    "        return [f\"Error generating follow-up questions: {e}\"]\n",
    "\n",
    "# Streamlit App Title\n",
    "st.title(\"AI Assistant\")\n",
    "\n",
    "# Sidebar Configuration\n",
    "st.sidebar.header(\"Configuration\")\n",
    "llm_provider = st.sidebar.selectbox(\"Choose LLM Provider\", [\"GPT-4o\", \"GPT-4o-mini\", \"Claude-3.5-Sonnet\", \"Gemini-2.0-Flash\", \"Grok-2-Latest\"])\n",
    "user_persona = st.sidebar.text_input(\"User Persona\", \"General User\")\n",
    "system_persona = st.sidebar.text_input(\"System Persona\", \"AI Assistant\")\n",
    "response_length = st.sidebar.radio(\"Response Length\", [\"Succinct\", \"Standard\", \"Thorough\"], index=1)\n",
    "temperature_setting = st.sidebar.radio(\"Conversation Type (Temperature)\", [\"Creative\", \"Balanced\", \"Precise\"], index=1)\n",
    "num_references = st.sidebar.slider(\"Number of Referenced Responses\", 1, 10, 5)\n",
    "follow_up_enabled = st.sidebar.checkbox(\"Enable Follow-up Queries\")\n",
    "\n",
    "# Initialize session state variables\n",
    "if \"process_query\" not in st.session_state:\n",
    "    st.session_state.process_query = False\n",
    "if \"web_search\" not in st.session_state:\n",
    "    st.session_state.web_search = False\n",
    "if \"user_input\" not in st.session_state:\n",
    "    st.session_state.user_input = \"\"\n",
    "if \"search_results\" not in st.session_state:\n",
    "    st.session_state.search_results = []\n",
    "\n",
    "# Query Interface at the Top\n",
    "st.subheader(\"Enter Your Query\")\n",
    "\n",
    "with st.form(key=\"query_form\"):\n",
    "    st.text_area(\"Enter your prompt:\", key=\"user_input\")\n",
    "    col1, col2 = st.columns([0.5, 0.5])\n",
    "    with col1:\n",
    "        submit_button = st.form_submit_button(\"Submit Query\")\n",
    "    with col2:\n",
    "        web_search_button = st.form_submit_button(\"Web Search with Tavily\")\n",
    "\n",
    "if submit_button:\n",
    "    if not st.session_state.user_input:\n",
    "        st.warning(\"Please enter a prompt before submitting.\")\n",
    "    else:\n",
    "        try:\n",
    "            temperature_values = {\"Creative\": 0.8, \"Balanced\": 0.5, \"Precise\": 0.2}\n",
    "            temperature = temperature_values[temperature_setting]\n",
    "            \n",
    "            if llm_provider == \"GPT-4o\":\n",
    "                response = gpt4o_chat.invoke([HumanMessage(content=st.session_state.user_input)], temperature=temperature).content\n",
    "            elif llm_provider == \"GPT-4o-mini\":\n",
    "                response = gpt4o_mini_chat.invoke([HumanMessage(content=st.session_state.user_input)], temperature=temperature).content\n",
    "            elif llm_provider == \"Claude-3.5-Sonnet\":\n",
    "                response = claude_chat.invoke(st.session_state.user_input).content\n",
    "            elif llm_provider == \"Gemini-2.0-Flash\":\n",
    "                response = gemini_model.generate_content(st.session_state.user_input).text\n",
    "            elif llm_provider == \"Grok-2-Latest\":\n",
    "                response = query_grok(st.session_state.user_input)\n",
    "            else:\n",
    "                response = \"Invalid model selection.\"\n",
    "            \n",
    "            st.subheader(\"Response\")\n",
    "            st.success(f\"{system_persona}: {response}\")\n",
    "            \n",
    "            if follow_up_enabled:\n",
    "                st.subheader(\"Follow-up Questions\")\n",
    "                follow_up_questions = generate_follow_up_queries(st.session_state.user_input, response)\n",
    "                for question in follow_up_questions:\n",
    "                    st.write(f\"- {question}\")\n",
    "        except Exception as e:\n",
    "            st.error(f\"Error: {e}\")\n",
    "\n",
    "if web_search_button:\n",
    "    if not st.session_state.user_input:\n",
    "        st.warning(\"Please enter a search query before submitting.\")\n",
    "    else:\n",
    "        search_results = query_tavily(st.session_state.user_input)\n",
    "        st.subheader(\"Tavily Search Results:\")\n",
    "        if search_results:\n",
    "            for idx, result in enumerate(search_results[:5]):\n",
    "                st.markdown(f\"**{idx+1}. [{result['title']}]({result['url']})**\")\n",
    "                st.write(f\"{result['content']}\")\n",
    "        else:\n",
    "            st.write(\"No relevant search results found.\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d27374-cad1-40aa-9938-5b34c606fb34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
