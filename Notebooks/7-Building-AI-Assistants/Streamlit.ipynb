{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca957d72-3f50-4493-b504-28d53a041556",
   "metadata": {},
   "source": [
    "# Building a Streamlit application to act as our AI Assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5708dfd4-c8b4-4db1-bf62-545b8e9133b5",
   "metadata": {},
   "source": [
    "You need to run this **Streamlit app separately** from your Jupyter notebook. **Streamlit** runs as a web app and doesn't natively integrate within a Jupyter notebook like standard Python scripts. Here's how you can run it:\n",
    "\n",
    "### **Steps to Run the Streamlit App**\n",
    "1. **Save the Code**  \n",
    "   - Save the provided code in a Python script, e.g., `app.py`.\n",
    "  \n",
    "```python\n",
    "import streamlit as st\n",
    "import os\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "from openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from anthropic import Anthropic\n",
    "from langchain_core.messages import HumanMessage  \n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "def get_env_var(var: str):\n",
    "    value = os.getenv(var)\n",
    "    if value is None:\n",
    "        raise ValueError(f\"{var} not found in environment variables. Make sure it is set in your .env file.\")\n",
    "    return value\n",
    "\n",
    "# Load API keys\n",
    "openai_api_key = get_env_var(\"OPENAI_COURSE_KEY\")  \n",
    "gemini_api_key = get_env_var(\"GEMINI_API_KEY\")  \n",
    "anthropic_api_key = get_env_var(\"ANTHROPIC_API_KEY\")  \n",
    "xai_api_key = get_env_var(\"XAI_API_KEY\")\n",
    "\n",
    "# Initialize LLMs\n",
    "gpt4o_chat = ChatOpenAI(model=\"gpt-4o\", temperature=0, openai_api_key=openai_api_key)\n",
    "gpt4o_mini_chat = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, openai_api_key=openai_api_key)\n",
    "claude_chat = ChatAnthropic(model=\"claude-3-5-sonnet-20241022\", temperature=0, anthropic_api_key=anthropic_api_key)\n",
    "genai.configure(api_key=gemini_api_key)  \n",
    "gemini_model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "grok_client = OpenAI(api_key=xai_api_key, base_url=\"https://api.x.ai/v1\")\n",
    "\n",
    "def query_grok(prompt: str):\n",
    "    try:\n",
    "        completion = grok_client.chat.completions.create(\n",
    "            model=\"grok-2-latest\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are Grok, a chatbot inspired by the Hitchhiker's Guide to the Galaxy.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "        )\n",
    "        return completion.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error querying Grok: {e}\"\n",
    "\n",
    "# Streamlit App Title\n",
    "st.title(\"LLM & Agent Interaction App\")\n",
    "\n",
    "# Sidebar Configuration\n",
    "st.sidebar.header(\"Configuration\")\n",
    "llm_provider = st.sidebar.selectbox(\"Choose LLM Provider\", [\"GPT-4o\", \"GPT-4o-mini\", \"Claude-3.5-Sonnet\", \"Gemini-2.0-Flash\", \"Grok-2-Latest\"])\n",
    "user_input = st.text_area(\"Enter your prompt:\")\n",
    "\n",
    "if st.button(\"Submit Query\"):\n",
    "    if not user_input:\n",
    "        st.warning(\"Please enter a prompt before submitting.\")\n",
    "    else:\n",
    "        try:\n",
    "            if llm_provider == \"GPT-4o\":\n",
    "                response = gpt4o_chat.invoke([HumanMessage(content=user_input)]).content\n",
    "            elif llm_provider == \"GPT-4o-mini\":\n",
    "                response = gpt4o_mini_chat.invoke([HumanMessage(content=user_input)]).content\n",
    "            elif llm_provider == \"Claude-3.5-Sonnet\":\n",
    "                response = claude_chat.invoke(user_input).content\n",
    "            elif llm_provider == \"Gemini-2.0-Flash\":\n",
    "                response = gemini_model.generate_content(user_input).text\n",
    "            elif llm_provider == \"Grok-2-Latest\":\n",
    "                response = query_grok(user_input)\n",
    "            else:\n",
    "                response = \"Invalid model selection.\"\n",
    "            st.success(response)\n",
    "        except Exception as e:\n",
    "            st.error(f\"Error: {e}\")\n",
    "\n",
    "```\n",
    "\n",
    "2. **Install Streamlit et al (if not installed)**\n",
    "   ```bash\n",
    "\n",
    "        place below pip installs into your requirements.txt\n",
    "   \n",
    "        streamlit\n",
    "        requests\n",
    "        python-dotenv\n",
    "        google-generativeai\n",
    "        google-genai\n",
    "        openai\n",
    "        anthropic\n",
    "        langchain-openai\n",
    "        langchain-anthropic\n",
    "        langchain-core\n",
    "        langchain-community\n",
    "\n",
    "        python3 -m venv AI-Assistant\n",
    "        source AI-Assistant/bin/activate\n",
    "        pip install -r requirements.txt\n",
    "\n",
    "\n",
    "\n",
    "   ```\n",
    "\n",
    "3. **Run the App**\n",
    "   Navigate to the directory where your `app.py` file is located and run:\n",
    "   ```bash\n",
    "   streamlit run app.py\n",
    "   ```\n",
    "   This will launch a web interface in your browser.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d19a28a-7298-4577-9800-d9fd5d46f888",
   "metadata": {},
   "source": [
    "## Adding more advanced features to our AI Assistant\n",
    "\n",
    "```python\n",
    "import streamlit as st\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "from openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from anthropic import Anthropic\n",
    "from langchain_core.messages import HumanMessage  \n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "def get_env_var(var: str):\n",
    "    value = os.getenv(var)\n",
    "    if value is None:\n",
    "        raise ValueError(f\"{var} not found in environment variables. Make sure it is set in your .env file.\")\n",
    "    return value\n",
    "\n",
    "# Load API keys\n",
    "openai_api_key = get_env_var(\"OPENAI_API_COURSE_KEY\")  \n",
    "gemini_api_key = get_env_var(\"GEMINI_API_KEY\")  \n",
    "anthropic_api_key = get_env_var(\"ANTHROPIC_API_KEY\")  \n",
    "xai_api_key = get_env_var(\"XAI_API_KEY\")\n",
    "tavily_api_key = get_env_var(\"TAVILY_API_KEY\")\n",
    "\n",
    "# Initialize LLMs\n",
    "gpt4o_chat = ChatOpenAI(model=\"gpt-4o\", temperature=0, openai_api_key=openai_api_key)\n",
    "gpt4o_mini_chat = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, openai_api_key=openai_api_key)\n",
    "claude_chat = ChatAnthropic(model=\"claude-3-5-sonnet-20241022\", temperature=0, anthropic_api_key=anthropic_api_key)\n",
    "genai.configure(api_key=gemini_api_key)  \n",
    "gemini_model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "grok_client = OpenAI(api_key=xai_api_key, base_url=\"https://api.x.ai/v1\")\n",
    "\n",
    "def query_grok(prompt: str):\n",
    "    try:\n",
    "        completion = grok_client.chat.completions.create(\n",
    "            model=\"grok-2-latest\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "        )\n",
    "        return completion.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error querying Grok: {e}\"\n",
    "\n",
    "def query_tavily(search_query: str):\n",
    "    \"\"\"Perform a web search using Tavily API.\"\"\"\n",
    "    try:\n",
    "        url = \"https://api.tavily.com/search\"\n",
    "        headers = {\"Authorization\": f\"Bearer {tavily_api_key}\", \"Content-Type\": \"application/json\"}\n",
    "        payload = {\"query\": search_query}\n",
    "        response = requests.post(url, json=payload, headers=headers)\n",
    "        data = response.json()\n",
    "        return data.get(\"results\", [])\n",
    "    except Exception as e:\n",
    "        return [f\"Error querying Tavily: {e}\"]\n",
    "\n",
    "def generate_follow_up_queries(prompt: str, response: str):\n",
    "    \"\"\"Generate LLM-based follow-up queries based on the original query and response.\"\"\"\n",
    "    follow_up_prompt = f\"Based on the following user query and response, suggest three relevant follow-up questions:\\n\\nUser Query: {prompt}\\n\\nResponse: {response}\\n\\nFollow-up Questions:\"\n",
    "    try:\n",
    "        follow_up_text = gpt4o_chat.invoke([HumanMessage(content=follow_up_prompt)]).content\n",
    "        questions = [q.strip() for q in follow_up_text.split(\"\\n\") if q.strip()]\n",
    "        return questions\n",
    "    except Exception as e:\n",
    "        return [f\"Error generating follow-up questions: {e}\"]\n",
    "\n",
    "# Streamlit App Title\n",
    "st.title(\"AI Assistant\")\n",
    "\n",
    "# Sidebar Configuration\n",
    "st.sidebar.header(\"Configuration\")\n",
    "llm_provider = st.sidebar.selectbox(\"Choose LLM Provider\", [\"GPT-4o\", \"GPT-4o-mini\", \"Claude-3.5-Sonnet\", \"Gemini-2.0-Flash\", \"Grok-2-Latest\"])\n",
    "user_persona = st.sidebar.text_input(\"User Persona\", \"General User\")\n",
    "system_persona = st.sidebar.text_input(\"System Persona\", \"AI Assistant\")\n",
    "response_length = st.sidebar.radio(\"Response Length\", [\"Succinct\", \"Standard\", \"Thorough\"], index=1)\n",
    "temperature_setting = st.sidebar.radio(\"Conversation Type (Temperature)\", [\"Creative\", \"Balanced\", \"Precise\"], index=1)\n",
    "num_references = st.sidebar.slider(\"Number of Referenced Responses\", 1, 10, 5)\n",
    "follow_up_enabled = st.sidebar.checkbox(\"Enable Follow-up Queries\")\n",
    "\n",
    "# Initialize session state variables\n",
    "if \"process_query\" not in st.session_state:\n",
    "    st.session_state.process_query = False\n",
    "if \"web_search\" not in st.session_state:\n",
    "    st.session_state.web_search = False\n",
    "if \"user_input\" not in st.session_state:\n",
    "    st.session_state.user_input = \"\"\n",
    "if \"search_results\" not in st.session_state:\n",
    "    st.session_state.search_results = []\n",
    "\n",
    "# Query Interface at the Top\n",
    "st.subheader(\"Enter Your Query\")\n",
    "\n",
    "with st.form(key=\"query_form\"):\n",
    "    st.text_area(\"Enter your prompt:\", key=\"user_input\")\n",
    "    col1, col2 = st.columns([0.5, 0.5])\n",
    "    with col1:\n",
    "        submit_button = st.form_submit_button(\"Submit Query\")\n",
    "    with col2:\n",
    "        web_search_button = st.form_submit_button(\"Web Search with Tavily\")\n",
    "\n",
    "if submit_button:\n",
    "    if not st.session_state.user_input:\n",
    "        st.warning(\"Please enter a prompt before submitting.\")\n",
    "    else:\n",
    "        try:\n",
    "            temperature_values = {\"Creative\": 0.8, \"Balanced\": 0.5, \"Precise\": 0.2}\n",
    "            temperature = temperature_values[temperature_setting]\n",
    "            \n",
    "            if llm_provider == \"GPT-4o\":\n",
    "                response = gpt4o_chat.invoke([HumanMessage(content=st.session_state.user_input)], temperature=temperature).content\n",
    "            elif llm_provider == \"GPT-4o-mini\":\n",
    "                response = gpt4o_mini_chat.invoke([HumanMessage(content=st.session_state.user_input)], temperature=temperature).content\n",
    "            elif llm_provider == \"Claude-3.5-Sonnet\":\n",
    "                response = claude_chat.invoke(st.session_state.user_input).content\n",
    "            elif llm_provider == \"Gemini-2.0-Flash\":\n",
    "                response = gemini_model.generate_content(st.session_state.user_input).text\n",
    "            elif llm_provider == \"Grok-2-Latest\":\n",
    "                response = query_grok(st.session_state.user_input)\n",
    "            else:\n",
    "                response = \"Invalid model selection.\"\n",
    "            \n",
    "            st.subheader(\"Response\")\n",
    "            st.success(f\"{system_persona}: {response}\")\n",
    "            \n",
    "            if follow_up_enabled:\n",
    "                st.subheader(\"Follow-up Questions\")\n",
    "                follow_up_questions = generate_follow_up_queries(st.session_state.user_input, response)\n",
    "                for question in follow_up_questions:\n",
    "                    st.write(f\"- {question}\")\n",
    "        except Exception as e:\n",
    "            st.error(f\"Error: {e}\")\n",
    "\n",
    "if web_search_button:\n",
    "    if not st.session_state.user_input:\n",
    "        st.warning(\"Please enter a search query before submitting.\")\n",
    "    else:\n",
    "        search_results = query_tavily(st.session_state.user_input)\n",
    "        st.subheader(\"Tavily Search Results:\")\n",
    "        if search_results:\n",
    "            for idx, result in enumerate(search_results[:5]):\n",
    "                st.markdown(f\"**{idx+1}. [{result['title']}]({result['url']})**\")\n",
    "                st.write(f\"{result['content']}\")\n",
    "        else:\n",
    "            st.write(\"No relevant search results found.\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22d27374-cad1-40aa-9938-5b34c606fb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4.5-preview\n",
      "gpt-4.5-preview-2025-02-27\n",
      "gpt-4o-mini-audio-preview-2024-12-17\n",
      "dall-e-3\n",
      "dall-e-2\n",
      "gpt-4o-audio-preview-2024-10-01\n",
      "gpt-4o-audio-preview\n",
      "gpt-4o-mini-realtime-preview-2024-12-17\n",
      "gpt-4o-mini-realtime-preview\n",
      "o1-mini-2024-09-12\n",
      "o1-mini\n",
      "omni-moderation-latest\n",
      "gpt-4o-mini-audio-preview\n",
      "omni-moderation-2024-09-26\n",
      "whisper-1\n",
      "gpt-4o-realtime-preview-2024-10-01\n",
      "babbage-002\n",
      "gpt-4-turbo-preview\n",
      "chatgpt-4o-latest\n",
      "tts-1-hd-1106\n",
      "text-embedding-3-large\n",
      "gpt-4-0125-preview\n",
      "gpt-4o-audio-preview-2024-12-17\n",
      "gpt-4o-2024-05-13\n",
      "gpt-4\n",
      "o3-mini-2025-01-31\n",
      "o3-mini\n",
      "tts-1-hd\n",
      "o1-preview\n",
      "o1-preview-2024-09-12\n",
      "gpt-3.5-turbo-instruct-0914\n",
      "tts-1\n",
      "tts-1-1106\n",
      "davinci-002\n",
      "gpt-3.5-turbo-1106\n",
      "gpt-4-turbo\n",
      "gpt-3.5-turbo-instruct\n",
      "o1\n",
      "gpt-4o-2024-08-06\n",
      "gpt-3.5-turbo-0125\n",
      "gpt-4o-realtime-preview-2024-12-17\n",
      "gpt-3.5-turbo\n",
      "gpt-4-turbo-2024-04-09\n",
      "gpt-4o-realtime-preview\n",
      "gpt-3.5-turbo-16k\n",
      "gpt-4o-mini-2024-07-18\n",
      "gpt-4o\n",
      "gpt-4o-mini\n",
      "text-embedding-3-small\n",
      "gpt-4-1106-preview\n",
      "text-embedding-ada-002\n",
      "gpt-4-0613\n",
      "o1-2024-12-17\n",
      "gpt-4o-2024-11-20\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_COURSE_KEY\")\n",
    "url = \"https://api.openai.com/v1/models\"\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "if response.ok:\n",
    "    models = response.json().get(\"data\", [])\n",
    "    for model in models:\n",
    "        print(model[\"id\"])\n",
    "else:\n",
    "    print(\"Error retrieving models:\", response.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e094ad5-c1f2-4e75-ad9b-cbf89ecf0751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Google Models:\n",
      "models/chat-bison-001\n",
      "models/text-bison-001\n",
      "models/embedding-gecko-001\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# Load environment variables from your .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve the Google API key from the .env file.\n",
    "# Ensure your .env file has a line like: GEMINI_API_KEY=your_google_api_key_here\n",
    "google_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "if not google_api_key:\n",
    "    raise ValueError(\"GEMINI_API_KEY not found in the .env file.\")\n",
    "\n",
    "# Construct the URL with the API key as a query parameter\n",
    "url = f\"https://generativelanguage.googleapis.com/v1beta2/models?key={google_api_key}\"\n",
    "\n",
    "# Call the Google API endpoint\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the JSON response\n",
    "    data = response.json()\n",
    "    # Extract the list of models (assuming the response contains a \"models\" key)\n",
    "    models = data.get(\"models\", [])\n",
    "    # Print out the list of models\n",
    "    print(\"Available Google Models:\")\n",
    "    for model in models:\n",
    "        print(model.get(\"name\", \"No name provided\"))\n",
    "else:\n",
    "    print(\"Error retrieving Google models:\", response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bae22de-028c-4523-b98b-ed52c6cb8f15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
