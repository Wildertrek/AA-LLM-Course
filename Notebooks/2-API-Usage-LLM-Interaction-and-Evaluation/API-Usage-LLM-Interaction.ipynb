{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b669b815-cf4b-4061-a20e-13a64110ba13",
   "metadata": {},
   "source": [
    "# API-Usage-LLM-Interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3deedde1-147e-41de-a6b1-33ba720e7774",
   "metadata": {},
   "source": [
    "Let's use a `.env` file and the `dotenv` library. This allows you to keep your API key secure and separated from your code.\n",
    "\n",
    "Here‚Äôs how you can setup to load the OpenAI API key using a `.env` file:\n",
    "\n",
    "### Step 1: Install Dependencies\n",
    "Make sure you have `python-dotenv` installed if you're using `.env` files:\n",
    "```bash\n",
    "pip install python-dotenv\n",
    "pip install -U langsmith\n",
    "pip install openai\n",
    "pip install langchain_openai\n",
    "```\n",
    "\n",
    "### Step 2: Create a `.env` File\n",
    "In your project directory, create a `.env` file with the following content:\n",
    "```\n",
    "OPENAI_API_KEY=your-api-key-here\n",
    "```\n",
    "\n",
    "### Step 3: Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94a134a4-878c-4270-a8b1-ca214909e237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Function to load environment variables or raise an error if not found\n",
    "def get_env_var(var: str):\n",
    "    value = os.getenv(var)\n",
    "    if value is None:\n",
    "        raise ValueError(f\"{var} not found in environment variables. Make sure it is set in your .env file.\")\n",
    "    return value\n",
    "\n",
    "# Load API keys from the environment\n",
    "langchain_api_key = get_env_var(\"LANGCHAIN_API_KEY\")\n",
    "langchain_tracing_v2 = get_env_var(\"LANGCHAIN_TRACING_V2\")\n",
    "openai_api_key = get_env_var(\"OPENAI_API_KEY\")\n",
    "tavily_api_key = get_env_var(\"TAVILY_API_KEY\")\n",
    "\n",
    "# Now, you can use the keys in our setup as needed\n",
    "from langchain_openai import ChatOpenAI\n",
    "gpt4o_chat = ChatOpenAI(model=\"gpt-4o\", temperature=0, openai_api_key=openai_api_key)\n",
    "gpt35_chat = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0, openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63a679ba-05a7-4e72-b838-2bb60337abb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from GPT-4o: Hello! How can I assist you today?\n",
      "Response from GPT-4o-mini: Hello, Joseph! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Function to load environment variables or raise an error if not found\n",
    "def get_env_var(var: str):\n",
    "    value = os.getenv(var)\n",
    "    if value is None:\n",
    "        raise ValueError(f\"{var} not found in environment variables. Make sure it is set in your .env file.\")\n",
    "    return value\n",
    "\n",
    "# Load API keys from the environment\n",
    "langchain_api_key = get_env_var(\"LANGCHAIN_API_KEY\")  # LangChain tracing (if applicable)\n",
    "langchain_tracing_v2 = get_env_var(\"LANGCHAIN_TRACING_V2\")  # LangChain tracing V2 (optional)\n",
    "openai_api_key = get_env_var(\"OPENAI_API_COURSE_KEY\")  # OpenAI API key\n",
    "tavily_api_key = get_env_var(\"TAVILY_API_KEY\")  # Other API key (if used)\n",
    "\n",
    "# Import and configure LangChain OpenAI integration\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize ChatOpenAI with the desired models and temperature settings\n",
    "gpt4o_chat = ChatOpenAI(model=\"gpt-4o\", temperature=0, openai_api_key=openai_api_key)\n",
    "gpt4o_mini_chat = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "# Import LangChain message classes\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Create a message using HumanMessage\n",
    "msg = HumanMessage(content=\"Hello world\", name=\"Joseph\")  # Add content and optional metadata\n",
    "\n",
    "# Create a list of messages\n",
    "messages = [msg]\n",
    "\n",
    "# Invoke the GPT-4o model with the message list\n",
    "response_gpt4o = gpt4o_chat.invoke(messages)\n",
    "print(\"Response from GPT-4o:\", response_gpt4o.content)\n",
    "\n",
    "# Invoke the GPT-4o-mini model with the message list\n",
    "response_gpt4o_mini = gpt4o_mini_chat.invoke(messages)\n",
    "print(\"Response from GPT-4o-mini:\", response_gpt4o_mini.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8aaa1a2c-ea45-4552-9075-198123903ce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 11, 'total_tokens': 20, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_4691090a87', 'finish_reason': 'stop', 'logprobs': None}, id='run-8f88359a-5575-4258-af8f-485cf4fe7ad2-0', usage_metadata={'input_tokens': 11, 'output_tokens': 9, 'total_tokens': 20, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Create a message\n",
    "msg = HumanMessage(content=\"Hello world\", name=\"Joseph\")\n",
    "\n",
    "# Message list\n",
    "messages = [msg]\n",
    "\n",
    "# Invoke the model with a list of messages \n",
    "gpt4o_chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1acfc41-e0cc-404a-8c4e-c107b18884bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 9, 'total_tokens': 19, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_50cad350e4', 'finish_reason': 'stop', 'logprobs': None}, id='run-9a184456-aa97-4429-ac4b-2a23866c11f6-0', usage_metadata={'input_tokens': 9, 'output_tokens': 10, 'total_tokens': 19, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt4o_chat.invoke(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93bcdd11-9a59-4e39-8daf-cdff98ec116c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 9, 'total_tokens': 19, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_bd83329f63', 'finish_reason': 'stop', 'logprobs': None}, id='run-a3ff6f77-dc45-4182-bf7d-b79b295c0acc-0', usage_metadata={'input_tokens': 9, 'output_tokens': 10, 'total_tokens': 19, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt4o_mini_chat.invoke(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf97e30f-a1ab-41c1-80f2-c5fd729a495d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Tavily using the API key from the environment\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "tavily_search = TavilySearchResults(max_results=3, api_key=tavily_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a31011e-cffb-4c8d-8482-b0b70d94771f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://langchain-ai.github.io/langgraph/',\n",
       "  'content': 'Overview¬∂. LangGraph is a library for building stateful, multi-actor applications with LLMs, used to create agent and multi-agent workflows. Compared to other LLM frameworks, it offers these core benefits: cycles, controllability, and persistence. LangGraph allows you to define flows that involve cycles, essential for most agentic architectures, differentiating it from DAG-based solutions.'},\n",
       " {'url': 'https://www.datacamp.com/tutorial/langgraph-tutorial',\n",
       "  'content': 'LangGraph is a library within the LangChain ecosystem designed to tackle these challenges head-on. LangGraph provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured manner.'},\n",
       " {'url': 'https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141',\n",
       "  'content': 'LangGraph is a versatile tool for building complex, stateful applications with LLMs. By understanding its core concepts and working through simple examples, beginners can start to leverage its'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform a search with Tavily\n",
    "search_docs = tavily_search.invoke(\"What is LangGraph?\")\n",
    "search_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9677a7af-8d33-4754-97ef-3bcc4c4a3f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain tracing started successfully.\n",
      "Agent's Response:\n",
      "content='Transformers have revolutionized natural language processing (NLP) by significantly improving the performance of various NLP tasks. Some of the key benefits of using transformers in NLP include:\\n\\n1. Enhanced performance: Transformers have shown superior performance compared to traditional NLP models, especially in tasks such as language translation, text generation, and sentiment analysis.\\n\\n2. Attention mechanism: Transformers use an attention mechanism that allows the model to focus on relevant parts of the input sequence, leading to better understanding and representation of the text.\\n\\n3. Parallel processing: Transformers can process input sequences in parallel, making them more efficient and faster compared to sequential models.\\n\\n4. Transfer learning: Transformers can be fine-tuned on specific tasks with relatively small amounts of data, making them highly adaptable and versatile for various NLP applications.\\n\\n5. Scalability: Transformers can be easily scaled to handle large datasets and complex language structures, making them suitable for a wide range of NLP tasks.\\n\\nOverall, transformers have become the state-of-the-art models in NLP due to their ability to capture long-range dependencies, handle complex language structures, and achieve superior performance on various tasks.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 227, 'prompt_tokens': 33, 'total_tokens': 260, 'completion_tokens_details': {'reasoning_tokens': 0}, 'prompt_tokens_details': {'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-78892699-df7b-4b27-a7be-528ff1a27142-0' usage_metadata={'input_tokens': 33, 'output_tokens': 227, 'total_tokens': 260}\n",
      "Follow-up Response:\n",
      "content='Certainly! GPT (Generative Pre-trained Transformer) and BERT (Bidirectional Encoder Representations from Transformers) are both popular models in the field of natural language processing (NLP). \\n\\nThe main difference between GPT and BERT lies in their architecture and training objectives. GPT is a generative model that predicts the next word in a sequence of text, while BERT is a discriminative model that is trained to understand the context of a word in a sentence by considering both the words before and after it.\\n\\nIn simpler terms, GPT is better suited for tasks that require generating text, such as language translation or text completion, while BERT is more suitable for tasks that require understanding the meaning of a sentence, such as sentiment analysis or question answering.\\n\\nOverall, both GPT and BERT have their own strengths and weaknesses, and the choice between them depends on the specific NLP task at hand.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 184, 'prompt_tokens': 33, 'total_tokens': 217, 'completion_tokens_details': {'reasoning_tokens': 0}, 'prompt_tokens_details': {'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-47d37e82-a8f1-40d4-91de-2c6f2f250540-0' usage_metadata={'input_tokens': 33, 'output_tokens': 184, 'total_tokens': 217}\n",
      "\n",
      "Total Tokens Used: 477\n",
      "Total Cost (USD): $0.0006495000000000001\n",
      "LangChain tracing ended successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Function to load environment variables or raise an error if not found\n",
    "def get_env_var(var: str):\n",
    "    value = os.getenv(var)\n",
    "    if value is None:\n",
    "        raise ValueError(f\"{var} not found in environment variables. Make sure it is set in your .env file.\")\n",
    "    return value\n",
    "\n",
    "# Load API keys from the environment\n",
    "openai_api_key = get_env_var(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Initialize the OpenAI LLM via LangChain\n",
    "gpt_chat = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "# Define a simple prompt template for conversation\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"user_input\"],\n",
    "    template=\"You are a helpful assistant. User asks: {user_input}. How would you respond?\"\n",
    ")\n",
    "\n",
    "# Instead of using RunnableSequence, directly chain the prompt and model using the | operator\n",
    "conversation_chain = prompt_template | gpt_chat\n",
    "\n",
    "try:\n",
    "    # Start callback for tracing OpenAI usage\n",
    "    with get_openai_callback() as cb:\n",
    "        print(\"LangChain tracing started successfully.\")\n",
    "\n",
    "        # Example conversation - LangChain will trace this operation\n",
    "        user_input = \"What are the benefits of using transformers in natural language processing?\"\n",
    "        response = conversation_chain.invoke({\"user_input\": user_input})\n",
    "\n",
    "        print(\"Agent's Response:\")\n",
    "        print(response)\n",
    "\n",
    "        # You can extend this conversation further\n",
    "        follow_up = \"Can you explain the difference between GPT and BERT?\"\n",
    "        follow_up_response = conversation_chain.invoke({\"user_input\": follow_up})\n",
    "\n",
    "        print(\"Follow-up Response:\")\n",
    "        print(follow_up_response)\n",
    "\n",
    "        # Output callback information\n",
    "        print(f\"\\nTotal Tokens Used: {cb.total_tokens}\")\n",
    "        print(f\"Total Cost (USD): ${cb.total_cost}\")\n",
    "\n",
    "    print(\"LangChain tracing ended successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error with LangChain tracing: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b7eb07e-c9f5-4945-9163-405db7cdaa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install deepeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5550b7c1-675f-4d5d-8edc-e681dbbde76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1ca92f39-72c7-47fc-ab0e-9efb644cdb9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 1 test case(s) in parallel: |‚ñà|100% (1/1) [Time Taken: 00:14, 14.06s/test"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚úÖ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the explanation perfectly covers the benefits of using transformers in natural language processing without any irrelevant statements. Great job!, error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Explain the benefits of using transformers in natural language processing.\n",
      "  - actual output: Transformers have revolutionized the field of natural language processing (NLP) due to their unique architecture and capabilities. Here are some of the key benefits of using transformers in NLP:\n",
      "\n",
      "1. Parallelization and Efficiency: Unlike recurrent neural networks (RNNs), which process data sequentially, transformers can process entire sequences of data simultaneously. This parallelization significantly speeds up training and inference, making transformers more efficient, especially when dealing with large datasets.\n",
      "\n",
      "2. Handling Long-Range Dependencies: Transformers use a mechanism called self-attention, which allows them to weigh the importance of different words in a sentence, regardless of their position. This capability enables transformers to capture long-range dependencies and contextual relationships more effectively than traditional models.\n",
      "\n",
      "3. Scalability: Transformers are highly scalable and can be trained on massive datasets. This scalability has led to the development of large pre-trained models like BERT, GPT, and T5, which can be fine-tuned for a variety of downstream tasks with relatively small amounts of task-specific data.\n",
      "\n",
      "4. Transfer Learning: Pre-trained transformer models can be fine-tuned for specific tasks, allowing for transfer learning. This approach leverages the knowledge gained from large-scale pre-training on diverse datasets, improving performance on specific tasks with less data and computational resources.\n",
      "\n",
      "5. Versatility: Transformers are versatile and can be applied to a wide range of NLP tasks, including text classification, translation, summarization, question answering, and more. Their architecture is flexible enough to be adapted for different types of input and output formats.\n",
      "\n",
      "6. State-of-the-Art Performance: Transformers have achieved state-of-the-art results on numerous NLP benchmarks and tasks. Their ability to model complex language patterns and nuances has led to significant improvements in the quality and accuracy of NLP applications.\n",
      "\n",
      "7. Robustness to Noise: The self-attention mechanism in transformers helps them to be more robust to noise and irrelevant information in the input data. This robustness is crucial for real-world applications where data can be messy or incomplete.\n",
      "\n",
      "8. Community and Ecosystem: The popularity of transformers has led to a rich ecosystem of tools, libraries, and community support. Frameworks like Hugging Face's Transformers library provide easy access to pre-trained models and tools for fine-tuning, making it easier for researchers and developers to experiment and deploy transformer-based models.\n",
      "\n",
      "Overall, the transformer architecture has set a new standard in NLP, enabling more accurate, efficient, and versatile language models that continue to push the boundaries of what is possible in the field.\n",
      "  - expected output: None\n",
      "  - context: None\n",
      "  - retrieval context: ['Transformers enable handling long-range dependencies, parallel processing, and fine-tuning for various NLP tasks.']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Answer Relevancy: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">‚úì</span> Tests finished üéâ! Run <span style=\"color: #008000; text-decoration-color: #008000\">'deepeval login'</span> to save and analyze evaluation results on Confident AI. \n",
       "‚ÄºÔ∏è  Friendly reminder üòá: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
       "instead.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;5;245;141m‚úì\u001b[0m Tests finished üéâ! Run \u001b[32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI. \n",
       "‚ÄºÔ∏è  Friendly reminder üòá: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
       "instead.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Summary:\n",
      "\n",
      "Test Case:\n",
      "  - Input: Explain the benefits of using transformers in natural language processing.\n",
      "  - Actual Output: Transformers have revolutionized the field of natural language processing (NLP) due to their unique architecture and capabilities. Here are some of the key benefits of using transformers in NLP:\n",
      "\n",
      "1. Parallelization and Efficiency: Unlike recurrent neural networks (RNNs), which process data sequentially, transformers can process entire sequences of data simultaneously. This parallelization significantly speeds up training and inference, making transformers more efficient, especially when dealing with large datasets.\n",
      "\n",
      "2. Handling Long-Range Dependencies: Transformers use a mechanism called self-attention, which allows them to weigh the importance of different words in a sentence, regardless of their position. This capability enables transformers to capture long-range dependencies and contextual relationships more effectively than traditional models.\n",
      "\n",
      "3. Scalability: Transformers are highly scalable and can be trained on massive datasets. This scalability has led to the development of large pre-trained models like BERT, GPT, and T5, which can be fine-tuned for a variety of downstream tasks with relatively small amounts of task-specific data.\n",
      "\n",
      "4. Transfer Learning: Pre-trained transformer models can be fine-tuned for specific tasks, allowing for transfer learning. This approach leverages the knowledge gained from large-scale pre-training on diverse datasets, improving performance on specific tasks with less data and computational resources.\n",
      "\n",
      "5. Versatility: Transformers are versatile and can be applied to a wide range of NLP tasks, including text classification, translation, summarization, question answering, and more. Their architecture is flexible enough to be adapted for different types of input and output formats.\n",
      "\n",
      "6. State-of-the-Art Performance: Transformers have achieved state-of-the-art results on numerous NLP benchmarks and tasks. Their ability to model complex language patterns and nuances has led to significant improvements in the quality and accuracy of NLP applications.\n",
      "\n",
      "7. Robustness to Noise: The self-attention mechanism in transformers helps them to be more robust to noise and irrelevant information in the input data. This robustness is crucial for real-world applications where data can be messy or incomplete.\n",
      "\n",
      "8. Community and Ecosystem: The popularity of transformers has led to a rich ecosystem of tools, libraries, and community support. Frameworks like Hugging Face's Transformers library provide easy access to pre-trained models and tools for fine-tuning, making it easier for researchers and developers to experiment and deploy transformer-based models.\n",
      "\n",
      "Overall, the transformer architecture has set a new standard in NLP, enabling more accurate, efficient, and versatile language models that continue to push the boundaries of what is possible in the field.\n",
      "\n",
      "Metric: Answer Relevancy\n",
      "  - Score: 1.0\n",
      "  - Success: ‚úÖ\n",
      "  - Threshold: 0.5\n",
      "  - Reason: The score is 1.00 because the explanation perfectly covers the benefits of using transformers in natural language processing without any irrelevant statements. Great job!\n",
      "  - Evaluation Model: gpt-4o\n",
      "  - Evaluation Cost: $0.01340\n",
      "Evaluation results have been logged to 'evaluation_summary.json'\n"
     ]
    }
   ],
   "source": [
    "# GPT-4 Evaluation\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "from deepeval import evaluate\n",
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Initialize the OpenAI LLM via LangChain\n",
    "gpt4o_chat = ChatOpenAI(model=\"gpt-4o\", temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "# Define a prompt for the LLM\n",
    "prompt = \"Explain the benefits of using transformers in natural language processing.\"\n",
    "\n",
    "# Invoke the model and capture the response\n",
    "response = gpt4o_chat.invoke(prompt)\n",
    "response_text = response.content if hasattr(response, 'content') else response\n",
    "\n",
    "# Clean up any Markdown bold formatting\n",
    "cleaned_response_text = re.sub(r'\\*\\*([^*]+)\\*\\*', r'\\1', response_text)\n",
    "\n",
    "# Initialize the Answer Relevancy Metric\n",
    "answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.5)\n",
    "\n",
    "# Create a test case\n",
    "test_case = LLMTestCase(\n",
    "    input=prompt,\n",
    "    actual_output=cleaned_response_text,\n",
    "    retrieval_context=[\"Transformers enable handling long-range dependencies, parallel processing, and fine-tuning for various NLP tasks.\"]\n",
    ")\n",
    "\n",
    "# Evaluate using evaluate function to capture results\n",
    "evaluation_results = evaluate([test_case], [answer_relevancy_metric])\n",
    "\n",
    "print(\"Evaluation Summary:\")\n",
    "\n",
    "try:\n",
    "    # Assuming `evaluation_results` is a single EvaluationResult object with a list of TestResult objects\n",
    "    for result in evaluation_results.test_results:\n",
    "        print(\"\\nTest Case:\")\n",
    "        print(f\"  - Input: {result.input}\")\n",
    "        print(f\"  - Actual Output: {result.actual_output}\")\n",
    "\n",
    "        # Loop through metric data in the test result\n",
    "        for metric_data in result.metrics_data:\n",
    "            print(f\"\\nMetric: {metric_data.name}\")\n",
    "            print(f\"  - Score: {metric_data.score}\")\n",
    "            print(f\"  - Success: {'‚úÖ' if metric_data.success else '‚ùå'}\")\n",
    "            print(f\"  - Threshold: {metric_data.threshold}\")\n",
    "            print(f\"  - Reason: {metric_data.reason}\")\n",
    "            print(f\"  - Evaluation Model: {metric_data.evaluation_model}\")\n",
    "            print(f\"  - Evaluation Cost: ${metric_data.evaluation_cost:.5f}\")\n",
    "\n",
    "    # Log the results to a JSON file\n",
    "    with open('evaluation_summary.json', 'w') as file:\n",
    "        json.dump(evaluation_results.dict(), file, indent=4)\n",
    "    print(\"Evaluation results have been logged to 'evaluation_summary.json'\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred while processing evaluation results:\", str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bdd18dfc-790c-42b9-b534-31a3d02efaaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 1 test case(s) in parallel: |‚ñà|100% (1/1) [Time Taken: 00:15, 15.28s/test"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚úÖ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the explanation provided is perfectly relevant and directly addresses the benefits of using transformers in natural language processing without any irrelevant information. Excellent job!, error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Explain the benefits of using transformers in natural language processing.\n",
      "  - actual output: Transformers have revolutionized the field of natural language processing (NLP) due to several key benefits:\n",
      "\n",
      "1. **Parallelization**: Unlike recurrent neural networks (RNNs), which process data sequentially, transformers allow for parallel processing of data. This is achieved through the self-attention mechanism, which enables the model to consider all words in a sentence simultaneously. This parallelization significantly speeds up training and inference times.\n",
      "\n",
      "2. **Handling Long-Range Dependencies**: Transformers are particularly effective at capturing long-range dependencies in text. The self-attention mechanism allows the model to weigh the importance of different words in a sentence, regardless of their distance from each other. This is crucial for understanding context and meaning in complex sentences.\n",
      "\n",
      "3. **Scalability**: Transformers can be scaled up to create very large models, such as GPT-3 and BERT, which have demonstrated state-of-the-art performance on a wide range of NLP tasks. The architecture's ability to handle large amounts of data and parameters makes it suitable for training on extensive datasets.\n",
      "\n",
      "4. **Transfer Learning**: Pre-trained transformer models can be fine-tuned on specific tasks with relatively small amounts of task-specific data. This transfer learning capability allows for efficient use of resources and improves performance on tasks like sentiment analysis, question answering, and machine translation.\n",
      "\n",
      "5. **Versatility**: Transformers are highly versatile and can be applied to various NLP tasks, including text classification, translation, summarization, and more. Their architecture is flexible enough to be adapted for different types of input and output formats.\n",
      "\n",
      "6. **Improved Contextual Understanding**: Transformers provide a better understanding of context by using mechanisms like self-attention to focus on relevant parts of the input text. This leads to more accurate and nuanced language understanding, which is essential for tasks like sentiment analysis and language generation.\n",
      "\n",
      "7. **Reduced Need for Feature Engineering**: Traditional NLP models often required extensive feature engineering to capture linguistic nuances. Transformers, on the other hand, learn these features automatically from the data, reducing the need for manual intervention and domain expertise.\n",
      "\n",
      "8. **Robustness to Noise**: Transformers tend to be more robust to noise and variations in input data compared to some traditional models. This robustness is partly due to their ability to focus on relevant parts of the input and ignore irrelevant or misleading information.\n",
      "\n",
      "Overall, the transformer architecture has set new benchmarks in NLP by providing a powerful, flexible, and efficient framework for understanding and generating human language.\n",
      "  - expected output: None\n",
      "  - context: None\n",
      "  - retrieval context: ['Transformers enable handling long-range dependencies, parallel processing, and fine-tuning for various NLP tasks.']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Answer Relevancy: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">‚úì</span> Tests finished üéâ! Run <span style=\"color: #008000; text-decoration-color: #008000\">'deepeval login'</span> to save and analyze evaluation results on Confident AI. \n",
       "‚ÄºÔ∏è  Friendly reminder üòá: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
       "instead.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;5;245;141m‚úì\u001b[0m Tests finished üéâ! Run \u001b[32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI. \n",
       "‚ÄºÔ∏è  Friendly reminder üòá: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
       "instead.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 1 test case(s) in parallel: |‚ñà|100% (1/1) [Time Taken: 00:07,  7.33s/test"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚úÖ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response is perfectly relevant and effectively addresses the benefits of using transformers in natural language processing. Great job!, error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Explain the benefits of using transformers in natural language processing.\n",
      "  - actual output: Transformers are a type of deep learning model that has revolutionized natural language processing (NLP) tasks. One of the main benefits of using transformers in NLP is their ability to capture long-range dependencies in text data. Traditional models like recurrent neural networks struggle with this because they process text sequentially, which can lead to information loss over long distances. Transformers, on the other hand, use self-attention mechanisms to weigh the importance of each word in a sentence, allowing them to capture relationships between words that are far apart.\n",
      "\n",
      "Another benefit of transformers is their scalability. They can be easily scaled up to handle large amounts of data and complex tasks, making them suitable for a wide range of NLP applications. Additionally, transformers have been pre-trained on massive amounts of text data, which allows them to learn general language patterns and semantics. This pre-training can be fine-tuned on specific tasks, making transformers highly adaptable and versatile for various NLP tasks.\n",
      "\n",
      "Overall, transformers have significantly improved the performance of NLP models, leading to better accuracy, efficiency, and generalization on a wide range of tasks, such as text classification, sentiment analysis, machine translation, and more. Their ability to capture long-range dependencies, scalability, and pre-training capabilities make them a powerful tool for NLP researchers and practitioners.\n",
      "  - expected output: None\n",
      "  - context: None\n",
      "  - retrieval context: ['Transformers enable handling long-range dependencies, parallel processing, and fine-tuning for various NLP tasks.']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Answer Relevancy: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">‚úì</span> Tests finished üéâ! Run <span style=\"color: #008000; text-decoration-color: #008000\">'deepeval login'</span> to save and analyze evaluation results on Confident AI. \n",
       "‚ÄºÔ∏è  Friendly reminder üòá: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
       "instead.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;5;245;141m‚úì\u001b[0m Tests finished üéâ! Run \u001b[32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI. \n",
       "‚ÄºÔ∏è  Friendly reminder üòá: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
       "instead.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GPT-4 Evaluation Results ===\n",
      "\n",
      "Input: Explain the benefits of using transformers in natural language processing.\n",
      "Output: Transformers have revolutionized the field of natural language processing (NLP) due to several key benefits:\n",
      "\n",
      "1. **Parallelization**: Unlike recurrent neural networks (RNNs), which process data sequentially, transformers allow for parallel processing of data. This is achieved through the self-attention mechanism, which enables the model to consider all words in a sentence simultaneously. This parallelization significantly speeds up training and inference times.\n",
      "\n",
      "2. **Handling Long-Range Dependencies**: Transformers are particularly effective at capturing long-range dependencies in text. The self-attention mechanism allows the model to weigh the importance of different words in a sentence, regardless of their distance from each other. This is crucial for understanding context and meaning in complex sentences.\n",
      "\n",
      "3. **Scalability**: Transformers can be scaled up to create very large models, such as GPT-3 and BERT, which have demonstrated state-of-the-art performance on a wide range of NLP tasks. The architecture's ability to handle large amounts of data and parameters makes it suitable for training on extensive datasets.\n",
      "\n",
      "4. **Transfer Learning**: Pre-trained transformer models can be fine-tuned on specific tasks with relatively small amounts of task-specific data. This transfer learning capability allows for efficient use of resources and improves performance on tasks like sentiment analysis, question answering, and machine translation.\n",
      "\n",
      "5. **Versatility**: Transformers are highly versatile and can be applied to various NLP tasks, including text classification, translation, summarization, and more. Their architecture is flexible enough to be adapted for different types of input and output formats.\n",
      "\n",
      "6. **Improved Contextual Understanding**: Transformers provide a better understanding of context by using mechanisms like self-attention to focus on relevant parts of the input text. This leads to more accurate and nuanced language understanding, which is essential for tasks like sentiment analysis and language generation.\n",
      "\n",
      "7. **Reduced Need for Feature Engineering**: Traditional NLP models often required extensive feature engineering to capture linguistic nuances. Transformers, on the other hand, learn these features automatically from the data, reducing the need for manual intervention and domain expertise.\n",
      "\n",
      "8. **Robustness to Noise**: Transformers tend to be more robust to noise and variations in input data compared to some traditional models. This robustness is partly due to their ability to focus on relevant parts of the input and ignore irrelevant or misleading information.\n",
      "\n",
      "Overall, the transformer architecture has set new benchmarks in NLP by providing a powerful, flexible, and efficient framework for understanding and generating human language.\n",
      "\n",
      "Metric: Answer Relevancy\n",
      " - Score: 1.0\n",
      " - Success: ‚úÖ\n",
      " - Reason: The score is 1.00 because the explanation provided is perfectly relevant and directly addresses the benefits of using transformers in natural language processing without any irrelevant information. Excellent job!\n",
      " - Evaluation Model: gpt-4o\n",
      " - Evaluation Cost: $0.01331\n",
      "\n",
      "==================================================\n",
      "\n",
      "=== GPT-3.5 Evaluation Results ===\n",
      "\n",
      "Input: Explain the benefits of using transformers in natural language processing.\n",
      "Output: Transformers are a type of deep learning model that has revolutionized natural language processing (NLP) tasks. One of the main benefits of using transformers in NLP is their ability to capture long-range dependencies in text data. Traditional models like recurrent neural networks struggle with this because they process text sequentially, which can lead to information loss over long distances. Transformers, on the other hand, use self-attention mechanisms to weigh the importance of each word in a sentence, allowing them to capture relationships between words that are far apart.\n",
      "\n",
      "Another benefit of transformers is their scalability. They can be easily scaled up to handle large amounts of data and complex tasks, making them suitable for a wide range of NLP applications. Additionally, transformers have been pre-trained on massive amounts of text data, which allows them to learn general language patterns and semantics. This pre-training can be fine-tuned on specific tasks, making transformers highly adaptable and versatile for various NLP tasks.\n",
      "\n",
      "Overall, transformers have significantly improved the performance of NLP models, leading to better accuracy, efficiency, and generalization on a wide range of tasks, such as text classification, sentiment analysis, machine translation, and more. Their ability to capture long-range dependencies, scalability, and pre-training capabilities make them a powerful tool for NLP researchers and practitioners.\n",
      "\n",
      "Metric: Answer Relevancy\n",
      " - Score: 1.0\n",
      " - Success: ‚úÖ\n",
      " - Reason: The score is 1.00 because the response is perfectly relevant and effectively addresses the benefits of using transformers in natural language processing. Great job!\n",
      " - Evaluation Model: gpt-4o\n",
      " - Evaluation Cost: $0.00776\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from deepeval import evaluate\n",
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Initialize both GPT-4 and GPT-3.5 models\n",
    "gpt4o_chat = ChatOpenAI(model=\"gpt-4o\", temperature=0, openai_api_key=openai_api_key)\n",
    "gpt35_chat = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "# Define a prompt for the LLMs\n",
    "prompt = \"Explain the benefits of using transformers in natural language processing.\"\n",
    "\n",
    "# Get responses from both models\n",
    "gpt4_response = gpt4o_chat.invoke(prompt).content\n",
    "gpt35_response = gpt35_chat.invoke(prompt).content\n",
    "\n",
    "# Initialize the Answer Relevancy Metric\n",
    "answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.5)\n",
    "\n",
    "# Create test cases for each model response\n",
    "gpt4_test_case = LLMTestCase(\n",
    "    input=prompt,\n",
    "    actual_output=gpt4_response,\n",
    "    retrieval_context=[\"Transformers enable handling long-range dependencies, parallel processing, and fine-tuning for various NLP tasks.\"]\n",
    ")\n",
    "\n",
    "gpt35_test_case = LLMTestCase(\n",
    "    input=prompt,\n",
    "    actual_output=gpt35_response,\n",
    "    retrieval_context=[\"Transformers enable handling long-range dependencies, parallel processing, and fine-tuning for various NLP tasks.\"]\n",
    ")\n",
    "\n",
    "# Evaluate both test cases\n",
    "evaluation_results_gpt4 = evaluate([gpt4_test_case], [answer_relevancy_metric])\n",
    "evaluation_results_gpt35 = evaluate([gpt35_test_case], [answer_relevancy_metric])\n",
    "\n",
    "# Function to process and print results\n",
    "def print_results(model_name, evaluation_results):\n",
    "    print(f\"\\n=== {model_name} Evaluation Results ===\")\n",
    "    for result in evaluation_results.test_results:\n",
    "        print(f\"\\nInput: {result.input}\")\n",
    "        print(f\"Output: {result.actual_output}\\n\")\n",
    "        for metric_data in result.metrics_data:\n",
    "            print(f\"Metric: {metric_data.name}\")\n",
    "            print(f\" - Score: {metric_data.score}\")\n",
    "            print(f\" - Success: {'‚úÖ' if metric_data.success else '‚ùå'}\")\n",
    "            print(f\" - Reason: {metric_data.reason}\")\n",
    "            print(f\" - Evaluation Model: {metric_data.evaluation_model}\")\n",
    "            print(f\" - Evaluation Cost: ${metric_data.evaluation_cost:.5f}\")\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Print and log results for both models\n",
    "print_results(\"GPT-4\", evaluation_results_gpt4)\n",
    "print_results(\"GPT-3.5\", evaluation_results_gpt35)\n",
    "\n",
    "# Optionally log results to file\n",
    "results_to_log = {\n",
    "    \"GPT-4\": evaluation_results_gpt4.dict(),\n",
    "    \"GPT-3.5\": evaluation_results_gpt35.dict()\n",
    "}\n",
    "with open('evaluation_summary_comparison.json', 'w') as f:\n",
    "    json.dump(results_to_log, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "08235524-5e75-4327-84b0-59901c101889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 1 test case(s) in parallel: |‚ñà|100% (1/1) [Time Taken: 00:11, 11.16s/test"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚úÖ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response is perfectly relevant and directly addresses the benefits of using transformers in natural language processing without any irrelevant information. Great job!, error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Explain the benefits of using transformers in natural language processing.\n",
      "  - actual output: Transformers have revolutionized the field of natural language processing (NLP) due to their unique architecture and capabilities. Here are some of the key benefits of using transformers in NLP:\n",
      "\n",
      "1. Parallelization and Efficiency: Unlike recurrent neural networks (RNNs), which process data sequentially, transformers can process entire sequences of data simultaneously. This parallelization significantly speeds up training and inference, making transformers more efficient, especially when dealing with large datasets.\n",
      "\n",
      "2. Handling Long-Range Dependencies: Transformers use a mechanism called self-attention, which allows them to weigh the importance of different words in a sentence, regardless of their position. This capability enables transformers to capture long-range dependencies and contextual relationships more effectively than traditional models like RNNs or LSTMs.\n",
      "\n",
      "3. Scalability: Transformers are highly scalable and can be trained on massive datasets. This scalability has led to the development of large pre-trained models like BERT, GPT, and T5, which can be fine-tuned for a variety of downstream tasks with relatively small amounts of task-specific data.\n",
      "\n",
      "4. Transfer Learning: Pre-trained transformer models can be fine-tuned on specific tasks, allowing them to leverage the knowledge gained from large-scale pre-training. This transfer learning approach has been highly successful in improving performance across a wide range of NLP tasks, such as sentiment analysis, question answering, and machine translation.\n",
      "\n",
      "5. Versatility: Transformers are versatile and can be applied to various NLP tasks without significant architectural changes. They have been used for tasks such as text classification, named entity recognition, language modeling, and more. This versatility makes them a go-to choice for many NLP applications.\n",
      "\n",
      "6. State-of-the-Art Performance: Transformers have consistently achieved state-of-the-art results on numerous NLP benchmarks. Their ability to model complex language patterns and understand context has led to significant improvements in the accuracy and quality of NLP systems.\n",
      "\n",
      "7. Robustness to Input Variability: The self-attention mechanism in transformers allows them to be more robust to variations in input data, such as changes in word order or the presence of irrelevant information. This robustness contributes to their effectiveness in real-world applications.\n",
      "\n",
      "8. Support for Multimodal Learning: Transformers can be extended to handle multimodal data, such as text and images, by incorporating additional modalities into the self-attention mechanism. This capability is useful for tasks that require understanding and integrating information from multiple sources.\n",
      "\n",
      "Overall, the transformer architecture has become a cornerstone of modern NLP due to its efficiency, flexibility, and ability to produce high-quality results across a wide range of tasks.\n",
      "  - expected output: None\n",
      "  - context: None\n",
      "  - retrieval context: ['Transformers enable handling long-range dependencies, parallel processing, and fine-tuning for various NLP tasks.']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Answer Relevancy: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">‚úì</span> Tests finished üéâ! Run <span style=\"color: #008000; text-decoration-color: #008000\">'deepeval login'</span> to save and analyze evaluation results on Confident AI. \n",
       "‚ÄºÔ∏è  Friendly reminder üòá: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
       "instead.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;5;245;141m‚úì\u001b[0m Tests finished üéâ! Run \u001b[32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI. \n",
       "‚ÄºÔ∏è  Friendly reminder üòá: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
       "instead.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 1 test case(s) in parallel: |‚ñà|100% (1/1) [Time Taken: 00:06,  6.34s/test"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚úÖ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response is perfectly relevant and fully addresses the benefits of using transformers in natural language processing without any irrelevant information. Great job!, error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Explain the benefits of using transformers in natural language processing.\n",
      "  - actual output: Transformers are a type of deep learning model that has revolutionized natural language processing (NLP) tasks. One of the main benefits of using transformers in NLP is their ability to capture long-range dependencies in text data. Traditional models like recurrent neural networks (RNNs) struggle with this because they process text sequentially, which can lead to information loss over long distances. Transformers, on the other hand, use self-attention mechanisms to weigh the importance of each word in a sentence, allowing them to capture relationships between words that are far apart.\n",
      "\n",
      "Another benefit of transformers is their scalability. Transformers can be easily scaled up to handle large amounts of data and complex tasks, making them well-suited for a wide range of NLP applications. Additionally, transformers have been pre-trained on massive amounts of text data, which allows them to learn general language patterns and semantics. This pre-training can be fine-tuned on specific tasks, making transformers highly adaptable and versatile for various NLP tasks.\n",
      "\n",
      "Overall, transformers have significantly improved the performance of NLP models by capturing long-range dependencies, being scalable, and having the ability to learn from large amounts of data. Their flexibility and efficiency make them a powerful tool for a wide range of NLP applications.\n",
      "  - expected output: None\n",
      "  - context: None\n",
      "  - retrieval context: ['Transformers enable handling long-range dependencies, parallel processing, and fine-tuning for various NLP tasks.']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Answer Relevancy: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">‚úì</span> Tests finished üéâ! Run <span style=\"color: #008000; text-decoration-color: #008000\">'deepeval login'</span> to save and analyze evaluation results on Confident AI. \n",
       "‚ÄºÔ∏è  Friendly reminder üòá: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
       "instead.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;5;245;141m‚úì\u001b[0m Tests finished üéâ! Run \u001b[32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI. \n",
       "‚ÄºÔ∏è  Friendly reminder üòá: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
       "instead.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GPT-4o Evaluation Results ===\n",
      "\n",
      "Input: Explain the benefits of using transformers in natural language processing.\n",
      "Output: Transformers have revolutionized the field of natural language processing (NLP) due to their unique architecture and capabilities. Here are some of the key benefits of using transformers in NLP:\n",
      "\n",
      "1. Parallelization and Efficiency: Unlike recurrent neural networks (RNNs), which process data sequentially, transformers can process entire sequences of data simultaneously. This parallelization significantly speeds up training and inference, making transformers more efficient, especially when dealing with large datasets.\n",
      "\n",
      "2. Handling Long-Range Dependencies: Transformers use a mechanism called self-attention, which allows them to weigh the importance of different words in a sentence, regardless of their position. This capability enables transformers to capture long-range dependencies and contextual relationships more effectively than traditional models like RNNs or LSTMs.\n",
      "\n",
      "3. Scalability: Transformers are highly scalable and can be trained on massive datasets. This scalability has led to the development of large pre-trained models like BERT, GPT, and T5, which can be fine-tuned for a variety of downstream tasks with relatively small amounts of task-specific data.\n",
      "\n",
      "4. Transfer Learning: Pre-trained transformer models can be fine-tuned on specific tasks, allowing them to leverage the knowledge gained from large-scale pre-training. This transfer learning approach has been highly successful in improving performance across a wide range of NLP tasks, such as sentiment analysis, question answering, and machine translation.\n",
      "\n",
      "5. Versatility: Transformers are versatile and can be applied to various NLP tasks without significant architectural changes. They have been used for tasks such as text classification, named entity recognition, language modeling, and more. This versatility makes them a go-to choice for many NLP applications.\n",
      "\n",
      "6. State-of-the-Art Performance: Transformers have consistently achieved state-of-the-art results on numerous NLP benchmarks. Their ability to model complex language patterns and understand context has led to significant improvements in the accuracy and quality of NLP systems.\n",
      "\n",
      "7. Robustness to Input Variability: The self-attention mechanism in transformers allows them to be more robust to variations in input data, such as changes in word order or the presence of irrelevant information. This robustness contributes to their effectiveness in real-world applications.\n",
      "\n",
      "8. Support for Multimodal Learning: Transformers can be extended to handle multimodal data, such as text and images, by incorporating additional modalities into the self-attention mechanism. This capability is useful for tasks that require understanding and integrating information from multiple sources.\n",
      "\n",
      "Overall, the transformer architecture has become a cornerstone of modern NLP due to its efficiency, flexibility, and ability to produce high-quality results across a wide range of tasks.\n",
      "\n",
      "Metric: Answer Relevancy\n",
      " - Score: 1.0\n",
      " - Success: ‚úÖ\n",
      " - Reason: The score is 1.00 because the response is perfectly relevant and directly addresses the benefits of using transformers in natural language processing without any irrelevant information. Great job!\n",
      " - Evaluation Model: gpt-4o\n",
      " - Evaluation Cost: $0.01262\n",
      "==================================================\n",
      "\n",
      "=== GPT-3.5-turbo Evaluation Results ===\n",
      "\n",
      "Input: Explain the benefits of using transformers in natural language processing.\n",
      "Output: Transformers are a type of deep learning model that has revolutionized natural language processing (NLP) tasks. One of the main benefits of using transformers in NLP is their ability to capture long-range dependencies in text data. Traditional models like recurrent neural networks (RNNs) struggle with this because they process text sequentially, which can lead to information loss over long distances. Transformers, on the other hand, use self-attention mechanisms to weigh the importance of each word in a sentence, allowing them to capture relationships between words that are far apart.\n",
      "\n",
      "Another benefit of transformers is their scalability. Transformers can be easily scaled up to handle large amounts of data and complex tasks, making them well-suited for a wide range of NLP applications. Additionally, transformers have been pre-trained on massive amounts of text data, which allows them to learn general language patterns and semantics. This pre-training can be fine-tuned on specific tasks, making transformers highly adaptable and versatile for various NLP tasks.\n",
      "\n",
      "Overall, transformers have significantly improved the performance of NLP models by capturing long-range dependencies, being scalable, and having the ability to learn from large amounts of data. Their flexibility and efficiency make them a powerful tool for a wide range of NLP applications.\n",
      "\n",
      "Metric: Answer Relevancy\n",
      " - Score: 1.0\n",
      " - Success: ‚úÖ\n",
      " - Reason: The score is 1.00 because the response is perfectly relevant and fully addresses the benefits of using transformers in natural language processing without any irrelevant information. Great job!\n",
      " - Evaluation Model: gpt-4o\n",
      " - Evaluation Cost: $0.00763\n",
      "==================================================\n",
      "\n",
      "Evaluation results have been logged to 'evaluation_summary.json'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from deepeval import evaluate\n",
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Initialize the OpenAI LLMs via LangChain\n",
    "gpt4o_chat = ChatOpenAI(model=\"gpt-4o\", temperature=0, openai_api_key=openai_api_key)\n",
    "gpt35_chat = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "# Define a prompt for the LLM\n",
    "prompt = \"Explain the benefits of using transformers in natural language processing.\"\n",
    "\n",
    "# Retrieve responses from both models\n",
    "gpt4_response = gpt4o_chat.invoke(prompt).content\n",
    "gpt35_response = gpt35_chat.invoke(prompt).content\n",
    "\n",
    "# Clean up any Markdown bold formatting\n",
    "cleaned_gpt4_response = re.sub(r'\\*\\*([^*]+)\\*\\*', r'\\1', gpt4_response)\n",
    "cleaned_gpt35_response = re.sub(r'\\*\\*([^*]+)\\*\\*', r'\\1', gpt35_response)\n",
    "\n",
    "# Initialize the Answer Relevancy Metric\n",
    "answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.5)\n",
    "\n",
    "# Create test cases for each model's response\n",
    "gpt4_test_case = LLMTestCase(\n",
    "    input=prompt,\n",
    "    actual_output=cleaned_gpt4_response,\n",
    "    retrieval_context=[\"Transformers enable handling long-range dependencies, parallel processing, and fine-tuning for various NLP tasks.\"]\n",
    ")\n",
    "\n",
    "gpt35_test_case = LLMTestCase(\n",
    "    input=prompt,\n",
    "    actual_output=cleaned_gpt35_response,\n",
    "    retrieval_context=[\"Transformers enable handling long-range dependencies, parallel processing, and fine-tuning for various NLP tasks.\"]\n",
    ")\n",
    "\n",
    "# Evaluate both test cases\n",
    "evaluation_results_gpt4 = evaluate([gpt4_test_case], [answer_relevancy_metric])\n",
    "evaluation_results_gpt35 = evaluate([gpt35_test_case], [answer_relevancy_metric])\n",
    "\n",
    "# Function to print evaluation summary\n",
    "def print_evaluation_summary(model_name, evaluation_results):\n",
    "    print(f\"\\n=== {model_name} Evaluation Results ===\\n\")\n",
    "    for result in evaluation_results.test_results:\n",
    "        print(f\"Input: {result.input}\")\n",
    "        print(f\"Output: {result.actual_output}\\n\")\n",
    "        \n",
    "        for metric_data in result.metrics_data:\n",
    "            print(f\"Metric: {metric_data.name}\")\n",
    "            print(f\" - Score: {metric_data.score}\")\n",
    "            print(f\" - Success: {'‚úÖ' if metric_data.success else '‚ùå'}\")\n",
    "            print(f\" - Reason: {metric_data.reason}\")\n",
    "            print(f\" - Evaluation Model: {metric_data.evaluation_model}\")\n",
    "            print(f\" - Evaluation Cost: ${metric_data.evaluation_cost:.5f}\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "# Print evaluation summaries for both models\n",
    "print_evaluation_summary(\"GPT-4o\", evaluation_results_gpt4)\n",
    "print_evaluation_summary(\"GPT-3.5-turbo\", evaluation_results_gpt35)\n",
    "\n",
    "# Save the results to a JSON file\n",
    "output_data = {\n",
    "    \"gpt-4o\": evaluation_results_gpt4.dict(),\n",
    "    \"gpt-3.5-turbo\": evaluation_results_gpt35.dict()\n",
    "}\n",
    "with open('evaluation_summary.json', 'w') as f:\n",
    "    json.dump(output_data, f, indent=4)\n",
    "\n",
    "print(\"\\nEvaluation results have been logged to 'evaluation_summary.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "26309e36-5f55-423e-b17d-925de8a36401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 1 test case(s) in parallel: |‚ñà|100% (1/1) [Time Taken: 00:11, 11.03s/test"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚úÖ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the answer perfectly addresses the question about the benefits of using transformers in natural language processing without any irrelevant statements. Great job!, error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Explain the benefits of using transformers in natural language processing.\n",
      "  - actual output: Transformers have revolutionized the field of natural language processing (NLP) due to their unique architecture and capabilities. Here are some of the key benefits of using transformers in NLP:\n",
      "\n",
      "1. Parallelization: Unlike recurrent neural networks (RNNs), transformers do not require sequential data processing, which allows for parallelization during training. This significantly speeds up the training process and makes it feasible to train on large datasets.\n",
      "\n",
      "2. Handling Long-Range Dependencies: Transformers use self-attention mechanisms that allow them to consider the entire context of a sentence or document when making predictions. This is particularly useful for capturing long-range dependencies in text, which is a limitation of traditional RNNs and LSTMs.\n",
      "\n",
      "3. Scalability: The architecture of transformers is highly scalable. Models like BERT, GPT, and T5 have billions of parameters, which allows them to learn complex patterns and representations from vast amounts of data.\n",
      "\n",
      "4. Transfer Learning: Transformers have popularized the use of transfer learning in NLP. Pre-trained models can be fine-tuned on specific tasks with relatively small amounts of data, which reduces the need for large labeled datasets and computational resources.\n",
      "\n",
      "5. State-of-the-Art Performance: Transformers have achieved state-of-the-art results on a wide range of NLP tasks, including machine translation, text summarization, sentiment analysis, and question answering. Their ability to model complex language patterns has set new benchmarks in the field.\n",
      "\n",
      "6. Versatility: The transformer architecture is highly versatile and can be adapted for various tasks beyond NLP, such as image processing and protein folding. This adaptability makes transformers a powerful tool across different domains.\n",
      "\n",
      "7. Robustness to Input Variability: Transformers can handle variable-length input sequences effectively, making them suitable for tasks involving diverse text lengths and structures.\n",
      "\n",
      "8. Improved Contextual Understanding: The self-attention mechanism allows transformers to weigh the importance of different words in a sentence, leading to a better understanding of context and meaning.\n",
      "\n",
      "9. Reduced Vanishing Gradient Problem: By avoiding the sequential processing of data, transformers mitigate the vanishing gradient problem that often plagues RNNs, leading to more stable and effective training.\n",
      "\n",
      "Overall, transformers have become a cornerstone of modern NLP due to their efficiency, effectiveness, and flexibility, enabling significant advancements in the field.\n",
      "  - expected output: None\n",
      "  - context: None\n",
      "  - retrieval context: ['Transformers enable handling long-range dependencies, parallel processing, and fine-tuning for various NLP tasks.']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Answer Relevancy: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">‚úì</span> Tests finished üéâ! Run <span style=\"color: #008000; text-decoration-color: #008000\">'deepeval login'</span> to save and analyze evaluation results on Confident AI. \n",
       "‚ÄºÔ∏è  Friendly reminder üòá: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
       "instead.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;5;245;141m‚úì\u001b[0m Tests finished üéâ! Run \u001b[32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI. \n",
       "‚ÄºÔ∏è  Friendly reminder üòá: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
       "instead.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 1 test case(s) in parallel: |‚ñà|100% (1/1) [Time Taken: 00:07,  7.55s/test"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚úÖ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the output perfectly addresses the input with no irrelevant statements, providing a clear and focused explanation of the benefits of using transformers in natural language processing. Great job!, error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Explain the benefits of using transformers in natural language processing.\n",
      "  - actual output: Transformers are a type of deep learning model that has revolutionized natural language processing (NLP) tasks. One of the main benefits of using transformers in NLP is their ability to capture long-range dependencies in text data. Traditional models like recurrent neural networks (RNNs) struggle with this because they process text sequentially, which can lead to information loss over long distances. Transformers, on the other hand, use self-attention mechanisms to weigh the importance of each word in a sentence, allowing them to capture relationships between words that are far apart.\n",
      "\n",
      "Another benefit of transformers is their scalability. Transformers can be easily scaled up to handle large amounts of data and complex tasks, making them well-suited for a wide range of NLP applications. Additionally, transformers have been pre-trained on massive amounts of text data, which allows them to learn general language patterns and semantics. This pre-training can be fine-tuned on specific tasks, making transformers highly adaptable and versatile for various NLP tasks.\n",
      "\n",
      "Overall, transformers have significantly improved the performance of NLP models by capturing long-range dependencies, being scalable, and having the ability to learn from large amounts of data. Their flexibility and efficiency make them a powerful tool for a wide range of NLP applications.\n",
      "  - expected output: None\n",
      "  - context: None\n",
      "  - retrieval context: ['Transformers enable handling long-range dependencies, parallel processing, and fine-tuning for various NLP tasks.']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Answer Relevancy: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">‚úì</span> Tests finished üéâ! Run <span style=\"color: #008000; text-decoration-color: #008000\">'deepeval login'</span> to save and analyze evaluation results on Confident AI. \n",
       "‚ÄºÔ∏è  Friendly reminder üòá: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
       "instead.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;5;245;141m‚úì\u001b[0m Tests finished üéâ! Run \u001b[32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI. \n",
       "‚ÄºÔ∏è  Friendly reminder üòá: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
       "instead.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GPT-4o Evaluation Results ===\n",
      "\n",
      "Input: Explain the benefits of using transformers in natural language processing.\n",
      "Output: Transformers have revolutionized the field of natural language processing (NLP) due to their unique architecture and capabilities. Here are some of the key benefits of using transformers in NLP:\n",
      "\n",
      "1. Parallelization: Unlike recurrent neural networks (RNNs), transformers do not require sequential data processing, which allows for parallelization during training. This significantly speeds up the training process and makes it feasible to train on large datasets.\n",
      "\n",
      "2. Handling Long-Range Dependencies: Transformers use self-attention mechanisms that allow them to consider the entire context of a sentence or document when making predictions. This is particularly useful for capturing long-range dependencies in text, which is a limitation of traditional RNNs and LSTMs.\n",
      "\n",
      "3. Scalability: The architecture of transformers is highly scalable. Models like BERT, GPT, and T5 have billions of parameters, which allows them to learn complex patterns and representations from vast amounts of data.\n",
      "\n",
      "4. Transfer Learning: Transformers have popularized the use of transfer learning in NLP. Pre-trained models can be fine-tuned on specific tasks with relatively small amounts of data, which reduces the need for large labeled datasets and computational resources.\n",
      "\n",
      "5. State-of-the-Art Performance: Transformers have achieved state-of-the-art results on a wide range of NLP tasks, including machine translation, text summarization, sentiment analysis, and question answering. Their ability to model complex language patterns has set new benchmarks in the field.\n",
      "\n",
      "6. Versatility: The transformer architecture is highly versatile and can be adapted for various tasks beyond NLP, such as image processing and protein folding. This adaptability makes transformers a powerful tool across different domains.\n",
      "\n",
      "7. Robustness to Input Variability: Transformers can handle variable-length input sequences effectively, making them suitable for tasks involving diverse text lengths and structures.\n",
      "\n",
      "8. Improved Contextual Understanding: The self-attention mechanism allows transformers to weigh the importance of different words in a sentence, leading to a better understanding of context and meaning.\n",
      "\n",
      "9. Reduced Vanishing Gradient Problem: By avoiding the sequential processing of data, transformers mitigate the vanishing gradient problem that often plagues RNNs, leading to more stable and effective training.\n",
      "\n",
      "Overall, transformers have become a cornerstone of modern NLP due to their efficiency, effectiveness, and flexibility, enabling significant advancements in the field.\n",
      "\n",
      "Metric: Answer Relevancy\n",
      " - Score: 1.0\n",
      " - Success: ‚úÖ\n",
      " - Reason: The score is 1.00 because the answer perfectly addresses the question about the benefits of using transformers in natural language processing without any irrelevant statements. Great job!\n",
      " - Evaluation Model: gpt-4o\n",
      " - Evaluation Cost: $0.01159\n",
      "==================================================\n",
      "\n",
      "=== GPT-3.5-turbo Evaluation Results ===\n",
      "\n",
      "Input: Explain the benefits of using transformers in natural language processing.\n",
      "Output: Transformers are a type of deep learning model that has revolutionized natural language processing (NLP) tasks. One of the main benefits of using transformers in NLP is their ability to capture long-range dependencies in text data. Traditional models like recurrent neural networks (RNNs) struggle with this because they process text sequentially, which can lead to information loss over long distances. Transformers, on the other hand, use self-attention mechanisms to weigh the importance of each word in a sentence, allowing them to capture relationships between words that are far apart.\n",
      "\n",
      "Another benefit of transformers is their scalability. Transformers can be easily scaled up to handle large amounts of data and complex tasks, making them well-suited for a wide range of NLP applications. Additionally, transformers have been pre-trained on massive amounts of text data, which allows them to learn general language patterns and semantics. This pre-training can be fine-tuned on specific tasks, making transformers highly adaptable and versatile for various NLP tasks.\n",
      "\n",
      "Overall, transformers have significantly improved the performance of NLP models by capturing long-range dependencies, being scalable, and having the ability to learn from large amounts of data. Their flexibility and efficiency make them a powerful tool for a wide range of NLP applications.\n",
      "\n",
      "Metric: Answer Relevancy\n",
      " - Score: 1.0\n",
      " - Success: ‚úÖ\n",
      " - Reason: The score is 1.00 because the output perfectly addresses the input with no irrelevant statements, providing a clear and focused explanation of the benefits of using transformers in natural language processing. Great job!\n",
      " - Evaluation Model: gpt-4o\n",
      " - Evaluation Cost: $0.00769\n",
      "==================================================\n",
      "\n",
      "Evaluation results have been logged to 'evaluation_summary.json'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from deepeval import evaluate\n",
    "from deepeval.metrics import AnswerRelevancyMetric  # Import available metrics only\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Initialize the OpenAI LLMs via LangChain\n",
    "gpt4o_chat = ChatOpenAI(model=\"gpt-4o\", temperature=0, openai_api_key=openai_api_key)\n",
    "gpt35_chat = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "# Define a prompt for the LLM\n",
    "prompt = \"Explain the benefits of using transformers in natural language processing.\"\n",
    "\n",
    "# Retrieve responses from both models\n",
    "gpt4_response = gpt4o_chat.invoke(prompt).content\n",
    "gpt35_response = gpt35_chat.invoke(prompt).content\n",
    "\n",
    "# Clean up any Markdown bold formatting\n",
    "cleaned_gpt4_response = re.sub(r'\\*\\*([^*]+)\\*\\*', r'\\1', gpt4_response)\n",
    "cleaned_gpt35_response = re.sub(r'\\*\\*([^*]+)\\*\\*', r'\\1', gpt35_response)\n",
    "\n",
    "# Define the available metric(s) to evaluate\n",
    "metrics_list = [AnswerRelevancyMetric(threshold=0.5)]\n",
    "\n",
    "# Create test cases for each model's response\n",
    "gpt4_test_case = LLMTestCase(\n",
    "    input=prompt,\n",
    "    actual_output=cleaned_gpt4_response,\n",
    "    retrieval_context=[\"Transformers enable handling long-range dependencies, parallel processing, and fine-tuning for various NLP tasks.\"]\n",
    ")\n",
    "\n",
    "gpt35_test_case = LLMTestCase(\n",
    "    input=prompt,\n",
    "    actual_output=cleaned_gpt35_response,\n",
    "    retrieval_context=[\"Transformers enable handling long-range dependencies, parallel processing, and fine-tuning for various NLP tasks.\"]\n",
    ")\n",
    "\n",
    "# Evaluate both test cases with the metrics list\n",
    "evaluation_results_gpt4 = evaluate([gpt4_test_case], metrics_list)\n",
    "evaluation_results_gpt35 = evaluate([gpt35_test_case], metrics_list)\n",
    "\n",
    "# Function to print evaluation summaries\n",
    "def print_evaluation_summary(model_name, evaluation_results):\n",
    "    print(f\"\\n=== {model_name} Evaluation Results ===\\n\")\n",
    "    for result in evaluation_results.test_results:\n",
    "        print(f\"Input: {result.input}\")\n",
    "        print(f\"Output: {result.actual_output}\\n\")\n",
    "        \n",
    "        for metric_data in result.metrics_data:\n",
    "            print(f\"Metric: {metric_data.name}\")\n",
    "            print(f\" - Score: {metric_data.score}\")\n",
    "            print(f\" - Success: {'‚úÖ' if metric_data.success else '‚ùå'}\")\n",
    "            print(f\" - Reason: {metric_data.reason}\")\n",
    "            print(f\" - Evaluation Model: {metric_data.evaluation_model}\")\n",
    "            print(f\" - Evaluation Cost: ${metric_data.evaluation_cost:.5f}\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "# Print evaluation summaries for both models\n",
    "print_evaluation_summary(\"GPT-4o\", evaluation_results_gpt4)\n",
    "print_evaluation_summary(\"GPT-3.5-turbo\", evaluation_results_gpt35)\n",
    "\n",
    "# Save the results to a JSON file\n",
    "output_data = {\n",
    "    \"gpt-4o\": evaluation_results_gpt4.dict(),\n",
    "    \"gpt-3.5-turbo\": evaluation_results_gpt35.dict()\n",
    "}\n",
    "with open('evaluation_summary.json', 'w') as f:\n",
    "    json.dump(output_data, f, indent=4)\n",
    "\n",
    "print(\"\\nEvaluation results have been logged to 'evaluation_summary.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "386ed0a1-6fce-44eb-8061-981b952dd45e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 1 test case(s) in parallel: |‚ñà|100% (1/1) [Time Taken: 00:09,  9.96s/test"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚úÖ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the explanation was perfectly relevant and concise, with no irrelevant statements. Great job!, error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Explain the benefits of using transformers in natural language processing.\n",
      "  - actual output: Transformers have revolutionized the field of natural language processing (NLP) due to their unique architecture and capabilities. Here are some of the key benefits of using transformers in NLP:\n",
      "\n",
      "1. Parallelization: Unlike recurrent neural networks (RNNs), which process data sequentially, transformers can process entire sequences of data in parallel. This parallelization significantly speeds up training and inference times, making transformers more efficient and scalable for large datasets.\n",
      "\n",
      "2. Attention Mechanism: The self-attention mechanism in transformers allows the model to weigh the importance of different words in a sentence, regardless of their position. This ability to focus on relevant parts of the input sequence helps in capturing long-range dependencies and contextual information more effectively than traditional models.\n",
      "\n",
      "3. Handling Long-Range Dependencies: Transformers are particularly adept at managing long-range dependencies in text, which is a common challenge in NLP tasks. The self-attention mechanism enables the model to consider the entire context of a sentence or document, improving the understanding of complex language structures.\n",
      "\n",
      "4. Transfer Learning: Transformers, especially models like BERT, GPT, and their variants, have popularized the use of transfer learning in NLP. Pre-trained transformer models can be fine-tuned on specific tasks with relatively small amounts of task-specific data, leading to state-of-the-art performance across a wide range of applications.\n",
      "\n",
      "5. Versatility: Transformers are highly versatile and can be applied to various NLP tasks, including text classification, translation, summarization, question answering, and more. Their architecture can be adapted to both supervised and unsupervised learning scenarios.\n",
      "\n",
      "6. State-of-the-Art Performance: Transformers have consistently achieved state-of-the-art results on numerous NLP benchmarks and competitions. Their ability to model complex language patterns and nuances has set new standards for accuracy and performance in the field.\n",
      "\n",
      "7. Reduced Need for Feature Engineering: The ability of transformers to automatically learn and extract features from raw text data reduces the need for extensive feature engineering, which was often required in traditional NLP approaches. This makes it easier to develop and deploy NLP models.\n",
      "\n",
      "8. Robustness to Noise: Transformers tend to be more robust to noise and variations in input data compared to older models. This robustness is partly due to their ability to capture context and meaning more effectively.\n",
      "\n",
      "9. Flexibility in Model Design: The transformer architecture is highly modular, allowing researchers and practitioners to experiment with different configurations, such as varying the number of layers, attention heads, and hidden dimensions, to tailor models to specific tasks and computational constraints.\n",
      "\n",
      "Overall, transformers have become the backbone of modern NLP due to their efficiency, flexibility, and superior performance across a wide range of language tasks.\n",
      "  - expected output: None\n",
      "  - context: None\n",
      "  - retrieval context: ['Transformers enable handling long-range dependencies, parallel processing, and fine-tuning for various NLP tasks.']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Answer Relevancy: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">‚úì</span> Tests finished üéâ! Run <span style=\"color: #008000; text-decoration-color: #008000\">'deepeval login'</span> to save and analyze evaluation results on Confident AI. \n",
       "‚ÄºÔ∏è  Friendly reminder üòá: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
       "instead.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;5;245;141m‚úì\u001b[0m Tests finished üéâ! Run \u001b[32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI. \n",
       "‚ÄºÔ∏è  Friendly reminder üòá: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
       "instead.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 1 test case(s) in parallel: |‚ñà|100% (1/1) [Time Taken: 00:06,  6.26s/test"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚úÖ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the answer is perfectly relevant with no irrelevant statements. Great job!, error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Explain the benefits of using transformers in natural language processing.\n",
      "  - actual output: Transformers are a type of deep learning model that has revolutionized natural language processing (NLP) tasks. One of the main benefits of using transformers in NLP is their ability to capture long-range dependencies in text data. This means that transformers can understand the context of a word or phrase in relation to the entire sentence or document, rather than just the words immediately surrounding it.\n",
      "\n",
      "Additionally, transformers are highly parallelizable, which allows for faster training and inference times compared to traditional sequential models. This makes them well-suited for processing large amounts of text data efficiently.\n",
      "\n",
      "Another benefit of transformers is their ability to handle variable-length input sequences, which is common in NLP tasks. This flexibility allows transformers to effectively process text data of different lengths without the need for padding or truncation.\n",
      "\n",
      "Furthermore, transformers have shown state-of-the-art performance on a wide range of NLP tasks, including language modeling, machine translation, sentiment analysis, and question answering. Their ability to learn complex patterns and relationships in text data has made them a popular choice for NLP researchers and practitioners.\n",
      "\n",
      "Overall, the benefits of using transformers in NLP include their ability to capture long-range dependencies, parallel processing capabilities, flexibility with variable-length input sequences, and superior performance on a variety of NLP tasks.\n",
      "  - expected output: None\n",
      "  - context: None\n",
      "  - retrieval context: ['Transformers enable handling long-range dependencies, parallel processing, and fine-tuning for various NLP tasks.']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Answer Relevancy: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">‚úì</span> Tests finished üéâ! Run <span style=\"color: #008000; text-decoration-color: #008000\">'deepeval login'</span> to save and analyze evaluation results on Confident AI. \n",
       "‚ÄºÔ∏è  Friendly reminder üòá: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
       "instead.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;5;245;141m‚úì\u001b[0m Tests finished üéâ! Run \u001b[32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI. \n",
       "‚ÄºÔ∏è  Friendly reminder üòá: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
       "instead.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GPT-4o Evaluation Results ===\n",
      "\n",
      "Input: Explain the benefits of using transformers in natural language processing.\n",
      "Output: Transformers have revolutionized the field of natural language processing (NLP) due to their unique architecture and capabilities. Here are some of the key benefits of using transformers in NLP:\n",
      "\n",
      "1. Parallelization: Unlike recurrent neural networks (RNNs), which process data sequentially, transformers can process entire sequences of data in parallel. This parallelization significantly speeds up training and inference times, making transformers more efficient and scalable for large datasets.\n",
      "\n",
      "2. Attention Mechanism: The self-attention mechanism in transformers allows the model to weigh the importance of different words in a sentence, regardless of their position. This ability to focus on relevant parts of the input sequence helps in capturing long-range dependencies and contextual information more effectively than traditional models.\n",
      "\n",
      "3. Handling Long-Range Dependencies: Transformers are particularly adept at managing long-range dependencies in text, which is a common challenge in NLP tasks. The self-attention mechanism enables the model to consider the entire context of a sentence or document, improving the understanding of complex language structures.\n",
      "\n",
      "4. Transfer Learning: Transformers, especially models like BERT, GPT, and their variants, have popularized the use of transfer learning in NLP. Pre-trained transformer models can be fine-tuned on specific tasks with relatively small amounts of task-specific data, leading to state-of-the-art performance across a wide range of applications.\n",
      "\n",
      "5. Versatility: Transformers are highly versatile and can be applied to various NLP tasks, including text classification, translation, summarization, question answering, and more. Their architecture can be adapted to both supervised and unsupervised learning scenarios.\n",
      "\n",
      "6. State-of-the-Art Performance: Transformers have consistently achieved state-of-the-art results on numerous NLP benchmarks and competitions. Their ability to model complex language patterns and nuances has set new standards for accuracy and performance in the field.\n",
      "\n",
      "7. Reduced Need for Feature Engineering: The ability of transformers to automatically learn and extract features from raw text data reduces the need for extensive feature engineering, which was often required in traditional NLP approaches. This makes it easier to develop and deploy NLP models.\n",
      "\n",
      "8. Robustness to Noise: Transformers tend to be more robust to noise and variations in input data compared to older models. This robustness is partly due to their ability to capture context and meaning more effectively.\n",
      "\n",
      "9. Flexibility in Model Design: The transformer architecture is highly modular, allowing researchers and practitioners to experiment with different configurations, such as varying the number of layers, attention heads, and hidden dimensions, to tailor models to specific tasks and computational constraints.\n",
      "\n",
      "Overall, transformers have become the backbone of modern NLP due to their efficiency, flexibility, and superior performance across a wide range of language tasks.\n",
      "\n",
      "Metric: Answer Relevancy\n",
      " - Score: 1.0\n",
      " - Success: ‚úÖ\n",
      " - Reason: The score is 1.00 because the explanation was perfectly relevant and concise, with no irrelevant statements. Great job!\n",
      " - Evaluation Model: gpt-4o\n",
      " - Evaluation Cost: $0.01287\n",
      "==================================================\n",
      "\n",
      "=== GPT-3.5-turbo Evaluation Results ===\n",
      "\n",
      "Input: Explain the benefits of using transformers in natural language processing.\n",
      "Output: Transformers are a type of deep learning model that has revolutionized natural language processing (NLP) tasks. One of the main benefits of using transformers in NLP is their ability to capture long-range dependencies in text data. This means that transformers can understand the context of a word or phrase in relation to the entire sentence or document, rather than just the words immediately surrounding it.\n",
      "\n",
      "Additionally, transformers are highly parallelizable, which allows for faster training and inference times compared to traditional sequential models. This makes them well-suited for processing large amounts of text data efficiently.\n",
      "\n",
      "Another benefit of transformers is their ability to handle variable-length input sequences, which is common in NLP tasks. This flexibility allows transformers to effectively process text data of different lengths without the need for padding or truncation.\n",
      "\n",
      "Furthermore, transformers have shown state-of-the-art performance on a wide range of NLP tasks, including language modeling, machine translation, sentiment analysis, and question answering. Their ability to learn complex patterns and relationships in text data has made them a popular choice for NLP researchers and practitioners.\n",
      "\n",
      "Overall, the benefits of using transformers in NLP include their ability to capture long-range dependencies, parallel processing capabilities, flexibility with variable-length input sequences, and superior performance on a variety of NLP tasks.\n",
      "\n",
      "Metric: Answer Relevancy\n",
      " - Score: 1.0\n",
      " - Success: ‚úÖ\n",
      " - Reason: The score is 1.00 because the answer is perfectly relevant with no irrelevant statements. Great job!\n",
      " - Evaluation Model: gpt-4o\n",
      " - Evaluation Cost: $0.00755\n",
      "==================================================\n",
      "\n",
      "Evaluation results have been logged to 'evaluation_summary.json'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from deepeval import evaluate\n",
    "from deepeval.metrics import AnswerRelevancyMetric  # Import any available metrics one-by-one\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Initialize the OpenAI LLMs via LangChain\n",
    "gpt4o_chat = ChatOpenAI(model=\"gpt-4o\", temperature=0, openai_api_key=openai_api_key)\n",
    "gpt35_chat = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "# Define a prompt for the LLM\n",
    "prompt = \"Explain the benefits of using transformers in natural language processing.\"\n",
    "\n",
    "# Retrieve responses from both models\n",
    "gpt4_response = gpt4o_chat.invoke(prompt).content\n",
    "gpt35_response = gpt35_chat.invoke(prompt).content\n",
    "\n",
    "# Clean up any Markdown bold formatting\n",
    "cleaned_gpt4_response = re.sub(r'\\*\\*([^*]+)\\*\\*', r'\\1', gpt4_response)\n",
    "cleaned_gpt35_response = re.sub(r'\\*\\*([^*]+)\\*\\*', r'\\1', gpt35_response)\n",
    "\n",
    "# Initialize the list of metrics, attempting to import and append other metrics if available\n",
    "metrics_list = [AnswerRelevancyMetric(threshold=0.5)]\n",
    "\n",
    "# Create test cases for each model's response\n",
    "gpt4_test_case = LLMTestCase(\n",
    "    input=prompt,\n",
    "    actual_output=cleaned_gpt4_response,\n",
    "    retrieval_context=[\"Transformers enable handling long-range dependencies, parallel processing, and fine-tuning for various NLP tasks.\"]\n",
    ")\n",
    "\n",
    "gpt35_test_case = LLMTestCase(\n",
    "    input=prompt,\n",
    "    actual_output=cleaned_gpt35_response,\n",
    "    retrieval_context=[\"Transformers enable handling long-range dependencies, parallel processing, and fine-tuning for various NLP tasks.\"]\n",
    ")\n",
    "\n",
    "# Evaluate both test cases with the metrics list\n",
    "evaluation_results_gpt4 = evaluate([gpt4_test_case], metrics_list)\n",
    "evaluation_results_gpt35 = evaluate([gpt35_test_case], metrics_list)\n",
    "\n",
    "# Function to print evaluation summaries\n",
    "def print_evaluation_summary(model_name, evaluation_results):\n",
    "    print(f\"\\n=== {model_name} Evaluation Results ===\\n\")\n",
    "    for result in evaluation_results.test_results:\n",
    "        print(f\"Input: {result.input}\")\n",
    "        print(f\"Output: {result.actual_output}\\n\")\n",
    "        \n",
    "        for metric_data in result.metrics_data:\n",
    "            print(f\"Metric: {metric_data.name}\")\n",
    "            print(f\" - Score: {metric_data.score}\")\n",
    "            print(f\" - Success: {'‚úÖ' if metric_data.success else '‚ùå'}\")\n",
    "            print(f\" - Reason: {metric_data.reason}\")\n",
    "            print(f\" - Evaluation Model: {metric_data.evaluation_model}\")\n",
    "            print(f\" - Evaluation Cost: ${metric_data.evaluation_cost:.5f}\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "# Print evaluation summaries for both models\n",
    "print_evaluation_summary(\"GPT-4o\", evaluation_results_gpt4)\n",
    "print_evaluation_summary(\"GPT-3.5-turbo\", evaluation_results_gpt35)\n",
    "\n",
    "# Save the results to a JSON file\n",
    "output_data = {\n",
    "    \"gpt-4o\": evaluation_results_gpt4.dict(),\n",
    "    \"gpt-3.5-turbo\": evaluation_results_gpt35.dict()\n",
    "}\n",
    "with open('evaluation_summary.json', 'w') as f:\n",
    "    json.dump(output_data, f, indent=4)\n",
    "\n",
    "print(\"\\nEvaluation results have been logged to 'evaluation_summary.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6c498090-3b80-4dd1-ab28-cb4c17dfdc16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Contextual Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mContextual Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Hallucination Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mHallucination Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Faithfulness Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mFaithfulness Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 1 test case(s) in parallel: | |  0% (0/1) [Time Taken: 00:00, ?test case/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 1 test case(s) in parallel: |‚ñà|100% (1/1) [Time Taken: 00:11, 11.53s/test"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚úÖ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response was perfectly relevant and addressed the question thoroughly with no irrelevant statements. Great job!, error: None)\n",
      "  - ‚úÖ Contextual Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the relevant statement perfectly aligns with the input, highlighting key benefits of transformers in NLP like handling long-range dependencies, parallel processing, and fine-tuning. Great job!, error: None)\n",
      "  - ‚úÖ Hallucination (score: 0.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because there are no factual alignments or contradictions found, indicating perfect alignment between the actual output and the context., error: None)\n",
      "  - ‚úÖ Faithfulness (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because there are no contradictions, indicating that the actual output is perfectly aligned with the retrieval context. Great job!, error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Explain the benefits of using transformers in natural language processing.\n",
      "  - actual output: Transformers have revolutionized the field of natural language processing (NLP) due to several key benefits:\n",
      "\n",
      "1. Parallelization: Unlike recurrent neural networks (RNNs), which process data sequentially, transformers allow for parallel processing of data. This is achieved through the self-attention mechanism, which enables the model to consider all words in a sentence simultaneously. This parallelization significantly speeds up training and inference times.\n",
      "\n",
      "2. Handling Long-Range Dependencies: Transformers are particularly effective at capturing long-range dependencies in text. The self-attention mechanism allows the model to weigh the importance of different words in a sentence, regardless of their position, which is crucial for understanding context and meaning in complex sentences.\n",
      "\n",
      "3. Scalability: Transformers can be scaled up to create very large models, such as GPT-3 and BERT, which have demonstrated state-of-the-art performance on a wide range of NLP tasks. The architecture is well-suited to benefit from increased data and computational resources.\n",
      "\n",
      "4. Transfer Learning: Pre-trained transformer models can be fine-tuned on specific tasks with relatively small amounts of task-specific data. This transfer learning capability allows for efficient adaptation to various NLP tasks, such as sentiment analysis, translation, and question answering, often with minimal additional training.\n",
      "\n",
      "5. Versatility: Transformers are highly versatile and have been successfully applied to a variety of tasks beyond NLP, including image processing and protein folding. This adaptability is largely due to the model's ability to learn complex patterns and relationships in data.\n",
      "\n",
      "6. Improved Contextual Understanding: Transformers provide a better understanding of context by using mechanisms like attention to focus on relevant parts of the input. This leads to more accurate and nuanced language understanding, which is crucial for tasks like machine translation and summarization.\n",
      "\n",
      "7. Reduced Need for Feature Engineering: Traditional NLP models often required extensive feature engineering to capture linguistic nuances. Transformers, on the other hand, can automatically learn relevant features from raw text data, reducing the need for manual intervention and domain expertise.\n",
      "\n",
      "8. Robustness to Input Variability: The attention mechanism in transformers allows them to be more robust to variations in input, such as changes in word order or the presence of irrelevant information, making them more reliable in real-world applications.\n",
      "\n",
      "Overall, the transformer architecture has set new benchmarks in NLP by providing a powerful, flexible, and efficient framework for understanding and generating human language.\n",
      "  - expected output: None\n",
      "  - context: ['Transformers enable handling long-range dependencies, parallel processing, and fine-tuning for various NLP tasks.']\n",
      "  - retrieval context: ['Transformers enable handling long-range dependencies, parallel processing, and fine-tuning for various NLP tasks.']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Answer Relevancy: 100.00% pass rate\n",
      "Contextual Relevancy: 100.00% pass rate\n",
      "Hallucination: 100.00% pass rate\n",
      "Faithfulness: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">‚úì</span> Tests finished üéâ! Run <span style=\"color: #008000; text-decoration-color: #008000\">'deepeval login'</span> to save and analyze evaluation results on Confident AI. \n",
       "‚ÄºÔ∏è  Friendly reminder üòá: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
       "instead.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;5;245;141m‚úì\u001b[0m Tests finished üéâ! Run \u001b[32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI. \n",
       "‚ÄºÔ∏è  Friendly reminder üòá: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
       "instead.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Contextual Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mContextual Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Hallucination Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mHallucination Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Faithfulness Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mFaithfulness Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 1 test case(s) in parallel: | |  0% (0/1) [Time Taken: 00:00, ?test case/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 1 test case(s) in parallel: |‚ñà|100% (1/1) [Time Taken: 00:06,  6.98s/test"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚úÖ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the output is perfectly relevant and directly addresses the benefits of using transformers in natural language processing with no irrelevant information. Great job!, error: None)\n",
      "  - ‚úÖ Contextual Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the relevant statement perfectly aligns with the input, highlighting key benefits of transformers in NLP. Great job!, error: None)\n",
      "  - ‚úÖ Hallucination (score: 0.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because there are no factual alignments present, and the listed contradiction indicates that the actual output does not introduce any false information or discrepancies with the context, suggesting complete factual consistency., error: None)\n",
      "  - ‚úÖ Faithfulness (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because there are no contradictions, indicating a perfectly faithful alignment with the retrieval context. Great job!, error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Explain the benefits of using transformers in natural language processing.\n",
      "  - actual output: Transformers are a type of deep learning model that has revolutionized natural language processing (NLP) tasks. One of the main benefits of using transformers in NLP is their ability to capture long-range dependencies in text data. Traditional models like recurrent neural networks struggle with this because they process text sequentially, which can lead to information loss over long distances. Transformers, on the other hand, use self-attention mechanisms to weigh the importance of each word in a sentence, allowing them to capture relationships between words that are far apart.\n",
      "\n",
      "Another benefit of transformers is their scalability. They can be easily scaled up to handle large amounts of data and complex tasks, making them suitable for a wide range of NLP applications. Additionally, transformers are highly parallelizable, which means they can be trained efficiently on powerful hardware like GPUs and TPUs.\n",
      "\n",
      "Furthermore, transformers have been shown to outperform traditional models on a variety of NLP tasks, including machine translation, text generation, and sentiment analysis. Their ability to learn complex patterns in text data and generalize well to new tasks makes them a valuable tool for researchers and practitioners in the field of NLP.\n",
      "\n",
      "Overall, the benefits of using transformers in NLP include their ability to capture long-range dependencies, scalability, parallelizability, and superior performance on a wide range of tasks. These advantages have made transformers the go-to model for many NLP applications and have significantly advanced the field of natural language processing.\n",
      "  - expected output: None\n",
      "  - context: ['Transformers enable handling long-range dependencies, parallel processing, and fine-tuning for various NLP tasks.']\n",
      "  - retrieval context: ['Transformers enable handling long-range dependencies, parallel processing, and fine-tuning for various NLP tasks.']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Answer Relevancy: 100.00% pass rate\n",
      "Contextual Relevancy: 100.00% pass rate\n",
      "Hallucination: 100.00% pass rate\n",
      "Faithfulness: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">‚úì</span> Tests finished üéâ! Run <span style=\"color: #008000; text-decoration-color: #008000\">'deepeval login'</span> to save and analyze evaluation results on Confident AI. \n",
       "‚ÄºÔ∏è  Friendly reminder üòá: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
       "instead.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;5;245;141m‚úì\u001b[0m Tests finished üéâ! Run \u001b[32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI. \n",
       "‚ÄºÔ∏è  Friendly reminder üòá: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
       "instead.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GPT-4o Evaluation Results ===\n",
      "\n",
      "Model: GPT-4o\n",
      "\n",
      "Input: Explain the benefits of using transformers in natural language processing.\n",
      "\n",
      "Metrics Summary:\n",
      " - Answer Relevancy: Score = 1.0, Success = ‚úÖ\n",
      "   Reason: The score is 1.00 because the response was perfectly relevant and addressed the question thoroughly with no irrelevant statements. Great job!\n",
      " - Contextual Relevancy: Score = 1.0, Success = ‚úÖ\n",
      "   Reason: The score is 1.00 because the relevant statement perfectly aligns with the input, highlighting key benefits of transformers in NLP like handling long-range dependencies, parallel processing, and fine-tuning. Great job!\n",
      " - Hallucination: Score = 0.0, Success = ‚úÖ\n",
      "   Reason: The score is 0.00 because there are no factual alignments or contradictions found, indicating perfect alignment between the actual output and the context.\n",
      " - Faithfulness: Score = 1.0, Success = ‚úÖ\n",
      "   Reason: The score is 1.00 because there are no contradictions, indicating that the actual output is perfectly aligned with the retrieval context. Great job!\n",
      "==================================================\n",
      "\n",
      "Output from GPT-4o:\n",
      "Transformers have revolutionized the field of natural language processing (NLP) due to several key benefits:\n",
      "\n",
      "1. Parallelization: Unlike recurrent neural networks (RNNs), which process data sequentially, transformers allow for parallel processing of data. This is achieved through the self-attention mechanism, which enables the model to consider all words in a sentence simultaneously. This parallelization significantly speeds up training and inference times.\n",
      "\n",
      "2. Handling Long-Range Dependencies: Transformers are particularly effective at capturing long-range dependencies in text. The self-attention mechanism allows the model to weigh the importance of different words in a sentence, regardless of their position, which is crucial for understanding context and meaning in complex sentences.\n",
      "\n",
      "3. Scalability: Transformers can be scaled up to create very large models, such as GPT-3 and BERT, which have demonstrated state-of-the-art performance on a wide range of NLP tasks. The architecture is well-suited to benefit from increased data and computational resources.\n",
      "\n",
      "4. Transfer Learning: Pre-trained transformer models can be fine-tuned on specific tasks with relatively small amounts of task-specific data. This transfer learning capability allows for efficient adaptation to various NLP tasks, such as sentiment analysis, translation, and question answering, often with minimal additional training.\n",
      "\n",
      "5. Versatility: Transformers are highly versatile and have been successfully applied to a variety of tasks beyond NLP, including image processing and protein folding. This adaptability is largely due to the model's ability to learn complex patterns and relationships in data.\n",
      "\n",
      "6. Improved Contextual Understanding: Transformers provide a better understanding of context by using mechanisms like attention to focus on relevant parts of the input. This leads to more accurate and nuanced language understanding, which is crucial for tasks like machine translation and summarization.\n",
      "\n",
      "7. Reduced Need for Feature Engineering: Traditional NLP models often required extensive feature engineering to capture linguistic nuances. Transformers, on the other hand, can automatically learn relevant features from raw text data, reducing the need for manual intervention and domain expertise.\n",
      "\n",
      "8. Robustness to Input Variability: The attention mechanism in transformers allows them to be more robust to variations in input, such as changes in word order or the presence of irrelevant information, making them more reliable in real-world applications.\n",
      "\n",
      "Overall, the transformer architecture has set new benchmarks in NLP by providing a powerful, flexible, and efficient framework for understanding and generating human language.\n",
      "\n",
      "==================================================\n",
      "\n",
      "=== GPT-3.5-turbo Evaluation Results ===\n",
      "\n",
      "Model: GPT-3.5-turbo\n",
      "\n",
      "Input: Explain the benefits of using transformers in natural language processing.\n",
      "\n",
      "Metrics Summary:\n",
      " - Answer Relevancy: Score = 1.0, Success = ‚úÖ\n",
      "   Reason: The score is 1.00 because the output is perfectly relevant and directly addresses the benefits of using transformers in natural language processing with no irrelevant information. Great job!\n",
      " - Contextual Relevancy: Score = 1.0, Success = ‚úÖ\n",
      "   Reason: The score is 1.00 because the relevant statement perfectly aligns with the input, highlighting key benefits of transformers in NLP. Great job!\n",
      " - Hallucination: Score = 0.0, Success = ‚úÖ\n",
      "   Reason: The score is 0.00 because there are no factual alignments present, and the listed contradiction indicates that the actual output does not introduce any false information or discrepancies with the context, suggesting complete factual consistency.\n",
      " - Faithfulness: Score = 1.0, Success = ‚úÖ\n",
      "   Reason: The score is 1.00 because there are no contradictions, indicating a perfectly faithful alignment with the retrieval context. Great job!\n",
      "==================================================\n",
      "\n",
      "Output from GPT-3.5-turbo:\n",
      "Transformers are a type of deep learning model that has revolutionized natural language processing (NLP) tasks. One of the main benefits of using transformers in NLP is their ability to capture long-range dependencies in text data. Traditional models like recurrent neural networks struggle with this because they process text sequentially, which can lead to information loss over long distances. Transformers, on the other hand, use self-attention mechanisms to weigh the importance of each word in a sentence, allowing them to capture relationships between words that are far apart.\n",
      "\n",
      "Another benefit of transformers is their scalability. They can be easily scaled up to handle large amounts of data and complex tasks, making them suitable for a wide range of NLP applications. Additionally, transformers are highly parallelizable, which means they can be trained efficiently on powerful hardware like GPUs and TPUs.\n",
      "\n",
      "Furthermore, transformers have been shown to outperform traditional models on a variety of NLP tasks, including machine translation, text generation, and sentiment analysis. Their ability to learn complex patterns in text data and generalize well to new tasks makes them a valuable tool for researchers and practitioners in the field of NLP.\n",
      "\n",
      "Overall, the benefits of using transformers in NLP include their ability to capture long-range dependencies, scalability, parallelizability, and superior performance on a wide range of tasks. These advantages have made transformers the go-to model for many NLP applications and have significantly advanced the field of natural language processing.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Evaluation results have been logged to 'evaluation_summary.json'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from deepeval import evaluate\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Attempt to import all relevant metrics and initialize them with the required parameters\n",
    "available_metrics = []\n",
    "metric_import_errors = []\n",
    "\n",
    "try:\n",
    "    from deepeval.metrics import AnswerRelevancyMetric\n",
    "    available_metrics.append(AnswerRelevancyMetric(threshold=0.5))\n",
    "except ImportError as e:\n",
    "    metric_import_errors.append(f\"AnswerRelevancyMetric: {e}\")\n",
    "\n",
    "try:\n",
    "    from deepeval.metrics import ContextualRelevancyMetric\n",
    "    available_metrics.append(ContextualRelevancyMetric(threshold=0.5))\n",
    "except ImportError as e:\n",
    "    metric_import_errors.append(f\"ContextualRelevancyMetric: {e}\")\n",
    "\n",
    "try:\n",
    "    from deepeval.metrics import HallucinationMetric\n",
    "    available_metrics.append(HallucinationMetric(threshold=0.5))\n",
    "except ImportError as e:\n",
    "    metric_import_errors.append(f\"HallucinationMetric: {e}\")\n",
    "\n",
    "try:\n",
    "    from deepeval.metrics import FaithfulnessMetric\n",
    "    available_metrics.append(FaithfulnessMetric(threshold=0.5))\n",
    "except ImportError as e:\n",
    "    metric_import_errors.append(f\"FaithfulnessMetric: {e}\")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Initialize the OpenAI LLMs via LangChain\n",
    "gpt4o_chat = ChatOpenAI(model=\"gpt-4o\", temperature=0, openai_api_key=openai_api_key)\n",
    "gpt35_chat = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "# Define a prompt for the LLM and the context it will be evaluated against\n",
    "prompt = \"Explain the benefits of using transformers in natural language processing.\"\n",
    "context = \"Transformers enable handling long-range dependencies, parallel processing, and fine-tuning for various NLP tasks.\"\n",
    "\n",
    "# Retrieve responses from both models\n",
    "gpt4_response = gpt4o_chat.invoke(prompt).content\n",
    "gpt35_response = gpt35_chat.invoke(prompt).content\n",
    "\n",
    "# Clean up any Markdown bold formatting\n",
    "cleaned_gpt4_response = re.sub(r'\\*\\*([^*]+)\\*\\*', r'\\1', gpt4_response)\n",
    "cleaned_gpt35_response = re.sub(r'\\*\\*([^*]+)\\*\\*', r'\\1', gpt35_response)\n",
    "\n",
    "# Create test cases for each model's response with the required context for hallucination metrics\n",
    "gpt4_test_case = LLMTestCase(\n",
    "    input=prompt,\n",
    "    actual_output=cleaned_gpt4_response,\n",
    "    retrieval_context=[context],  # This remains a list of strings for retrieval-based metrics\n",
    "    context=[context]  # Wrapping the context string in a list\n",
    ")\n",
    "\n",
    "gpt35_test_case = LLMTestCase(\n",
    "    input=prompt,\n",
    "    actual_output=cleaned_gpt35_response,\n",
    "    retrieval_context=[context],\n",
    "    context=[context]\n",
    ")\n",
    "\n",
    "\n",
    "# Evaluate both test cases\n",
    "evaluation_results_gpt4 = evaluate([gpt4_test_case], available_metrics)\n",
    "evaluation_results_gpt35 = evaluate([gpt35_test_case], available_metrics)\n",
    "\n",
    "def print_evaluation_summary(model_name, evaluation_results):\n",
    "    print(f\"\\n=== {model_name} Evaluation Results ===\\n\")\n",
    "    \n",
    "    # Loop over each test case result\n",
    "    for result in evaluation_results.test_results:\n",
    "        print(f\"Model: {model_name}\\n\")\n",
    "        print(f\"Input: {result.input}\\n\")\n",
    "        print(\"Metrics Summary:\")\n",
    "\n",
    "        # Loop over each metric data and print only essential info\n",
    "        for metric_data in result.metrics_data:\n",
    "            print(f\" - {metric_data.name}: Score = {metric_data.score}, Success = {'‚úÖ' if metric_data.success else '‚ùå'}\")\n",
    "            print(f\"   Reason: {metric_data.reason}\")\n",
    "        \n",
    "        print(\"=\" * 50)\n",
    "        print(f\"\\nOutput from {model_name}:\\n{result.actual_output}\\n\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "# Run the evaluation summaries for both models\n",
    "print_evaluation_summary(\"GPT-4o\", evaluation_results_gpt4)\n",
    "print_evaluation_summary(\"GPT-3.5-turbo\", evaluation_results_gpt35)\n",
    "\n",
    "# Log any metric import errors\n",
    "if metric_import_errors:\n",
    "    print(\"\\nMetric Import Errors:\")\n",
    "    for error in metric_import_errors:\n",
    "        print(f\" - {error}\")\n",
    "\n",
    "# Save the results to a JSON file\n",
    "output_data = {\n",
    "    \"gpt-4o\": evaluation_results_gpt4.dict(),\n",
    "    \"gpt-3.5-turbo\": evaluation_results_gpt35.dict()\n",
    "}\n",
    "with open('evaluation_summary.json', 'w') as f:\n",
    "    json.dump(output_data, f, indent=4)\n",
    "\n",
    "print(\"\\nEvaluation results have been logged to 'evaluation_summary.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "83db58ed-4035-4af3-8a2f-c799983dd0e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Contextual Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mContextual Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Hallucination Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mHallucination Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Faithfulness Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mFaithfulness Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 1 test case(s) in parallel: | |  0% (0/1) [Time Taken: 00:00, ?test case/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 1 test case(s) in parallel: |‚ñà|100% (1/1) [Time Taken: 00:11, 11.94s/test"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚úÖ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the output perfectly addresses the benefits of using transformers in natural language processing without any irrelevant information. Great job!, error: None)\n",
      "  - ‚úÖ Contextual Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the retrieval context perfectly addresses the input by highlighting how transformers handle long-range dependencies, enable parallel processing, and are adaptable for various NLP tasks. Great job!, error: None)\n",
      "  - ‚úÖ Hallucination (score: 0.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because there are no factual alignments and the contradictions indicate that the actual output aligns well with the context, suggesting no hallucination., error: None)\n",
      "  - ‚úÖ Faithfulness (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because there are no contradictions, indicating that the actual output is perfectly aligned with the retrieval context. Great job!, error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Explain the benefits of using transformers in natural language processing.\n",
      "  - actual output: Transformers have revolutionized the field of natural language processing (NLP) due to their unique architecture and capabilities. Here are some of the key benefits of using transformers in NLP:\n",
      "\n",
      "1. Parallelization: Unlike recurrent neural networks (RNNs), transformers do not require sequential data processing, which allows for parallelization during training. This significantly speeds up the training process and makes it feasible to train on large datasets.\n",
      "\n",
      "2. Handling Long-Range Dependencies: Transformers use self-attention mechanisms that allow them to consider the entire context of a sentence or document when making predictions. This is particularly useful for capturing long-range dependencies in text, which is a limitation of traditional RNNs and LSTMs.\n",
      "\n",
      "3. Scalability: The architecture of transformers is highly scalable. Models like BERT, GPT, and T5 have billions of parameters, which allows them to learn complex patterns and representations from vast amounts of data.\n",
      "\n",
      "4. Transfer Learning: Transformers have popularized the use of transfer learning in NLP. Pre-trained models can be fine-tuned on specific tasks with relatively small amounts of data, which reduces the need for large labeled datasets and computational resources.\n",
      "\n",
      "5. State-of-the-Art Performance: Transformers have achieved state-of-the-art results on a wide range of NLP tasks, including machine translation, text summarization, sentiment analysis, and question answering. Their ability to model complex language patterns has set new benchmarks in the field.\n",
      "\n",
      "6. Versatility: The transformer architecture is highly versatile and can be adapted for various tasks beyond NLP, such as image processing and protein folding. This adaptability makes it a powerful tool across different domains.\n",
      "\n",
      "7. Robustness to Input Variability: Transformers can handle variable-length input sequences and are less sensitive to the order of input data compared to RNNs. This makes them robust to different types of input variability.\n",
      "\n",
      "8. Improved Contextual Understanding: By leveraging mechanisms like self-attention, transformers can better understand the context and nuances of language, leading to more accurate and contextually relevant outputs.\n",
      "\n",
      "9. Reduced Need for Feature Engineering: The ability of transformers to learn complex representations directly from raw text reduces the need for extensive feature engineering, simplifying the model development process.\n",
      "\n",
      "Overall, the transformer architecture has become a cornerstone of modern NLP due to its efficiency, effectiveness, and flexibility, enabling significant advancements in the field.\n",
      "  - expected output: None\n",
      "  - context: ['Transformers enable handling long-range dependencies, parallel processing, and fine-tuning for various NLP tasks.']\n",
      "  - retrieval context: ['Transformers enable handling long-range dependencies, parallel processing, and fine-tuning for various NLP tasks.']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Answer Relevancy: 100.00% pass rate\n",
      "Contextual Relevancy: 100.00% pass rate\n",
      "Hallucination: 100.00% pass rate\n",
      "Faithfulness: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">‚úì</span> Tests finished üéâ! Run <span style=\"color: #008000; text-decoration-color: #008000\">'deepeval login'</span> to save and analyze evaluation results on Confident AI. \n",
       "‚ÄºÔ∏è  Friendly reminder üòá: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
       "instead.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;5;245;141m‚úì\u001b[0m Tests finished üéâ! Run \u001b[32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI. \n",
       "‚ÄºÔ∏è  Friendly reminder üòá: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
       "instead.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Contextual Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mContextual Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Hallucination Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mHallucination Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Faithfulness Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mFaithfulness Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 1 test case(s) in parallel: | |  0% (0/1) [Time Taken: 00:00, ?test case/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 1 test case(s) in parallel: |‚ñà|100% (1/1) [Time Taken: 00:09,  9.13s/test"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚úÖ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the output is fully relevant and effectively addresses the benefits of using transformers in natural language processing. Great job!, error: None)\n",
      "  - ‚úÖ Contextual Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the context perfectly aligns with the input, highlighting the benefits of transformers in NLP such as handling long-range dependencies and parallel processing. Great job!, error: None)\n",
      "  - ‚úÖ Hallucination (score: 0.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because there are no factual alignments or explicit contradictions identified, indicating that the actual output aligns well with the context provided., error: None)\n",
      "  - ‚úÖ Faithfulness (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the actual output perfectly aligns with the retrieval context, showing no contradictions. Great job!, error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Explain the benefits of using transformers in natural language processing.\n",
      "  - actual output: Transformers are a type of deep learning model that has revolutionized natural language processing (NLP) tasks. One of the main benefits of using transformers in NLP is their ability to capture long-range dependencies in text data. Traditional models like recurrent neural networks struggle with this because they process text sequentially, which can lead to information loss over long distances. Transformers, on the other hand, use self-attention mechanisms to weigh the importance of each word in a sentence, allowing them to capture relationships between words that are far apart.\n",
      "\n",
      "Another benefit of transformers is their scalability. They can be easily scaled up to handle large amounts of data and complex tasks, making them suitable for a wide range of NLP applications. Additionally, transformers have been shown to outperform traditional models on various NLP benchmarks, achieving state-of-the-art results in tasks such as machine translation, text generation, and sentiment analysis.\n",
      "\n",
      "Overall, transformers have significantly advanced the field of NLP by providing more accurate and efficient models for processing and understanding natural language text. Their ability to capture long-range dependencies, scalability, and superior performance make them a valuable tool for researchers and practitioners working in the field of NLP.\n",
      "  - expected output: None\n",
      "  - context: ['Transformers enable handling long-range dependencies, parallel processing, and fine-tuning for various NLP tasks.']\n",
      "  - retrieval context: ['Transformers enable handling long-range dependencies, parallel processing, and fine-tuning for various NLP tasks.']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Answer Relevancy: 100.00% pass rate\n",
      "Contextual Relevancy: 100.00% pass rate\n",
      "Hallucination: 100.00% pass rate\n",
      "Faithfulness: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">‚úì</span> Tests finished üéâ! Run <span style=\"color: #008000; text-decoration-color: #008000\">'deepeval login'</span> to save and analyze evaluation results on Confident AI. \n",
       "‚ÄºÔ∏è  Friendly reminder üòá: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
       "instead.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;5;245;141m‚úì\u001b[0m Tests finished üéâ! Run \u001b[32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI. \n",
       "‚ÄºÔ∏è  Friendly reminder üòá: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
       "instead.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GPT-4o Evaluation Results ===\n",
      "\n",
      "Metrics Summary:\n",
      " - Answer Relevancy: Score = 1.0, Success = ‚úÖ\n",
      "   Reason: The score is 1.00 because the output perfectly addresses the benefits of using transformers in natural language processing without any irrelevant information. Great job!\n",
      " - Contextual Relevancy: Score = 1.0, Success = ‚úÖ\n",
      "   Reason: The score is 1.00 because the retrieval context perfectly addresses the input by highlighting how transformers handle long-range dependencies, enable parallel processing, and are adaptable for various NLP tasks. Great job!\n",
      " - Hallucination: Score = 0.0, Success = ‚úÖ\n",
      "   Reason: The score is 0.00 because there are no factual alignments and the contradictions indicate that the actual output aligns well with the context, suggesting no hallucination.\n",
      " - Faithfulness: Score = 1.0, Success = ‚úÖ\n",
      "   Reason: The score is 1.00 because there are no contradictions, indicating that the actual output is perfectly aligned with the retrieval context. Great job!\n",
      "==================================================\n",
      "\n",
      "Output from GPT-4o:\n",
      "Transformers have revolutionized the field of natural language processing (NLP) due to their unique architecture and capabilities. Here are some of the key benefits of using transformers in NLP:\n",
      "\n",
      "1. Parallelization: Unlike recurrent neural networks (RNNs), transformers do not require sequential data processing, which allows for parallelization during training. This significantly speeds up the training process and makes it feasible to train on large datasets.\n",
      "\n",
      "2. Handling Long-Range Dependencies: Transformers use self-attention mechanisms that allow them to consider the entire context of a sentence or document when making predictions. This is particularly useful for capturing long-range dependencies in text, which is a limitation of traditional RNNs and LSTMs.\n",
      "\n",
      "3. Scalability: The architecture of transformers is highly scalable. Models like BERT, GPT, and T5 have billions of parameters, which allows them to learn complex patterns and representations from vast amounts of data.\n",
      "\n",
      "4. Transfer Learning: Transformers have popularized the use of transfer learning in NLP. Pre-trained models can be fine-tuned on specific tasks with relatively small amounts of data, which reduces the need for large labeled datasets and computational resources.\n",
      "\n",
      "5. State-of-the-Art Performance: Transformers have achieved state-of-the-art results on a wide range of NLP tasks, including machine translation, text summarization, sentiment analysis, and question answering. Their ability to model complex language patterns has set new benchmarks in the field.\n",
      "\n",
      "6. Versatility: The transformer architecture is highly versatile and can be adapted for various tasks beyond NLP, such as image processing and protein folding. This adaptability makes it a powerful tool across different domains.\n",
      "\n",
      "7. Robustness to Input Variability: Transformers can handle variable-length input sequences and are less sensitive to the order of input data compared to RNNs. This makes them robust to different types of input variability.\n",
      "\n",
      "8. Improved Contextual Understanding: By leveraging mechanisms like self-attention, transformers can better understand the context and nuances of language, leading to more accurate and contextually relevant outputs.\n",
      "\n",
      "9. Reduced Need for Feature Engineering: The ability of transformers to learn complex representations directly from raw text reduces the need for extensive feature engineering, simplifying the model development process.\n",
      "\n",
      "Overall, the transformer architecture has become a cornerstone of modern NLP due to its efficiency, effectiveness, and flexibility, enabling significant advancements in the field.\n",
      "\n",
      "==================================================\n",
      "\n",
      "=== GPT-3.5-turbo Evaluation Results ===\n",
      "\n",
      "Metrics Summary:\n",
      " - Answer Relevancy: Score = 1.0, Success = ‚úÖ\n",
      "   Reason: The score is 1.00 because the output is fully relevant and effectively addresses the benefits of using transformers in natural language processing. Great job!\n",
      " - Contextual Relevancy: Score = 1.0, Success = ‚úÖ\n",
      "   Reason: The score is 1.00 because the context perfectly aligns with the input, highlighting the benefits of transformers in NLP such as handling long-range dependencies and parallel processing. Great job!\n",
      " - Hallucination: Score = 0.0, Success = ‚úÖ\n",
      "   Reason: The score is 0.00 because there are no factual alignments or explicit contradictions identified, indicating that the actual output aligns well with the context provided.\n",
      " - Faithfulness: Score = 1.0, Success = ‚úÖ\n",
      "   Reason: The score is 1.00 because the actual output perfectly aligns with the retrieval context, showing no contradictions. Great job!\n",
      "==================================================\n",
      "\n",
      "Output from GPT-3.5-turbo:\n",
      "Transformers are a type of deep learning model that has revolutionized natural language processing (NLP) tasks. One of the main benefits of using transformers in NLP is their ability to capture long-range dependencies in text data. Traditional models like recurrent neural networks struggle with this because they process text sequentially, which can lead to information loss over long distances. Transformers, on the other hand, use self-attention mechanisms to weigh the importance of each word in a sentence, allowing them to capture relationships between words that are far apart.\n",
      "\n",
      "Another benefit of transformers is their scalability. They can be easily scaled up to handle large amounts of data and complex tasks, making them suitable for a wide range of NLP applications. Additionally, transformers have been shown to outperform traditional models on various NLP benchmarks, achieving state-of-the-art results in tasks such as machine translation, text generation, and sentiment analysis.\n",
      "\n",
      "Overall, transformers have significantly advanced the field of NLP by providing more accurate and efficient models for processing and understanding natural language text. Their ability to capture long-range dependencies, scalability, and superior performance make them a valuable tool for researchers and practitioners working in the field of NLP.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Evaluation results have been logged to 'evaluation_summary.json'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from deepeval import evaluate\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Attempt to import all relevant metrics and initialize them with the required parameters\n",
    "available_metrics = []\n",
    "metric_import_errors = []\n",
    "\n",
    "try:\n",
    "    from deepeval.metrics import AnswerRelevancyMetric\n",
    "    available_metrics.append(AnswerRelevancyMetric(threshold=0.5))\n",
    "except ImportError as e:\n",
    "    metric_import_errors.append(f\"AnswerRelevancyMetric: {e}\")\n",
    "\n",
    "try:\n",
    "    from deepeval.metrics import ContextualRelevancyMetric\n",
    "    available_metrics.append(ContextualRelevancyMetric(threshold=0.5))\n",
    "except ImportError as e:\n",
    "    metric_import_errors.append(f\"ContextualRelevancyMetric: {e}\")\n",
    "\n",
    "try:\n",
    "    from deepeval.metrics import HallucinationMetric\n",
    "    available_metrics.append(HallucinationMetric(threshold=0.5))\n",
    "except ImportError as e:\n",
    "    metric_import_errors.append(f\"HallucinationMetric: {e}\")\n",
    "\n",
    "try:\n",
    "    from deepeval.metrics import FaithfulnessMetric\n",
    "    available_metrics.append(FaithfulnessMetric(threshold=0.5))\n",
    "except ImportError as e:\n",
    "    metric_import_errors.append(f\"FaithfulnessMetric: {e}\")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Initialize the OpenAI LLMs via LangChain\n",
    "gpt4o_chat = ChatOpenAI(model=\"gpt-4o\", temperature=0, openai_api_key=openai_api_key)\n",
    "gpt35_chat = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "# Define a prompt for the LLM and the context it will be evaluated against\n",
    "prompt = \"Explain the benefits of using transformers in natural language processing.\"\n",
    "context = \"Transformers enable handling long-range dependencies, parallel processing, and fine-tuning for various NLP tasks.\"\n",
    "\n",
    "# Retrieve responses from both models\n",
    "gpt4_response = gpt4o_chat.invoke(prompt).content\n",
    "gpt35_response = gpt35_chat.invoke(prompt).content\n",
    "\n",
    "# Clean up any Markdown bold formatting\n",
    "cleaned_gpt4_response = re.sub(r'\\*\\*([^*]+)\\*\\*', r'\\1', gpt4_response)\n",
    "cleaned_gpt35_response = re.sub(r'\\*\\*([^*]+)\\*\\*', r'\\1', gpt35_response)\n",
    "\n",
    "# Create test cases for each model's response with the required context for hallucination metrics\n",
    "gpt4_test_case = LLMTestCase(\n",
    "    input=prompt,\n",
    "    actual_output=cleaned_gpt4_response,\n",
    "    retrieval_context=[context],  # This remains a list of strings for retrieval-based metrics\n",
    "    context=[context]  # Wrapping the context string in a list\n",
    ")\n",
    "\n",
    "gpt35_test_case = LLMTestCase(\n",
    "    input=prompt,\n",
    "    actual_output=cleaned_gpt35_response,\n",
    "    retrieval_context=[context],\n",
    "    context=[context]\n",
    ")\n",
    "\n",
    "# Evaluate both test cases\n",
    "evaluation_results_gpt4 = evaluate([gpt4_test_case], available_metrics)\n",
    "evaluation_results_gpt35 = evaluate([gpt35_test_case], available_metrics)\n",
    "\n",
    "def print_evaluation_summary(model_name, evaluation_results):\n",
    "    print(f\"\\n=== {model_name} Evaluation Results ===\\n\")\n",
    "\n",
    "    # Only loop through the first test case as they are printed separately in this function.\n",
    "    result = evaluation_results.test_results[0]\n",
    "\n",
    "    print(\"Metrics Summary:\")\n",
    "    for metric_data in result.metrics_data:\n",
    "        print(f\" - {metric_data.name}: Score = {metric_data.score}, Success = {'‚úÖ' if metric_data.success else '‚ùå'}\")\n",
    "        print(f\"   Reason: {metric_data.reason}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    print(f\"\\nOutput from {model_name}:\\n{result.actual_output}\\n\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "\n",
    "# Run the evaluation summaries for both models and avoid redundant information\n",
    "print_evaluation_summary(\"GPT-4o\", evaluation_results_gpt4)\n",
    "print_evaluation_summary(\"GPT-3.5-turbo\", evaluation_results_gpt35)\n",
    "\n",
    "# Log any metric import errors\n",
    "if metric_import_errors:\n",
    "    print(\"\\nMetric Import Errors:\")\n",
    "    for error in metric_import_errors:\n",
    "        print(f\" - {error}\")\n",
    "\n",
    "# Save the results to a JSON file with just final summary data\n",
    "output_data = {\n",
    "    \"gpt-4o\": evaluation_results_gpt4.dict(),\n",
    "    \"gpt-3.5-turbo\": evaluation_results_gpt35.dict()\n",
    "}\n",
    "with open('evaluation_summary.json', 'w') as f:\n",
    "    json.dump(output_data, f, indent=4)\n",
    "\n",
    "print(\"\\nEvaluation results have been logged to 'evaluation_summary.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5a394b9c-8084-4ab2-af87-79acd237742c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Contextual Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mContextual Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Hallucination Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mHallucination Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Faithfulness Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mFaithfulness Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 1 test case(s) in parallel: | |  0% (0/1) [Time Taken: 00:00, ?test case/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 1 test case(s) in parallel: |‚ñà|100% (1/1) [Time Taken: 00:10, 10.56s/test"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚úÖ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the explanation perfectly addressed the benefits of using transformers in natural language processing without including any irrelevant information. Great job!, error: None)\n",
      "  - ‚úÖ Contextual Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the relevant statement perfectly addresses the input by highlighting key benefits of transformers in NLP such as handling long-range dependencies and enabling parallel processing., error: None)\n",
      "  - ‚úÖ Hallucination (score: 0.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because there are no factual alignments provided and the contradictions indicate that the actual output aligns with the context, suggesting no hallucination in the output., error: None)\n",
      "  - ‚úÖ Faithfulness (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because there are no contradictions, reflecting perfect alignment between the actual output and the retrieval context. Great job!, error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Explain the benefits of using transformers in natural language processing.\n",
      "  - actual output: Transformers have revolutionized the field of natural language processing (NLP) due to several key benefits:\n",
      "\n",
      "1. Parallelization: Unlike recurrent neural networks (RNNs), which process data sequentially, transformers allow for parallel processing of data. This is achieved through the self-attention mechanism, which enables the model to consider all words in a sentence simultaneously. This parallelization significantly speeds up training and inference times.\n",
      "\n",
      "2. Handling Long-Range Dependencies: Transformers are particularly effective at capturing long-range dependencies in text. The self-attention mechanism allows the model to weigh the importance of different words in a sentence, regardless of their distance from each other. This is crucial for understanding context and meaning in complex sentences.\n",
      "\n",
      "3. Scalability: Transformers can be scaled up to create very large models, such as GPT-3 and BERT, which have demonstrated state-of-the-art performance on a wide range of NLP tasks. The architecture's ability to handle large amounts of data and parameters makes it suitable for training on extensive datasets.\n",
      "\n",
      "4. Transfer Learning: Pre-trained transformer models can be fine-tuned on specific tasks with relatively small amounts of task-specific data. This transfer learning capability allows for efficient use of resources and improves performance on tasks like sentiment analysis, question answering, and machine translation.\n",
      "\n",
      "5. Versatility: Transformers are highly versatile and can be applied to various NLP tasks, including text classification, translation, summarization, and more. Their architecture is not task-specific, which makes them adaptable to different applications with minimal modifications.\n",
      "\n",
      "6. Improved Contextual Understanding: Transformers provide a better understanding of context by using mechanisms like self-attention to focus on relevant parts of the input text. This leads to more accurate interpretations of meaning and nuances in language.\n",
      "\n",
      "7. Reduced Need for Feature Engineering: Traditional NLP models often required extensive feature engineering to capture linguistic nuances. Transformers, on the other hand, learn these features automatically from the data, reducing the need for manual intervention and domain expertise.\n",
      "\n",
      "8. Robustness to Noise: Transformers tend to be more robust to noise and variations in input data compared to some traditional models. This robustness is partly due to their ability to capture context and relationships between words more effectively.\n",
      "\n",
      "Overall, the transformer architecture has set new benchmarks in NLP by providing a powerful, flexible, and efficient framework for understanding and generating human language.\n",
      "  - expected output: None\n",
      "  - context: ['Transformers enable handling long-range dependencies, parallel processing, and fine-tuning for various NLP tasks.']\n",
      "  - retrieval context: ['Transformers enable handling long-range dependencies, parallel processing, and fine-tuning for various NLP tasks.']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Answer Relevancy: 100.00% pass rate\n",
      "Contextual Relevancy: 100.00% pass rate\n",
      "Hallucination: 100.00% pass rate\n",
      "Faithfulness: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">‚úì</span> Tests finished üéâ! Run <span style=\"color: #008000; text-decoration-color: #008000\">'deepeval login'</span> to save and analyze evaluation results on Confident AI. \n",
       "‚ÄºÔ∏è  Friendly reminder üòá: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
       "instead.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;5;245;141m‚úì\u001b[0m Tests finished üéâ! Run \u001b[32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI. \n",
       "‚ÄºÔ∏è  Friendly reminder üòá: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
       "instead.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GPT-4o Evaluation Results ===\n",
      "\n",
      "Model: GPT-4o\n",
      "\n",
      "Input: Explain the benefits of using transformers in natural language processing.\n",
      "\n",
      "Metrics Summary:\n",
      " - Answer Relevancy: Score = 1.0, Success = ‚úÖ\n",
      "   Reason: The score is 1.00 because the explanation perfectly addressed the benefits of using transformers in natural language processing without including any irrelevant information. Great job!\n",
      " - Contextual Relevancy: Score = 1.0, Success = ‚úÖ\n",
      "   Reason: The score is 1.00 because the relevant statement perfectly addresses the input by highlighting key benefits of transformers in NLP such as handling long-range dependencies and enabling parallel processing.\n",
      " - Hallucination: Score = 0.0, Success = ‚úÖ\n",
      "   Reason: The score is 0.00 because there are no factual alignments provided and the contradictions indicate that the actual output aligns with the context, suggesting no hallucination in the output.\n",
      " - Faithfulness: Score = 1.0, Success = ‚úÖ\n",
      "   Reason: The score is 1.00 because there are no contradictions, reflecting perfect alignment between the actual output and the retrieval context. Great job!\n",
      "==================================================\n",
      "\n",
      "Output from GPT-4o:\n",
      "Transformers have revolutionized the field of natural language processing (NLP) due to several key benefits:\n",
      "\n",
      "1. Parallelization: Unlike recurrent neural networks (RNNs), which process data sequentially, transformers allow for parallel processing of data. This is achieved through the self-attention mechanism, which enables the model to consider all words in a sentence simultaneously. This parallelization significantly speeds up training and inference times.\n",
      "\n",
      "2. Handling Long-Range Dependencies: Transformers are particularly effective at capturing long-range dependencies in text. The self-attention mechanism allows the model to weigh the importance of different words in a sentence, regardless of their distance from each other. This is crucial for understanding context and meaning in complex sentences.\n",
      "\n",
      "3. Scalability: Transformers can be scaled up to create very large models, such as GPT-3 and BERT, which have demonstrated state-of-the-art performance on a wide range of NLP tasks. The architecture's ability to handle large amounts of data and parameters makes it suitable for training on extensive datasets.\n",
      "\n",
      "4. Transfer Learning: Pre-trained transformer models can be fine-tuned on specific tasks with relatively small amounts of task-specific data. This transfer learning capability allows for efficient use of resources and improves performance on tasks like sentiment analysis, question answering, and machine translation.\n",
      "\n",
      "5. Versatility: Transformers are highly versatile and can be applied to various NLP tasks, including text classification, translation, summarization, and more. Their architecture is not task-specific, which makes them adaptable to different applications with minimal modifications.\n",
      "\n",
      "6. Improved Contextual Understanding: Transformers provide a better understanding of context by using mechanisms like self-attention to focus on relevant parts of the input text. This leads to more accurate interpretations of meaning and nuances in language.\n",
      "\n",
      "7. Reduced Need for Feature Engineering: Traditional NLP models often required extensive feature engineering to capture linguistic nuances. Transformers, on the other hand, learn these features automatically from the data, reducing the need for manual intervention and domain expertise.\n",
      "\n",
      "8. Robustness to Noise: Transformers tend to be more robust to noise and variations in input data compared to some traditional models. This robustness is partly due to their ability to capture context and relationships between words more effectively.\n",
      "\n",
      "Overall, the transformer architecture has set new benchmarks in NLP by providing a powerful, flexible, and efficient framework for understanding and generating human language.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Evaluation results have been logged to 'evaluation_summary.json'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from deepeval import evaluate\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Attempt to import all relevant metrics and initialize them with the required parameters\n",
    "available_metrics = []\n",
    "metric_import_errors = []\n",
    "\n",
    "try:\n",
    "    from deepeval.metrics import AnswerRelevancyMetric\n",
    "    available_metrics.append(AnswerRelevancyMetric(threshold=0.5))\n",
    "except ImportError as e:\n",
    "    metric_import_errors.append(f\"AnswerRelevancyMetric: {e}\")\n",
    "\n",
    "try:\n",
    "    from deepeval.metrics import ContextualRelevancyMetric\n",
    "    available_metrics.append(ContextualRelevancyMetric(threshold=0.5))\n",
    "except ImportError as e:\n",
    "    metric_import_errors.append(f\"ContextualRelevancyMetric: {e}\")\n",
    "\n",
    "try:\n",
    "    from deepeval.metrics import HallucinationMetric\n",
    "    available_metrics.append(HallucinationMetric(threshold=0.5))\n",
    "except ImportError as e:\n",
    "    metric_import_errors.append(f\"HallucinationMetric: {e}\")\n",
    "\n",
    "try:\n",
    "    from deepeval.metrics import FaithfulnessMetric\n",
    "    available_metrics.append(FaithfulnessMetric(threshold=0.5))\n",
    "except ImportError as e:\n",
    "    metric_import_errors.append(f\"FaithfulnessMetric: {e}\")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Initialize GPT-4o LLM via LangChain\n",
    "gpt4o_chat = ChatOpenAI(model=\"gpt-4o\", temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "# Define a prompt for the LLM and the context it will be evaluated against\n",
    "prompt = \"Explain the benefits of using transformers in natural language processing.\"\n",
    "context = \"Transformers enable handling long-range dependencies, parallel processing, and fine-tuning for various NLP tasks.\"\n",
    "\n",
    "# Retrieve response from GPT-4o\n",
    "gpt4_response = gpt4o_chat.invoke(prompt).content\n",
    "\n",
    "# Clean up any Markdown bold formatting\n",
    "cleaned_gpt4_response = re.sub(r'\\*\\*([^*]+)\\*\\*', r'\\1', gpt4_response)\n",
    "\n",
    "# Create test case for GPT-4o's response with the required context for hallucination metrics\n",
    "gpt4_test_case = LLMTestCase(\n",
    "    input=prompt,\n",
    "    actual_output=cleaned_gpt4_response,\n",
    "    retrieval_context=[context],  # For retrieval-based metrics\n",
    "    context=[context]  # Wrapping the context string in a list\n",
    ")\n",
    "\n",
    "# Evaluate the test case\n",
    "evaluation_results_gpt4 = evaluate([gpt4_test_case], available_metrics)\n",
    "\n",
    "# Print evaluation summary for GPT-4o only\n",
    "def print_evaluation_summary(model_name, evaluation_results):\n",
    "    print(f\"\\n=== {model_name} Evaluation Results ===\\n\")\n",
    "    \n",
    "    # Loop over each test case result\n",
    "    for result in evaluation_results.test_results:\n",
    "        print(f\"Model: {model_name}\\n\")\n",
    "        print(f\"Input: {result.input}\\n\")\n",
    "        print(\"Metrics Summary:\")\n",
    "\n",
    "        # Loop over each metric data and print only essential info\n",
    "        for metric_data in result.metrics_data:\n",
    "            print(f\" - {metric_data.name}: Score = {metric_data.score}, Success = {'‚úÖ' if metric_data.success else '‚ùå'}\")\n",
    "            print(f\"   Reason: {metric_data.reason}\")\n",
    "        \n",
    "        print(\"=\" * 50)\n",
    "        print(f\"\\nOutput from {model_name}:\\n{result.actual_output}\\n\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "# Run the evaluation summary for GPT-4o\n",
    "print_evaluation_summary(\"GPT-4o\", evaluation_results_gpt4)\n",
    "\n",
    "# Log any metric import errors\n",
    "if metric_import_errors:\n",
    "    print(\"\\nMetric Import Errors:\")\n",
    "    for error in metric_import_errors:\n",
    "        print(f\" - {error}\")\n",
    "\n",
    "# Save the results to a JSON file for GPT-4o only\n",
    "output_data = {\"gpt-4o\": evaluation_results_gpt4.dict()}\n",
    "with open('evaluation_summary.json', 'w') as f:\n",
    "    json.dump(output_data, f, indent=4)\n",
    "\n",
    "print(\"\\nEvaluation results have been logged to 'evaluation_summary.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045995e5-64f8-4b4b-b9b4-027e8b7c267a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from deepeval import evaluate\n",
    "from deepeval.metrics import GEval\n",
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Initialize GPT-4o LLM via LangChain\n",
    "gpt4o_chat = ChatOpenAI(model=\"gpt-4o\", temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "# Define a prompt and context for the evaluation\n",
    "prompt = \"Explain the benefits of using transformers in natural language processing.\"\n",
    "context = \"Transformers enable handling long-range dependencies, parallel processing, and fine-tuning for various NLP tasks.\"\n",
    "\n",
    "# Retrieve the response from GPT-4o\n",
    "gpt4_response = gpt4o_chat.invoke(prompt).content\n",
    "cleaned_gpt4_response = re.sub(r'\\*\\*([^*]+)\\*\\*', r'\\1', gpt4_response)\n",
    "\n",
    "# Define a custom GEval metric for Correctness\n",
    "correctness_metric = GEval(\n",
    "    name=\"Correctness\",\n",
    "    criteria=\"Evaluate if the actual output accurately covers the benefits of transformers in NLP based on the context provided.\",\n",
    "    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.CONTEXT],\n",
    "    threshold=0.7,\n",
    "    async_mode=True,\n",
    "    verbose_mode=True\n",
    ")\n",
    "\n",
    "# Create a test case with the required context\n",
    "test_case = LLMTestCase(\n",
    "    input=prompt,\n",
    "    actual_output=cleaned_gpt4_response,\n",
    "    context=[context]\n",
    ")\n",
    "\n",
    "# Measure the custom GEval metric\n",
    "correctness_metric.measure(test_case)\n",
    "\n",
    "# Output the results for this custom metric\n",
    "print(\"G-Eval Correctness Metric Results:\")\n",
    "print(f\" - Score: {correctness_metric.score}\")\n",
    "print(f\" - Reason: {correctness_metric.reason}\")\n",
    "\n",
    "# Save the result to a JSON file\n",
    "output_data = {\n",
    "    \"G-Eval Correctness Metric\": {\n",
    "        \"Score\": correctness_metric.score,\n",
    "        \"Reason\": correctness_metric.reason\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('g_eval_correctness_summary.json', 'w') as f:\n",
    "    json.dump(output_data, f, indent=4)\n",
    "\n",
    "print(\"\\nEvaluation results have been logged to 'g_eval_correctness_summary.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc2df47-6023-4465-86ba-66c1f8097fce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
