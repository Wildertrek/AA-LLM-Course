{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a36ef637-fbe0-4d8b-85d9-1cdb24f1292b",
   "metadata": {},
   "source": [
    "Below is an example that demonstrates how to call OpenAI's Chat Completions API and retrieve both the response details and the accompanying debug and usage information that can be useful for our AI Assistant application. This example not only shows the minimal response data (such as the chat message, model, and usage statistics) but also emphasizes the debug headers (like processing time, API version, and request ID) that you can log for troubleshooting and performance monitoring.\n",
    "\n",
    "### Example Code to Call the Chat Completions API\n",
    "\n",
    "```python\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "def get_env_var(var: str):\n",
    "    value = os.getenv(var)\n",
    "    if value is None:\n",
    "        raise ValueError(f\"{var} not found in environment variables.\")\n",
    "    return value\n",
    "\n",
    "# Retrieve our OpenAI API key from the environment\n",
    "openai_api_key = get_env_var(\"OPENAI_API_COURSE_KEY\")\n",
    "\n",
    "# Define the endpoint for chat completions\n",
    "chat_completion_url = \"https://api.openai.com/v1/chat/completions\"\n",
    "\n",
    "# Create a sample payload for a chat completion request\n",
    "payload = {\n",
    "    \"model\": \"gpt-4o\",  # You can change this to the model of our choice (e.g., \"gpt-4o\", \"gpt-4o-mini\", etc.)\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello, how can you help me today?\"}\n",
    "    ],\n",
    "    # Optional parameters:\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 1.0,\n",
    "    \"n\": 1,  # Number of choices\n",
    "    \"max_completion_tokens\": 150  # Upper bound for generated tokens\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {openai_api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Make the POST request to the Chat Completions API\n",
    "response = requests.post(chat_completion_url, headers=headers, json=payload, timeout=30)\n",
    "\n",
    "if response.ok:\n",
    "    chat_completion = response.json()\n",
    "    print(\"Chat Completion Response:\")\n",
    "    print(json.dumps(chat_completion, indent=4))\n",
    "    \n",
    "    # Extract and display useful debug and rate limiting headers\n",
    "    headers_to_log = [\n",
    "        \"openai-organization\",\n",
    "        \"openai-processing-ms\",\n",
    "        \"openai-version\",\n",
    "        \"x-request-id\",\n",
    "        \"x-ratelimit-limit-requests\",\n",
    "        \"x-ratelimit-limit-tokens\",\n",
    "        \"x-ratelimit-remaining-requests\",\n",
    "        \"x-ratelimit-remaining-tokens\",\n",
    "        \"x-ratelimit-reset-requests\",\n",
    "        \"x-ratelimit-reset-tokens\"\n",
    "    ]\n",
    "    \n",
    "    debug_info = {header: response.headers.get(header) for header in headers_to_log}\n",
    "    print(\"\\nDebug/Rate Limiting Headers:\")\n",
    "    print(json.dumps(debug_info, indent=4))\n",
    "else:\n",
    "    print(f\"Failed to get chat completion: {response.status_code} {response.text}\")\n",
    "```\n",
    "\n",
    "### What Information Is Retrieved?\n",
    "\n",
    "When you call the chat completions endpoint, you get a response that includes:\n",
    "\n",
    "- **Chat Completion Object Details:**\n",
    "  - **id:** A unique identifier for the chat completion (e.g., `\"chatcmpl-123\"`).\n",
    "  - **object:** The object type (always `\"chat.completion\"`).\n",
    "  - **created:** Unix timestamp for when the chat completion was created.\n",
    "  - **model:** The model used to generate the response.\n",
    "  - **choices:** An array containing the generated message(s) along with metadata such as the `finish_reason`.\n",
    "  - **usage:** Detailed token usage, including prompt tokens, completion tokens, and total tokens.\n",
    "  - **service_tier:** The service tier used (e.g., `\"default\"`).\n",
    "\n",
    "- **Debug/Rate Limiting Information (from HTTP Headers):**\n",
    "  - **openai-organization:** The organization associated with the request.\n",
    "  - **openai-processing-ms:** Time taken to process our request in milliseconds.\n",
    "  - **openai-version:** The API version used.\n",
    "  - **x-request-id:** A unique ID for the request (useful for debugging).\n",
    "  - **x-ratelimit-*** headers: (These might be null for certain endpoints, but are useful for endpoints where rate limiting data is provided.)\n",
    "\n",
    "### Dynamically Pulling in a Special OpenAI Configuration\n",
    "\n",
    "For our AI Assistant, you might want to dynamically configure parameters based on the model selected. For example, if a particular model supports multi-modal outputs or has a different context window, you could maintain a configuration dictionary (or fetch additional details from a secondary source) and merge that into our request payload. Here’s a simple example:\n",
    "\n",
    "```python\n",
    "# Example: Additional configuration per model\n",
    "MODEL_CONFIG = {\n",
    "    \"gpt-4o\": {\n",
    "        \"max_completion_tokens\": 150,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 1.0,\n",
    "        \"presence_penalty\": 0.0,\n",
    "        \"frequency_penalty\": 0.0,\n",
    "        \"stop\": None\n",
    "    },\n",
    "    \"gpt-4o-mini\": {\n",
    "        \"max_completion_tokens\": 100,\n",
    "        \"temperature\": 0.5,\n",
    "        \"top_p\": 0.9,\n",
    "        \"presence_penalty\": 0.0,\n",
    "        \"frequency_penalty\": 0.0,\n",
    "        \"stop\": None\n",
    "    }\n",
    "}\n",
    "\n",
    "selected_model = \"gpt-4o\"  # This can be set dynamically\n",
    "model_specific_config = MODEL_CONFIG.get(selected_model, {})\n",
    "\n",
    "# Merge the base payload with the model-specific configuration\n",
    "payload.update(model_specific_config)\n",
    "payload[\"model\"] = selected_model\n",
    "```\n",
    "\n",
    "This approach allows our AI Assistant to dynamically adapt to different OpenAI models based on their capabilities and our application’s needs.\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **Automatic Data Retrieval:**  \n",
    "  You get the basic chat completion response (including the generated messages and usage stats) directly from the API.\n",
    "  \n",
    "- **Debug and Rate Limiting Data:**  \n",
    "  HTTP headers provide useful information like processing time and request IDs, which help with debugging and monitoring.\n",
    "  \n",
    "- **Dynamic Configuration:**  \n",
    "  By combining the API response with a custom configuration (stored in a dictionary), you can dynamically tailor requests to different OpenAI models to suit our assistant’s requirements.\n",
    "\n",
    "This setup should give you a robust foundation for integrating and dynamically configuring OpenAI's chat completions in our AI Assistant application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f54e174-680f-48f8-b6fa-a7d42be4dab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat Completion Response:\n",
      "{\n",
      "    \"id\": \"chatcmpl-BDXMEvOKQnSYFATQSDa0BbxRSDUrN\",\n",
      "    \"object\": \"chat.completion\",\n",
      "    \"created\": 1742566190,\n",
      "    \"model\": \"gpt-4o-2024-08-06\",\n",
      "    \"choices\": [\n",
      "        {\n",
      "            \"index\": 0,\n",
      "            \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"Hello! I'm here to assist you with a variety of tasks, such as answering questions, providing information, helping with problem-solving, or offering suggestions on a wide range of topics. How can I assist you today?\",\n",
      "                \"refusal\": null,\n",
      "                \"annotations\": []\n",
      "            },\n",
      "            \"logprobs\": null,\n",
      "            \"finish_reason\": \"stop\"\n",
      "        }\n",
      "    ],\n",
      "    \"usage\": {\n",
      "        \"prompt_tokens\": 27,\n",
      "        \"completion_tokens\": 44,\n",
      "        \"total_tokens\": 71,\n",
      "        \"prompt_tokens_details\": {\n",
      "            \"cached_tokens\": 0,\n",
      "            \"audio_tokens\": 0\n",
      "        },\n",
      "        \"completion_tokens_details\": {\n",
      "            \"reasoning_tokens\": 0,\n",
      "            \"audio_tokens\": 0,\n",
      "            \"accepted_prediction_tokens\": 0,\n",
      "            \"rejected_prediction_tokens\": 0\n",
      "        }\n",
      "    },\n",
      "    \"service_tier\": \"default\",\n",
      "    \"system_fingerprint\": \"fp_6ec83003ad\"\n",
      "}\n",
      "\n",
      "Debug/Rate Limiting Headers:\n",
      "{\n",
      "    \"openai-organization\": \"user-ue08pmd83ul7gjg1su5tgani\",\n",
      "    \"openai-processing-ms\": \"861\",\n",
      "    \"openai-version\": \"2020-10-01\",\n",
      "    \"x-request-id\": \"req_4d55e0b8ecd8e46447c0701e1f8b6051\",\n",
      "    \"x-ratelimit-limit-requests\": \"500\",\n",
      "    \"x-ratelimit-limit-tokens\": \"30000\",\n",
      "    \"x-ratelimit-remaining-requests\": \"499\",\n",
      "    \"x-ratelimit-remaining-tokens\": \"29981\",\n",
      "    \"x-ratelimit-reset-requests\": \"120ms\",\n",
      "    \"x-ratelimit-reset-tokens\": \"38ms\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "def get_env_var(var: str):\n",
    "    value = os.getenv(var)\n",
    "    if value is None:\n",
    "        raise ValueError(f\"{var} not found in environment variables.\")\n",
    "    return value\n",
    "\n",
    "# Retrieve our OpenAI API key from the environment\n",
    "openai_api_key = get_env_var(\"OPENAI_API_COURSE_KEY\")\n",
    "\n",
    "# Define the endpoint for chat completions\n",
    "chat_completion_url = \"https://api.openai.com/v1/chat/completions\"\n",
    "\n",
    "# Create a sample payload for a chat completion request\n",
    "payload = {\n",
    "    \"model\": \"gpt-4o\",  # You can change this to the model of our choice (e.g., \"gpt-4o\", \"gpt-4o-mini\", etc.)\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello, how can you help me today?\"}\n",
    "    ],\n",
    "    # Optional parameters:\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 1.0,\n",
    "    \"n\": 1,  # Number of choices\n",
    "    \"max_completion_tokens\": 150  # Upper bound for generated tokens\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {openai_api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Make the POST request to the Chat Completions API\n",
    "response = requests.post(chat_completion_url, headers=headers, json=payload, timeout=30)\n",
    "\n",
    "if response.ok:\n",
    "    chat_completion = response.json()\n",
    "    print(\"Chat Completion Response:\")\n",
    "    print(json.dumps(chat_completion, indent=4))\n",
    "    \n",
    "    # Extract and display useful debug and rate limiting headers\n",
    "    headers_to_log = [\n",
    "        \"openai-organization\",\n",
    "        \"openai-processing-ms\",\n",
    "        \"openai-version\",\n",
    "        \"x-request-id\",\n",
    "        \"x-ratelimit-limit-requests\",\n",
    "        \"x-ratelimit-limit-tokens\",\n",
    "        \"x-ratelimit-remaining-requests\",\n",
    "        \"x-ratelimit-remaining-tokens\",\n",
    "        \"x-ratelimit-reset-requests\",\n",
    "        \"x-ratelimit-reset-tokens\"\n",
    "    ]\n",
    "    \n",
    "    debug_info = {header: response.headers.get(header) for header in headers_to_log}\n",
    "    print(\"\\nDebug/Rate Limiting Headers:\")\n",
    "    print(json.dumps(debug_info, indent=4))\n",
    "else:\n",
    "    print(f\"Failed to get chat completion: {response.status_code} {response.text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0114602-85b7-425d-8731-8b7d43dd4ccd",
   "metadata": {},
   "source": [
    "The response shows that OpenAI’s Chat Completion endpoint returns a structured JSON object that includes not only the generated message but also detailed usage statistics and debug information. Here’s a breakdown of what you received:\n",
    "\n",
    "### Chat Completion Response Details\n",
    "- **id:** `\"chatcmpl-BDXMEvOKQnSYFATQSDa0BbxRSDUrN\"`  \n",
    "  A unique identifier for this chat completion.\n",
    "\n",
    "- **object:** `\"chat.completion\"`  \n",
    "  Indicates that the response is a chat completion object.\n",
    "\n",
    "- **created:** `1742566190`  \n",
    "  A Unix timestamp for when the completion was created.\n",
    "\n",
    "- **model:** `\"gpt-4o-2024-08-06\"`  \n",
    "  The specific model used to generate this response.\n",
    "\n",
    "- **choices:**  \n",
    "  An array containing one or more response choices. In this example, it has one choice:\n",
    "  - **index:** `0`  \n",
    "    The position of the choice in the list.\n",
    "  - **message:**  \n",
    "    The generated message:\n",
    "    - **role:** `\"assistant\"`  \n",
    "      Indicates the speaker in the conversation.\n",
    "    - **content:**  \n",
    "      The actual text response from the assistant.\n",
    "    - **refusal:** `null`  \n",
    "      No refusal data is present.\n",
    "    - **annotations:** `[]`  \n",
    "      No annotations included.\n",
    "  - **logprobs:** `null`  \n",
    "    No log probabilities returned.\n",
    "  - **finish_reason:** `\"stop\"`  \n",
    "    Indicates that the generation stopped naturally.\n",
    "\n",
    "- **usage:**  \n",
    "  Detailed token usage for the request:\n",
    "  - **prompt_tokens:** `27`  \n",
    "    Tokens used for the prompt.\n",
    "  - **completion_tokens:** `44`  \n",
    "    Tokens generated for the response.\n",
    "  - **total_tokens:** `71`  \n",
    "    Total tokens consumed.\n",
    "  - **prompt_tokens_details:** and **completion_tokens_details:**  \n",
    "    Provide further breakdown, though here they are mostly zeros.\n",
    "\n",
    "- **service_tier:** `\"default\"`  \n",
    "  Indicates the service tier used for processing the request.\n",
    "\n",
    "- **system_fingerprint:** `\"fp_6ec83003ad\"`  \n",
    "  A fingerprint for the backend configuration that generated the response.\n",
    "\n",
    "### Debug/Rate Limiting Headers\n",
    "- **openai-organization:** `\"user-ue08pmd83ul7gjg1su5tgani\"`  \n",
    "  The organization associated with the API request.\n",
    "\n",
    "- **openai-processing-ms:** `\"861\"`  \n",
    "  The time (in milliseconds) it took to process the request.\n",
    "\n",
    "- **openai-version:** `\"2020-10-01\"`  \n",
    "  The API version used.\n",
    "\n",
    "- **x-request-id:** `\"req_4d55e0b8ecd8e46447c0701e1f8b6051\"`  \n",
    "  A unique request ID, useful for debugging.\n",
    "\n",
    "- **Rate Limiting Headers:**  \n",
    "  These headers provide information on our rate limits:\n",
    "  - **x-ratelimit-limit-requests:** `\"500\"`\n",
    "  - **x-ratelimit-limit-tokens:** `\"30000\"`\n",
    "  - **x-ratelimit-remaining-requests:** `\"499\"`\n",
    "  - **x-ratelimit-remaining-tokens:** `\"29981\"`\n",
    "  - **x-ratelimit-reset-requests:** `\"120ms\"`\n",
    "  - **x-ratelimit-reset-tokens:** `\"38ms\"`\n",
    "\n",
    "### How This Information Can Be Used in Our AI Assistant Application\n",
    "\n",
    "1. **Dynamic Configuration:**  \n",
    "   You can adjust parameters like `temperature`, `max_completion_tokens`, and others dynamically based on the model’s capabilities and our application's needs. For example, you might use different configurations for a model that is more expensive or has a larger context window.\n",
    "\n",
    "2. **Usage Monitoring and Cost Management:**  \n",
    "   The `usage` section provides token counts that can help you monitor costs, since billing is often based on token usage. You can log these stats and even set up alerts if usage spikes.\n",
    "\n",
    "3. **Debugging and Support:**  \n",
    "   The debug headers (especially `x-request-id`) are invaluable when you need to troubleshoot issues with OpenAI support or log performance metrics for our application.\n",
    "\n",
    "4. **Rate Limit Management:**  \n",
    "   The rate limit headers let you know how many requests or tokens you have remaining in the current window, allowing you to manage our application's load and avoid hitting limits.\n",
    "\n",
    "5. **Backend Consistency:**  \n",
    "   The `system_fingerprint` can be used to track backend configuration changes. If you need determinism in our responses (for instance, when using a seed), this fingerprint helps you verify that the backend hasn’t changed unexpectedly.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Enrich Metadata:**  \n",
    "  Consider combining this API response data with additional model metadata from our internal configurations (e.g., context window sizes, pricing tiers) to have a complete picture of each model's capabilities.\n",
    "\n",
    "- **Logging and Analytics:**  \n",
    "  Integrate the logging of usage and debug headers into our application’s monitoring system to track performance over time.\n",
    "\n",
    "- **Dynamic Parameter Tuning:**  \n",
    "  Use the token usage statistics to dynamically adjust parameters such as `max_completion_tokens` for future requests, ensuring cost efficiency while maintaining response quality.\n",
    "\n",
    "This comprehensive view of the chat completion response and its associated headers is key to building an AI Assistant that is both efficient and cost-effective, while also providing a robust foundation for debugging and performance monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267642b1-390c-4a66-9773-3925d2ca1d20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
