{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "473d1dd5-f784-44e1-a481-1cfabce4a8d1",
   "metadata": {},
   "source": [
    "**Figure 1: Technology Tree of RAG Research**\n",
    "\n",
    "Figure 1 in the paper presents a hierarchical \"technology tree\" that maps the evolution and diversification of Retrieval-Augmented Generation (RAG) research, particularly as it applies to large language models (LLMs). The tree is structured around three key stages in the life cycle of language model development: **pre-training, fine-tuning,** and **inference**. Each branch of the tree represents how retrieval techniques have been incorporated and evolved across these stages.\n",
    "\n",
    "### 1. Inference Stage\n",
    "\n",
    "- **Early Focus on In-Context Learning:**  \n",
    "  At the inference stage, early RAG research capitalized on the remarkable in-context learning abilities of LLMs. In this phase, the models were used in a \"retrieve-then-generate\" manner, where external documents or knowledge snippets were fetched on the fly and then provided as context to guide the generation process.\n",
    "  \n",
    "- **Dynamic Retrieval without Model Modification:**  \n",
    "  The retrieval component worked as an external add-on that enriched the model's context during query time. This approach did not require any changes to the underlying model parameters and leveraged prompt engineering to integrate retrieved information seamlessly.\n",
    "\n",
    "### 2. Fine-Tuning Stage\n",
    "\n",
    "- **Integration During Training:**  \n",
    "  As research progressed, the focus shifted toward integrating retrieval mechanisms more deeply during the **fine-tuning** phase. Here, the external knowledge was not merely appended at inference but was also incorporated into the training process.\n",
    "  \n",
    "- **Enhanced Knowledge Fusion:**  \n",
    "  Fine-tuning with retrieved data allowed models to learn how to effectively combine generated content with retrieved information. This involved training strategies such as:\n",
    "  - Augmenting the training dataset with retrieved passages.\n",
    "  - Designing loss functions that encourage the model to balance internal knowledge with external evidence.\n",
    "  - Learning to rank and select the most relevant pieces of information for a given context.\n",
    "  \n",
    "- **Closer Coupling of Retrieval and Generation:**  \n",
    "  The fine-tuning stage represents a more integrated approach where the model’s parameters adapt to handle the interplay between its own learned representations and the dynamically retrieved content, leading to more coherent and factually accurate outputs.\n",
    "\n",
    "### 3. Pre-Training Stage\n",
    "\n",
    "- **Embedding Retrieval Capabilities from the Start:**  \n",
    "  The most recent frontier in RAG research is the exploration of incorporating retrieval-augmented techniques during the **pre-training** phase. This represents an effort to build retrieval functionalities directly into the foundational training of the model.\n",
    "  \n",
    "- **Intrinsic Retrieval-Enhanced Models:**  \n",
    "  By integrating retrieval objectives early on, researchers aim to develop models that are inherently equipped to retrieve and process external knowledge. This could involve:\n",
    "  - Designing novel pre-training tasks that require the model to retrieve relevant information as part of its learning process.\n",
    "  - Modifying the model architecture to include components that are dedicated to handling retrieval operations.\n",
    "  \n",
    "- **Potential for More Robust Knowledge Integration:**  \n",
    "  Embedding retrieval in the pre-training phase has the potential to produce models that are not only more knowledgeable but also more efficient in how they access and utilize external information during downstream tasks.\n",
    "\n",
    "### Overall Evolution Captured by the Technology Tree\n",
    "\n",
    "- **From Surface-Level Augmentation to Deep Integration:**  \n",
    "  The technology tree vividly illustrates the progression from simple retrieval augmentation during inference—where external data was simply tacked onto prompts—to a deeper integration where retrieval mechanisms are fused into both the fine-tuning and pre-training stages. This evolution underscores a trend toward models that are designed from the ground up to handle and integrate vast external knowledge bases.\n",
    "  \n",
    "- **Emerging Trends and Future Directions:**  \n",
    "  While the inference stage laid the groundwork by demonstrating the utility of external knowledge via in-context learning, the subsequent stages (fine-tuning and pre-training) represent ongoing research efforts to refine and embed retrieval more fundamentally within LLMs. This progression is seen as key to overcoming current limitations in factual accuracy and knowledge updating in language models.\n",
    "\n",
    "In summary, **Figure 1** serves as a visual roadmap of RAG research, highlighting how the integration of retrieval mechanisms has moved from an add-on feature at the inference stage to a deeply embedded component within fine-tuning and pre-training. This evolution reflects the broader trend in the field toward building more capable, knowledgeable, and adaptable language models by leveraging external information at every stage of their development."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dae3a90-6e24-4b47-aa86-75e0c761c54a",
   "metadata": {},
   "source": [
    "To install and test the `unstructured` Python package, follow these steps:\n",
    "\n",
    "[Unstructured Quickstart](https://docs.unstructured.io/open-source/introduction/quick-start?utm_source=chatgpt.com)\n",
    "\n",
    "1. **Install the Core Package**:\n",
    "   Begin by installing the core `unstructured` package using `pip`:\n",
    "\n",
    "   ```bash\n",
    "   pip install unstructured\n",
    "   ```\n",
    "\n",
    "   This installation supports processing of plain text files, HTML, XML, JSON, and emails without additional dependencies. \n",
    "\n",
    "2. **Install Additional Dependencies for Other Document Types**:\n",
    "   If you plan to process other document types, such as PDFs, images, or Microsoft Office documents, you'll need to install additional dependencies:\n",
    "\n",
    "   - **For PDFs and Images**:\n",
    "     ```bash\n",
    "     pip install \"unstructured[local-inference]\"\n",
    "     ```\n",
    "     This command installs the necessary packages for handling PDFs and images. \n",
    "\n",
    "   - **For Specific Document Types**:\n",
    "     You can install dependencies tailored to specific document formats:\n",
    "\n",
    "     - **DOCX (Word Documents)**:\n",
    "       ```bash\n",
    "       pip install \"unstructured[docx]\"\n",
    "       ```\n",
    "\n",
    "     - **PPTX (PowerPoint Presentations)**:\n",
    "       ```bash\n",
    "       pip install \"unstructured[pptx]\"\n",
    "       ```\n",
    "\n",
    "     - **To Install Dependencies for All Supported Document Types**:\n",
    "       ```bash\n",
    "       pip install \"unstructured[all-docs]\"\n",
    "       ```\n",
    "       This command ensures you have all the necessary packages for processing all supported document types. \n",
    "\n",
    "3. **Install System Dependencies**:\n",
    "   Depending on the document types you're processing, you might need to install additional system packages:\n",
    "\n",
    "   - **libmagic-dev**: For file type detection.\n",
    "   - **poppler-utils**: Required for PDFs and images.\n",
    "   - **tesseract-ocr**: Essential for OCR operations on images and PDFs.\n",
    "   - **libreoffice**: Needed for processing Microsoft Office documents.\n",
    "   - **pandoc**: Used for handling EPUBs, RTFs, and Open Office documents.\n",
    "\n",
    "   Installation commands for these dependencies vary based on your operating system. For instance, on Ubuntu, you can install them using `apt-get`:\n",
    "\n",
    "   ```bash\n",
    "   sudo apt-get install libmagic-dev poppler-utils tesseract-ocr libreoffice pandoc\n",
    "   ```\n",
    "\n",
    "   Ensure you have the appropriate permissions and that your system's package manager is up-to-date. \n",
    "\n",
    "4. **Validate the Installation**:\n",
    "   After installing the necessary packages and dependencies, you can test the installation by running a simple Python script:\n",
    "\n",
    "   ```python\n",
    "   from unstructured.partition.auto import partition\n",
    "\n",
    "   elements = partition(filename=\"path/to/your/document\")\n",
    "   for element in elements:\n",
    "       print(element)\n",
    "   ```\n",
    "\n",
    "   Replace `\"path/to/your/document\"` with the path to a document you wish to process. This script will partition the document into its constituent elements and print them out. \n",
    "\n",
    "By following these steps, you'll have the `unstructured` package installed and be ready to process various document types in your Python environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e90bef-c0d8-4357-974b-606d00def033",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
